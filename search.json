[
  {
    "objectID": "lectures/L09/Lecture-09.html#workflow",
    "href": "lectures/L09/Lecture-09.html#workflow",
    "title": "Regression: predictive modelling",
    "section": "Workflow",
    "text": "Workflow\n\nModel development\n\nExplore: visualise, summarise\nTransform predictors: linearise, reduce skewness/leverage\nModel: fit, check assumptions, interpret, transform. Repeat.\n\nVariable selection\n\nVIF: remove predictors with high variance inflation factor\nModel selection: stepwise selection, AIC, principle of parsimony, assumption checks\n\nPredictive modelling\n\nPredict: Use the model to predict new data\nValidate: Evaluate the model’s performance"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#previously-on-envx2001",
    "href": "lectures/L09/Lecture-09.html#previously-on-envx2001",
    "title": "Regression: predictive modelling",
    "section": "Previously on ENVX2001…",
    "text": "Previously on ENVX2001…\nWe fitted a multiple linear regression model to the data.\n\n\nCode\nfit &lt;- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(fit)\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#predictions-by-hand",
    "href": "lectures/L09/Lecture-09.html#predictions-by-hand",
    "title": "Regression: predictive modelling",
    "section": "Predictions by hand",
    "text": "Predictions by hand\n \\widehat{log(Ozone)}=-0.262 + \\color{darkorchid}{0.0492} \\cdot Temp + \\color{darkorange}{0.00252} \\cdot Solar.R - \\color{seagreen}{0.0616} \\cdot Wind \nOn a certain day, we measured (in imperial units):\n\ntemperature Temp to be 80 degrees Fahrenheit\nsolar radiation Solar.R to be 145 units (Langleys)\nwind speed Wind to be 10.9 miles per hour\n\nWhat is the predicted ozone level?\n\\widehat{log(Ozone)}= -0.262 + \\color{darkorchid}{0.0492 \\cdot 80} + \\color{darkorange}{0.00252 \\cdot 145} - \\color{seagreen}{0.0616 \\cdot 10.9}\nEasy! The two things we need to think about are…\n\nWhat is the uncertainty in this prediction?\nCan this model be used to predict ozone if we collect new data in the future?"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#uncertainty",
    "href": "lectures/L09/Lecture-09.html#uncertainty",
    "title": "Regression: predictive modelling",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nConfidence interval: uncertainty in the mean response at a given predictor value.\nPrediction interval: uncertainty in a single response at a given predictor value.\n\nWhat it means\n\n95% confidence interval: Given the parameters of the model, we are 95% confident that the mean response at a given predictor value is between y_1 and y_2.\n\n\n95% prediction interval: Given the parameters of the model, we are 95% confident that a single response at a given predictor value is between y_1 and y_2."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#equations",
    "href": "lectures/L09/Lecture-09.html#equations",
    "title": "Regression: predictive modelling",
    "section": "Equations",
    "text": "Equations\nThe equation to calculate any prediction interval is:\n \\widehat{y} \\pm t_{\\alpha/2} \\cdot se(\\widehat{y}) \nwhere:\n\n\\widehat{y} is the predicted value\nt_{\\alpha/2} is the critical value of the t-distribution for a given confidence level\nse(\\widehat{y}) is the standard error of the prediction\n\nThe difference in calculating a confidence interval and a prediction interval is in the standard error of the prediction."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#equations-1",
    "href": "lectures/L09/Lecture-09.html#equations-1",
    "title": "Regression: predictive modelling",
    "section": "Equations",
    "text": "Equations\nCI: standard error of the fit\n se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)} \nPI: standard error of the prediction\n se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)}\n\nx_0 is value of the predictor for which we want a response\nMSE is the mean squared error of the fit (SS_{xx})\n\\sum_{i=1}^n (x_i - \\bar{x})^2 is the sum of squares of the predictor values\nn is the number of observations\n\\bar{x} is the mean of the predictor values\n\nThe prediction interval formula has an additional term (1 \\cdot MSE). There is uncertainty that the mean prediction will be similar to the observed, and additional uncertainty/variability for a single response (equivalent to the MSE). Thus the confidence interval is always narrower than the prediction interval."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#predictions-in-r",
    "href": "lectures/L09/Lecture-09.html#predictions-in-r",
    "title": "Regression: predictive modelling",
    "section": "Predictions in R",
    "text": "Predictions in R\n\nFirst, we need to create a new data frame with the predictor values we want to predict at – it must include all variables in the fitted model.\n\n\n\nCode\npredict_df &lt;- data.frame(Temp = 80, Solar.R = 145, Wind = 10.9)\n\n\n\nWe use the predict() function to obtain the predicted value.\nSpecifying interval = \"confidence\" or interval = \"prediction\" also calculates the confidence or prediction interval.\n\n\n\nCode\npredict(fit, newdata = predict_df) # the predicted value\n\n\n       1 \n3.365227 \n\n\nCode\npredict(fit, newdata = predict_df, interval = \"confidence\") # predicted value and CI\n\n\n       fit      lwr      upr\n1 3.365227 3.246265 3.484189\n\n\nCode\npredict(fit, newdata = predict_df, interval = \"prediction\") # predicted value and PI\n\n\n       fit      lwr      upr\n1 3.365227 2.350051 4.380404"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#visualising-ci-and-pi",
    "href": "lectures/L09/Lecture-09.html#visualising-ci-and-pi",
    "title": "Regression: predictive modelling",
    "section": "Visualising CI and PI",
    "text": "Visualising CI and PI\n\n\nCode\nairquality$pred &lt;- predict(fit, newdata = airquality) # predict for existing data\n\npreds_ci &lt;- predict(fit, newdata = airquality, interval = \"confidence\") # confidence interval\npreds_pi &lt;- predict(fit, newdata = airquality, interval = \"prediction\") # prediction interval\n\n\n\nWe can now plot the CI and PI as shaded areas around the predicted line\n\n\n\nCode\np &lt;-\n  ggplot(airquality, aes(pred, log(Ozone))) +\n  geom_point() + \n  geom_line(data = preds_ci, aes(fit, lwr), color = \"blue\") +\n  geom_line(data = preds_ci, aes(fit, upr), color = \"blue\") +\n  geom_line(data = preds_pi, aes(fit, lwr), color = \"red\") +\n  geom_line(data = preds_pi, aes(fit, upr), color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\np"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#visualising-with-geom_smooth",
    "href": "lectures/L09/Lecture-09.html#visualising-with-geom_smooth",
    "title": "Regression: predictive modelling",
    "section": "Visualising with geom_smooth()",
    "text": "Visualising with geom_smooth()\n\n\n\ngeom_smooth() fits a linear model to obtain a smooth line\nFor visualisation we can use geom_smooth() instead of geom_line() to fit smoothed lines\n\n\n\nCode\np &lt;-\n  ggplot(airquality, aes(pred, log(Ozone))) +\n  geom_point() + \n  geom_smooth(data = preds_ci, aes(fit, lwr), color = \"blue\", se = F) +\n  geom_smooth(data = preds_ci, aes(fit, upr), color = \"blue\", se = F) +\n  geom_smooth(data = preds_pi, aes(fit, lwr), color = \"red\", se = F) +\n  geom_smooth(data = preds_pi, aes(fit, upr), color = \"red\", se = F) +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\np\n\n\n\n\n\n\n\n\n\n\n\nThe hyperparameter se = TRUE fits a smoothed CI around the predicted line\nSmoothed with loess functions\nCannot fit the prediction interval\n\n\n\nCode\np + geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\n\n\n\n\n\n\nCalibration and validation\nAll is good when we want to assess uncertainty in a model that we have already fit.\nWhat if we want to know how well the model predicts new data, i.e. data that we did not use to fit the model?\n\n\nWhy separate calibration and validation data?\nIn model development we try many models and choose the one that fits that specific dataset very well.\nSo our model may be too complex and overfits the data. If we predict onto new data (in the real world) the model does not give plausible predictions.\nWhy might this be a problem?\n\nPredict the wrong ozone levels (and people with respiratory issues are not warned)\nPredict the wrong disease numbers (and local health services are not prepared)\nPredict the wrong crop yield (and farmers under/overapply fertiliser)\n\nPredictions can directly be used for decision-making, which has consequences.\n\n\nGeneral Idea\nWhat if we want to know how well the model predicts new data, i.e. data that we did not use to fit the model?\n\nWe build the model with one dataset (calibration).\nWe validate the model’s predictions with an independent dataset.\n\nIf the model is good, we expect the predictions to be close to the actual values.\nIf the model is bad, we expect the predictions to be far from the actual values.\n\nThe dataset can be obtained by:\n\nCollecting new data.\nSplitting the existing data into two parts before model building.\n\nData splitting\nCross-validation\n\n\n\n\n\nDefinitions\nSometimes the terms for calibration and validation can get muddled.\nBest practice:\n\nCalibration/Training Dataset: the data used to train the model\nValidation: the data used to fine tune the model (e.g. variable selection, hyperparameters in machine learning)\nTest: remaining data that has not been used in any kind of model training\n\nTo keep things simple (and if datasets are small), also common:\n\nCalibration/Training: the data used to train the model\nValidation/Test: the data used to assess the model’s prediction performance\n\n\n\nOur data\n\n\n\n\n\nDataset\n\n\nCollecting new data\n\n\n\n\nDataset (train)\n+\n\n\n\n\nNew dataset (test)\n\n\nThe best way to assess how well a model predicts new data is to collect new data.\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\n\n\n\nCollecting new data\nPros\n\nThe new data is completely independent of the data used to fit the model.\nMore data to fit and validate compared to data splitting.\n\nCons\n\nIt can be expensive and time-consuming to collect new data.\nSome data may be impossible to collect (e.g. historical data).\n\n\n\nData splitting\n\n\n\n\n\n\nSplit the existing dataset into training and test datasets (80:20, 70:30, 60:40, etc.)\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\n\n\nData splitting\n\n\n\n\n\n\nOnly possible for larger datasets (hundreds of observations)\nSplit the existing dataset into calibration, validation and test datasets (70:15:15, etc.)\n\nCalibration set: used to fit the model.\nValidation set: used to test model development (prevent overfitting).\nTest set: used to assess how well the model predicts new data.\n\n\n\nData splitting\nPros\n\nCompared to collecting new data, it is cheaper and faster to split existing data.\n\nCons\n\nWe have less data to fit the model and less data to validate the model.\nHow do we split the data? Randomly? By time? By location?\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1\nIteration 2\nIteration 3\nAnd so on…\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1 Iteration 2 Iteration 3 3-fold cross-validation Fold 1 Fold 2 Fold 3\n\nLike data splitting, where existing data is split into two parts:\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\nThe difference is that the splitting is done multiple times, and the model is fit and validated multiple times.\nEach iteration or fold is used for testing once.\n\n\n\nk-fold cross-validation\nPros\n\nSame as data splitting, but also:\n\nThe model is fit and validated multiple times, so we can get a better estimate of how well the model predicts new data.\nGreatly reduces overfitting as the model’s performance is not just a result of the particular way the data was split.\n\n\nCons\n\nBias in small datasets: each fold may contain too little data to provide a representative sample.\nEach fold fits a new model so it is not used for interpretation, only for prediction quality.\n\nComputationally more expensive.\n\n\n\n\nCross-validation\nk-fold cross-validation splits data in each fold randomly.\nIf there is some underlying structure to the data, consider:\n\nSpatial cross-validation (e.g. fields on a farm, each fold is one field)\nTemporal cross-validation (e.g. time series data, each fold is a time period)\nStratified cross-validation (e.g. each fold has the same proportion of each category)\n\n\n\nAssessing prediction quality\nHow ‘good’ are the predictions? Observed vs predicted.\n\n\nVisually\n\nPlot: observed (y_i) vs predicted (\\hat y) values\n\n\n\nCode\nggplot(data = airquality, aes(x = log(Ozone), y = pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\n\n\n\nWe can also use metrics to quantify how well the model predicts new data:\n\nHow close points are to the 1:1 line\nHow linear the relationship is between observed and predicted values\n\n\n\nError\nThe smaller the error, the better the model.\nMean error: the average difference between observed and predicted values.\n\nCan be positive or negative to indicate over- or under-estimation (a measure of bias)\n\nME = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\hat{y}_i (in y units)\nMean absolute error: the average (absolute) difference between observed and predicted values (residual).\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| (in y units)\nMean squared error: the average of the squared residuals\n\nSquared so positive and negative errors do not cancel each other out\nPenalises poor predictions more\n\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\nRoot mean squared error: the standard deviation of the residuals\n\nSquaring the error penalises poor predictions more\n\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} (in y units)\n\n\nLinearity\nThe more linear the relationship between observed and predicted values, the better the model.\nPearson’s correlation coefficient (r):\n\nA measure of the strength and direction of a linear relationship between two variables.\nRanges from -1 to 1, with 0 indicating no relationship and 1 indicating a perfect positive linear relationship.\n\nr = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{\\hat{y}})^2}}\nR2:\n\nThe proportion of variance explained by the variables in the model.\nWhen two variables are compared (e.g. observed vs predicted) it is the same as correlation squared.\nA value of 1 indicates a perfect linear relationship.\n\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} Lin’s concordance correlation coefficient (\\rho_c):\n\nA measure of agreement between two variables (based on covariance, variances, and difference in means).\nHow well the points fit the 1:1 line.\nA value of 0 indicates no agreement and 1 indicating perfect agreement.\n\nLCCC = \\frac{2\\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) + (\\mu_X - \\mu_Y)^2}\n\n\nLCCC vs r and R2\nLCCC combines error and linearity so it better measures the fit to the 1:1 line.\n\n\nCode\ndf &lt;- tibble(y = seq(0, 100, 5),\n  \"45 degree line | CCC = 1\" = seq(0, 100, 5)) %&gt;%\n  mutate(\"Location shift | CCC = 0.89\" = `45 degree line | CCC = 1` - 15) %&gt;%\n  mutate(\"Scale shift | CCC = 0.52\" = y / 2) %&gt;%\n  mutate(\"Location and scale shift | CCC = 0.67\" = y * 2 - 20)\n\n# pivot\ndf_long &lt;- df %&gt;%\n  pivot_longer(-1, values_to = \"x\") %&gt;%\n  mutate(name = factor(name, \n    levels = c(\"45 degree line | CCC = 1\",\n      \"Location shift | CCC = 0.89\",\n      \"Scale shift | CCC = 0.52\",\n      \"Location and scale shift | CCC = 0.67\")))\n\nggplot(df_long, aes(x, y)) +\n  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = \"grey\") +\n  facet_wrap(~name) +\n  geom_point() +\n  xlim(0, 100) +\n  labs(x = \"\", y = \"\") +\n  theme_classic() +\n  geom_blank() \n\n\n\n\n\nLimitations\nNo one metric is perfect.\nEach prediction below has an LCCC of 0.6.\nWadoux and Minasny 2024  :::{.callout-tip}\nUse multiple metrics to test prediction quality, and always plot the predicted vs observed.\n\n\n\n\n\nExample: Loyn dataset\nWe will go through several examples to practice data splitting, cross-validation, and model evaluation.\n\n\nAbout\nData on the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.\nThe predictor variables are:\n\nALT Altitude (m)\nYR.ISOL Year when the patch was isolated (years)\nGRAZE Grazing (coded 1-5 which is light to heavy)\nAREA Patch area (ha)\nDIST Distance to nearest patch (km)\nLDIST Distance to largest patch (km)\n\n\n\nCode\nloyn &lt;- read_csv(\"images/loyn.csv\")\n\n\n\n\nDataset splitting\nWe will split the data into training and test sets.\nAs the dataset is quite small, we will use a 80:20 split.\n\n\nCode\nset.seed(100)\nindexes &lt;- sample(1:nrow(loyn), size = 0.2 * nrow(loyn)) # randomly sample 20% of rows in the dataset\nloyn_train &lt;- loyn[-indexes, ] # remove the 20% - training dataset\nloyn_test &lt;- loyn[indexes, ] # select the 20% - test dataset\n\n\n\n\nChecking the split\nCheck out the str() of the data to see if the split worked (number of observations).\n\n\nCode\nstr(loyn_train)\n\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ AREA   : num [1:45] 0.1 0.5 0.5 1 1 1 1 1 2 2 ...\n $ YR.ISOL: num [1:45] 1968 1920 1900 1966 1918 ...\n $ DIST   : num [1:45] 39 234 104 66 246 467 284 156 66 93 ...\n $ LDIST  : num [1:45] 39 234 311 66 246 ...\n $ GRAZE  : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT    : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n\n\nCode\nstr(loyn_test)\n\n\ntibble [11 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:11] 3 29.5 26 39.6 34.4 19.5 14.6 28.3 15.8 5 ...\n $ AREA   : num [1:11] 1 973 18 49 96 6 2 34 5 4 ...\n $ YR.ISOL: num [1:11] 1900 1970 1966 1972 1976 ...\n $ DIST   : num [1:11] 311 337 40 1427 39 ...\n $ LDIST  : num [1:11] 571 1323 3188 1557 519 ...\n $ GRAZE  : num [1:11] 5 1 2 1 2 3 1 1 3 5 ...\n $ ALT    : num [1:11] 130 190 190 180 175 170 210 110 130 120 ...\n\n\n\n\n\nModel development\nFrom now on, we will work with the training set only.\n\n\nExploratory data analysis\n\nThe next step is to visualise the data.\nExpore relationships between the predictors and the response via histograms, scatterplots, boxplots, correlations etc.\n\nIn this lecture we will just look at histograms.\n\n\nHistograms\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n\n\n\n\nLooks like AREA LDIST and DIST are skewed – we will transform them so that they are more normally distributed.\n\n\n\nTransforming predictors\nWe will use log10() to transform the predictors. The mutate() function from the dplyr package is useful for this as it can create new columns in the data frame with the transformed values.\n\n\nCode\nloyn_train &lt;- loyn_train %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    )\n\n\nThen, remove the untransformed variables from the dataset. Here we can use the select() function from the dplyr package to “delselect” columns by using the - sign.\n\n\nCode\nloyn_train &lt;- loyn_train %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nstr(loyn_train)\n\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND    : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ YR.ISOL  : num [1:45] 1968 1920 1900 1966 1918 ...\n $ GRAZE    : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT      : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n $ AREA_L10 : num [1:45] -1 -0.301 -0.301 0 0 ...\n $ LDIST_L10: num [1:45] 1.59 2.37 2.49 1.82 2.39 ...\n $ DIST_L10 : num [1:45] 1.59 2.37 2.02 1.82 2.39 ...\n\n\n\n\nFinal inspection\nView the histograms again to check that the transformation worked.\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n\n\n\n\n\nFull model\nWe start with a full model that includes all the predictors.\n\n\nCode\nfull_fit &lt;- lm(ABUND ~ ., data = loyn_train)\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n\n\n\n\nAssumptions - Round 1\nAs usual, we should check the assumptions of the model (CLINE + outliers). We will use the check_model() function from the perfomance package (because it looks nice and has interpretation instructions).\n\n\nCode\nperformance::check_model(full_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\"))\n\n\n\n\n\nAssumptions - Round 1\nWe check multicollinearity with variable inflation factors (VIF) - VIFs are all &lt; 10, so there is no multicollinearity. All assumptions are thus met.\n\n\nCode\ncheck_model(full_fit, check = c(\"vif\"))\n\n\n\n\n\nBackwards stepwise selection\nUse the step() function perform backwards stepwise selection. This function uses AIC to select the best model.\nDepending on the dataset splitting, the best model may be different each time we randomly sample the data. In this case we should all have the same results as we set the seed.\nIf we compare to the full model, the adjusted r-squared is slightly higher, and the AIC is lower.\n\n\nCode\nstep_fit &lt;- step(full_fit, direction = \"backward\")\n\n\nStart:  AIC=174.81\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- ALT        1     14.67 1618.5 173.22\n- LDIST_L10  1     16.92 1620.8 173.28\n- DIST_L10   1     69.18 1673.0 174.71\n&lt;none&gt;                   1603.9 174.81\n- GRAZE      1     78.00 1681.9 174.94\n- YR.ISOL    1    126.48 1730.3 176.22\n- AREA_L10   1    867.44 2471.3 192.26\n\nStep:  AIC=173.22\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1     10.76 1629.3 171.52\n&lt;none&gt;                   1618.5 173.22\n- DIST_L10   1     85.56 1704.1 173.54\n- GRAZE      1     98.23 1716.8 173.87\n- YR.ISOL    1    117.80 1736.3 174.38\n- AREA_L10   1   1088.05 2706.6 194.35\n\nStep:  AIC=171.52\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n&lt;none&gt;                  1629.3 171.52\n- GRAZE     1     93.97 1723.3 172.04\n- YR.ISOL   1    107.73 1737.0 172.40\n- DIST_L10  1    114.60 1743.9 172.57\n- AREA_L10  1   1161.66 2791.0 193.74\n\n\n\n\nThe selected model\n\n\n\n\nCode\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n\n\n\n\n\nCode\nsummary(step_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10, data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1683  -3.1961   0.3374   3.4834  14.2021 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -135.17508  102.72850  -1.316    0.196    \nYR.ISOL        0.08323    0.05118   1.626    0.112    \nGRAZE         -1.50496    0.99082  -1.519    0.137    \nAREA_L10       8.61888    1.61392   5.340 3.98e-06 ***\nDIST_L10      -4.87726    2.90770  -1.677    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.382 on 40 degrees of freedom\nMultiple R-squared:  0.6701,    Adjusted R-squared:  0.6371 \nF-statistic: 20.31 on 4 and 40 DF,  p-value: 3.365e-09\n\n\n\nAssumptions - Round 2\n\n\nCode\ncheck_model(step_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\", \"vif\"))\n\n\n\n\nModel validation\nIt looks like the model is good, so let’s bring in the test set to see how it performs!\n\n\nPrepare the test data\nSince the test data has not been transformed, we need to do that first.\nWe then predict onto the training and test dataset using the reduced stepwise model.\n\n\nCode\nloyn_test &lt;- loyn_test %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    ) %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nloyn_train$pred &lt;- predict(step_fit, newdata = loyn_train)\nloyn_test$pred &lt;- predict(step_fit, newdata = loyn_test)\n\n\n\n\nPlotting observed vs predicted\n\n\nCode\np1 &lt;- ggplot(loyn_train, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Training\") +\n  theme_classic()\n\np2 &lt;- ggplot(loyn_test, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Test\") +\n  theme_classic()\n\np1 + p2\n\n\n\n\n\nCalculating metrics - error\nGenerally the training dataset will have lower error and higher linearity than the test dataset. If this difference is very large – it suggests the model is not applicable to the test data and overfitting.\nWith error – lower is better. The following all measure error in the same units as the response variable (number of birds in each forest patch).\nMean error\n\n\nCode\nmean(loyn_train$ABUND - loyn_train$pred) |&gt; round(2)\n\n\n[1] 0\n\n\nCode\nmean(loyn_test$ABUND - loyn_test$pred) |&gt; round(2)\n\n\n[1] -1.65\n\n\nWe expect the ME for the training dataset to be near 0 – a well-fitted linear regression model will have positive and negative residuals balance each other out.\nMean absolute error\n\n\nCode\nmean(abs(loyn_train$ABUND - loyn_train$pred)) |&gt; round(2)\n\n\n[1] 4.43\n\n\nCode\nmean(abs(loyn_test$ABUND - loyn_test$pred)) |&gt; round(2)\n\n\n[1] 5.21\n\n\nRoot mean squared error (caret package)\n\n\nCode\nRMSE(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 6.02\n\n\nCode\nRMSE(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 6.72\n\n\n\n\nCalculating metrics – linearity\nWith linearity – higher is better.\nBoth training and test datasets perform similarly, which is a good sign the model is not overfitting.\nPearson’s correlation coefficient r\n\n\nCode\ncor(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nCode\ncor(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nR2\nCan either square the correlation coefficient or use the R2() function from the caret package.\n\n\nCode\nR2(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.67\n\n\nCode\nR2(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.68\n\n\nLin’s concordance correlation coefficient (CCC) (epiR package)\n\n\nCode\nepi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.8\n\n\nCode\nepi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.81\n\n\n\n\nConclusions\n\n\nCode\n# put all data into a tible and kable it\ntibble(\n    Dataset = c(\"Training\", \"Test\"),\n    ME = c(\n        mean(loyn_train$ABUND - loyn_train$pred),\n        mean(loyn_test$ABUND - loyn_test$pred)\n    ),\n    MAE = c(\n        mean(abs(loyn_train$ABUND - loyn_train$pred)),\n        mean(abs(loyn_test$ABUND - loyn_test$pred))\n    ),\n    RMSE = c(\n        RMSE(loyn_train$ABUND, loyn_train$pred),\n        RMSE(loyn_test$ABUND, loyn_test$pred)\n    ),\n    cor = c(\n        cor(loyn_train$ABUND, loyn_train$pred),\n        cor(loyn_test$ABUND, loyn_test$pred)\n    ),\n    R2 = c(\n        R2(loyn_train$ABUND, loyn_train$pred),\n        R2(loyn_test$ABUND, loyn_test$pred)\n    ),\n    LCCC = c(\n        epi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est,\n        epi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est\n    )\n) %&gt;%\n    knitr::kable(digits = 2)\n\n\n\n\n\nDataset\nME\nMAE\nRMSE\ncor\nR2\nLCCC\n\n\n\n\nTraining\n0.00\n4.43\n6.02\n0.82\n0.67\n0.80\n\n\nTest\n-1.65\n5.21\n6.72\n0.82\n0.68\n0.81\n\n\n\n\n\n\nThe model fit for the training dataset is marginally better (slight overfitting, but not a concern)\nSmall differences are also expected due to the small sample size or the chosen set.seed()\nThe model predicts bird abundance in forest patches in SE Victoria well (r = 0.82, LCCC = 0.81, MAE = 5.21 birds/patch, RMSE = 6.72 birds/patch).\n\n\n\nThanks!\nQuestions? Comments?\nSlides made with Quarto"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#why-separate-calibration-and-validation-data",
    "href": "lectures/L09/Lecture-09.html#why-separate-calibration-and-validation-data",
    "title": "Regression: predictive modelling",
    "section": "Why separate calibration and validation data?",
    "text": "Why separate calibration and validation data?\nIn model development we try many models and choose the one that fits that specific dataset very well.\nSo our model may be too complex and overfits the data. If we predict onto new data (in the real world) the model does not give plausible predictions.\nWhy might this be a problem?\n\nPredict the wrong ozone levels (and people with respiratory issues are not warned)\nPredict the wrong disease numbers (and local health services are not prepared)\nPredict the wrong crop yield (and farmers under/overapply fertiliser)\n\nPredictions can directly be used for decision-making, which has consequences."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#general-idea",
    "href": "lectures/L09/Lecture-09.html#general-idea",
    "title": "Regression: predictive modelling",
    "section": "General Idea",
    "text": "General Idea\nWhat if we want to know how well the model predicts new data, i.e. data that we did not use to fit the model?\n\nWe build the model with one dataset (calibration).\nWe validate the model’s predictions with an independent dataset.\n\nIf the model is good, we expect the predictions to be close to the actual values.\nIf the model is bad, we expect the predictions to be far from the actual values.\n\nThe dataset can be obtained by:\n\nCollecting new data.\nSplitting the existing data into two parts before model building.\n\nData splitting\nCross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#definitions",
    "href": "lectures/L09/Lecture-09.html#definitions",
    "title": "Regression: predictive modelling",
    "section": "Definitions",
    "text": "Definitions\nSometimes the terms for calibration and validation can get muddled.\nBest practice:\n\nCalibration/Training Dataset: the data used to train the model\nValidation: the data used to fine tune the model (e.g. variable selection, hyperparameters in machine learning)\nTest: remaining data that has not been used in any kind of model training\n\nTo keep things simple (and if datasets are small), also common:\n\nCalibration/Training: the data used to train the model\nValidation/Test: the data used to assess the model’s prediction performance"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#our-data",
    "href": "lectures/L09/Lecture-09.html#our-data",
    "title": "Regression: predictive modelling",
    "section": "Our data",
    "text": "Our data\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#collecting-new-data",
    "href": "lectures/L09/Lecture-09.html#collecting-new-data",
    "title": "Regression: predictive modelling",
    "section": "Collecting new data",
    "text": "Collecting new data\n\n\n\n\nDataset (train)\n+\n\n\n\n\nNew dataset (test)\n\n\nThe best way to assess how well a model predicts new data is to collect new data.\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#collecting-new-data-1",
    "href": "lectures/L09/Lecture-09.html#collecting-new-data-1",
    "title": "Regression: predictive modelling",
    "section": "Collecting new data",
    "text": "Collecting new data\nPros\n\nThe new data is completely independent of the data used to fit the model.\nMore data to fit and validate compared to data splitting.\n\nCons\n\nIt can be expensive and time-consuming to collect new data.\nSome data may be impossible to collect (e.g. historical data)."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting",
    "href": "lectures/L09/Lecture-09.html#data-splitting",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\n\n\n\n\n\n\nSplit the existing dataset into training and test datasets (80:20, 70:30, 60:40, etc.)\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\n\n\nData splitting\n\n\n\n\n\n\nOnly possible for larger datasets (hundreds of observations)\nSplit the existing dataset into calibration, validation and test datasets (70:15:15, etc.)\n\nCalibration set: used to fit the model.\nValidation set: used to test model development (prevent overfitting).\nTest set: used to assess how well the model predicts new data.\n\n\n\nData splitting\nPros\n\nCompared to collecting new data, it is cheaper and faster to split existing data.\n\nCons\n\nWe have less data to fit the model and less data to validate the model.\nHow do we split the data? Randomly? By time? By location?\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1\nIteration 2\nIteration 3\nAnd so on…\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1 Iteration 2 Iteration 3 3-fold cross-validation Fold 1 Fold 2 Fold 3\n\nLike data splitting, where existing data is split into two parts:\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\nThe difference is that the splitting is done multiple times, and the model is fit and validated multiple times.\nEach iteration or fold is used for testing once.\n\n\n\nk-fold cross-validation\nPros\n\nSame as data splitting, but also:\n\nThe model is fit and validated multiple times, so we can get a better estimate of how well the model predicts new data.\nGreatly reduces overfitting as the model’s performance is not just a result of the particular way the data was split.\n\n\nCons\n\nBias in small datasets: each fold may contain too little data to provide a representative sample.\nEach fold fits a new model so it is not used for interpretation, only for prediction quality.\n\nComputationally more expensive.\n\n\n\n\nCross-validation\nk-fold cross-validation splits data in each fold randomly.\nIf there is some underlying structure to the data, consider:\n\nSpatial cross-validation (e.g. fields on a farm, each fold is one field)\nTemporal cross-validation (e.g. time series data, each fold is a time period)\nStratified cross-validation (e.g. each fold has the same proportion of each category)\n\n\n\nAssessing prediction quality\nHow ‘good’ are the predictions? Observed vs predicted.\n\n\nVisually\n\nPlot: observed (y_i) vs predicted (\\hat y) values\n\n\n\nCode\nggplot(data = airquality, aes(x = log(Ozone), y = pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\n\n\n\nWe can also use metrics to quantify how well the model predicts new data:\n\nHow close points are to the 1:1 line\nHow linear the relationship is between observed and predicted values\n\n\n\nError\nThe smaller the error, the better the model.\nMean error: the average difference between observed and predicted values.\n\nCan be positive or negative to indicate over- or under-estimation (a measure of bias)\n\nME = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\hat{y}_i (in y units)\nMean absolute error: the average (absolute) difference between observed and predicted values (residual).\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| (in y units)\nMean squared error: the average of the squared residuals\n\nSquared so positive and negative errors do not cancel each other out\nPenalises poor predictions more\n\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\nRoot mean squared error: the standard deviation of the residuals\n\nSquaring the error penalises poor predictions more\n\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} (in y units)\n\n\nLinearity\nThe more linear the relationship between observed and predicted values, the better the model.\nPearson’s correlation coefficient (r):\n\nA measure of the strength and direction of a linear relationship between two variables.\nRanges from -1 to 1, with 0 indicating no relationship and 1 indicating a perfect positive linear relationship.\n\nr = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{\\hat{y}})^2}}\nR2:\n\nThe proportion of variance explained by the variables in the model.\nWhen two variables are compared (e.g. observed vs predicted) it is the same as correlation squared.\nA value of 1 indicates a perfect linear relationship.\n\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} Lin’s concordance correlation coefficient (\\rho_c):\n\nA measure of agreement between two variables (based on covariance, variances, and difference in means).\nHow well the points fit the 1:1 line.\nA value of 0 indicates no agreement and 1 indicating perfect agreement.\n\nLCCC = \\frac{2\\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) + (\\mu_X - \\mu_Y)^2}\n\n\nLCCC vs r and R2\nLCCC combines error and linearity so it better measures the fit to the 1:1 line.\n\n\nCode\ndf &lt;- tibble(y = seq(0, 100, 5),\n  \"45 degree line | CCC = 1\" = seq(0, 100, 5)) %&gt;%\n  mutate(\"Location shift | CCC = 0.89\" = `45 degree line | CCC = 1` - 15) %&gt;%\n  mutate(\"Scale shift | CCC = 0.52\" = y / 2) %&gt;%\n  mutate(\"Location and scale shift | CCC = 0.67\" = y * 2 - 20)\n\n# pivot\ndf_long &lt;- df %&gt;%\n  pivot_longer(-1, values_to = \"x\") %&gt;%\n  mutate(name = factor(name, \n    levels = c(\"45 degree line | CCC = 1\",\n      \"Location shift | CCC = 0.89\",\n      \"Scale shift | CCC = 0.52\",\n      \"Location and scale shift | CCC = 0.67\")))\n\nggplot(df_long, aes(x, y)) +\n  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = \"grey\") +\n  facet_wrap(~name) +\n  geom_point() +\n  xlim(0, 100) +\n  labs(x = \"\", y = \"\") +\n  theme_classic() +\n  geom_blank() \n\n\n\n\n\nLimitations\nNo one metric is perfect.\nEach prediction below has an LCCC of 0.6.\nWadoux and Minasny 2024  :::{.callout-tip}\nUse multiple metrics to test prediction quality, and always plot the predicted vs observed.\n\n\n\n\n\nExample: Loyn dataset\nWe will go through several examples to practice data splitting, cross-validation, and model evaluation.\n\n\nAbout\nData on the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.\nThe predictor variables are:\n\nALT Altitude (m)\nYR.ISOL Year when the patch was isolated (years)\nGRAZE Grazing (coded 1-5 which is light to heavy)\nAREA Patch area (ha)\nDIST Distance to nearest patch (km)\nLDIST Distance to largest patch (km)\n\n\n\nCode\nloyn &lt;- read_csv(\"images/loyn.csv\")\n\n\n\n\nDataset splitting\nWe will split the data into training and test sets.\nAs the dataset is quite small, we will use a 80:20 split.\n\n\nCode\nset.seed(100)\nindexes &lt;- sample(1:nrow(loyn), size = 0.2 * nrow(loyn)) # randomly sample 20% of rows in the dataset\nloyn_train &lt;- loyn[-indexes, ] # remove the 20% - training dataset\nloyn_test &lt;- loyn[indexes, ] # select the 20% - test dataset\n\n\n\n\nChecking the split\nCheck out the str() of the data to see if the split worked (number of observations).\n\n\nCode\nstr(loyn_train)\n\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ AREA   : num [1:45] 0.1 0.5 0.5 1 1 1 1 1 2 2 ...\n $ YR.ISOL: num [1:45] 1968 1920 1900 1966 1918 ...\n $ DIST   : num [1:45] 39 234 104 66 246 467 284 156 66 93 ...\n $ LDIST  : num [1:45] 39 234 311 66 246 ...\n $ GRAZE  : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT    : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n\n\nCode\nstr(loyn_test)\n\n\ntibble [11 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:11] 3 29.5 26 39.6 34.4 19.5 14.6 28.3 15.8 5 ...\n $ AREA   : num [1:11] 1 973 18 49 96 6 2 34 5 4 ...\n $ YR.ISOL: num [1:11] 1900 1970 1966 1972 1976 ...\n $ DIST   : num [1:11] 311 337 40 1427 39 ...\n $ LDIST  : num [1:11] 571 1323 3188 1557 519 ...\n $ GRAZE  : num [1:11] 5 1 2 1 2 3 1 1 3 5 ...\n $ ALT    : num [1:11] 130 190 190 180 175 170 210 110 130 120 ...\n\n\n\n\n\nModel development\nFrom now on, we will work with the training set only.\n\n\nExploratory data analysis\n\nThe next step is to visualise the data.\nExpore relationships between the predictors and the response via histograms, scatterplots, boxplots, correlations etc.\n\nIn this lecture we will just look at histograms.\n\n\nHistograms\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n\n\n\n\nLooks like AREA LDIST and DIST are skewed – we will transform them so that they are more normally distributed.\n\n\n\nTransforming predictors\nWe will use log10() to transform the predictors. The mutate() function from the dplyr package is useful for this as it can create new columns in the data frame with the transformed values.\n\n\nCode\nloyn_train &lt;- loyn_train %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    )\n\n\nThen, remove the untransformed variables from the dataset. Here we can use the select() function from the dplyr package to “delselect” columns by using the - sign.\n\n\nCode\nloyn_train &lt;- loyn_train %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nstr(loyn_train)\n\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND    : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ YR.ISOL  : num [1:45] 1968 1920 1900 1966 1918 ...\n $ GRAZE    : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT      : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n $ AREA_L10 : num [1:45] -1 -0.301 -0.301 0 0 ...\n $ LDIST_L10: num [1:45] 1.59 2.37 2.49 1.82 2.39 ...\n $ DIST_L10 : num [1:45] 1.59 2.37 2.02 1.82 2.39 ...\n\n\n\n\nFinal inspection\nView the histograms again to check that the transformation worked.\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n\n\n\n\n\nFull model\nWe start with a full model that includes all the predictors.\n\n\nCode\nfull_fit &lt;- lm(ABUND ~ ., data = loyn_train)\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n\n\n\n\nAssumptions - Round 1\nAs usual, we should check the assumptions of the model (CLINE + outliers). We will use the check_model() function from the perfomance package (because it looks nice and has interpretation instructions).\n\n\nCode\nperformance::check_model(full_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\"))\n\n\n\n\n\nAssumptions - Round 1\nWe check multicollinearity with variable inflation factors (VIF) - VIFs are all &lt; 10, so there is no multicollinearity. All assumptions are thus met.\n\n\nCode\ncheck_model(full_fit, check = c(\"vif\"))\n\n\n\n\n\nBackwards stepwise selection\nUse the step() function perform backwards stepwise selection. This function uses AIC to select the best model.\nDepending on the dataset splitting, the best model may be different each time we randomly sample the data. In this case we should all have the same results as we set the seed.\nIf we compare to the full model, the adjusted r-squared is slightly higher, and the AIC is lower.\n\n\nCode\nstep_fit &lt;- step(full_fit, direction = \"backward\")\n\n\nStart:  AIC=174.81\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- ALT        1     14.67 1618.5 173.22\n- LDIST_L10  1     16.92 1620.8 173.28\n- DIST_L10   1     69.18 1673.0 174.71\n&lt;none&gt;                   1603.9 174.81\n- GRAZE      1     78.00 1681.9 174.94\n- YR.ISOL    1    126.48 1730.3 176.22\n- AREA_L10   1    867.44 2471.3 192.26\n\nStep:  AIC=173.22\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1     10.76 1629.3 171.52\n&lt;none&gt;                   1618.5 173.22\n- DIST_L10   1     85.56 1704.1 173.54\n- GRAZE      1     98.23 1716.8 173.87\n- YR.ISOL    1    117.80 1736.3 174.38\n- AREA_L10   1   1088.05 2706.6 194.35\n\nStep:  AIC=171.52\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n&lt;none&gt;                  1629.3 171.52\n- GRAZE     1     93.97 1723.3 172.04\n- YR.ISOL   1    107.73 1737.0 172.40\n- DIST_L10  1    114.60 1743.9 172.57\n- AREA_L10  1   1161.66 2791.0 193.74\n\n\n\n\nThe selected model\n\n\n\n\nCode\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n\n\n\n\n\nCode\nsummary(step_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10, data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1683  -3.1961   0.3374   3.4834  14.2021 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -135.17508  102.72850  -1.316    0.196    \nYR.ISOL        0.08323    0.05118   1.626    0.112    \nGRAZE         -1.50496    0.99082  -1.519    0.137    \nAREA_L10       8.61888    1.61392   5.340 3.98e-06 ***\nDIST_L10      -4.87726    2.90770  -1.677    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.382 on 40 degrees of freedom\nMultiple R-squared:  0.6701,    Adjusted R-squared:  0.6371 \nF-statistic: 20.31 on 4 and 40 DF,  p-value: 3.365e-09\n\n\n\nAssumptions - Round 2\n\n\nCode\ncheck_model(step_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\", \"vif\"))\n\n\n\n\nModel validation\nIt looks like the model is good, so let’s bring in the test set to see how it performs!\n\n\nPrepare the test data\nSince the test data has not been transformed, we need to do that first.\nWe then predict onto the training and test dataset using the reduced stepwise model.\n\n\nCode\nloyn_test &lt;- loyn_test %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    ) %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nloyn_train$pred &lt;- predict(step_fit, newdata = loyn_train)\nloyn_test$pred &lt;- predict(step_fit, newdata = loyn_test)\n\n\n\n\nPlotting observed vs predicted\n\n\nCode\np1 &lt;- ggplot(loyn_train, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Training\") +\n  theme_classic()\n\np2 &lt;- ggplot(loyn_test, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Test\") +\n  theme_classic()\n\np1 + p2\n\n\n\n\n\nCalculating metrics - error\nGenerally the training dataset will have lower error and higher linearity than the test dataset. If this difference is very large – it suggests the model is not applicable to the test data and overfitting.\nWith error – lower is better. The following all measure error in the same units as the response variable (number of birds in each forest patch).\nMean error\n\n\nCode\nmean(loyn_train$ABUND - loyn_train$pred) |&gt; round(2)\n\n\n[1] 0\n\n\nCode\nmean(loyn_test$ABUND - loyn_test$pred) |&gt; round(2)\n\n\n[1] -1.65\n\n\nWe expect the ME for the training dataset to be near 0 – a well-fitted linear regression model will have positive and negative residuals balance each other out.\nMean absolute error\n\n\nCode\nmean(abs(loyn_train$ABUND - loyn_train$pred)) |&gt; round(2)\n\n\n[1] 4.43\n\n\nCode\nmean(abs(loyn_test$ABUND - loyn_test$pred)) |&gt; round(2)\n\n\n[1] 5.21\n\n\nRoot mean squared error (caret package)\n\n\nCode\nRMSE(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 6.02\n\n\nCode\nRMSE(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 6.72\n\n\n\n\nCalculating metrics – linearity\nWith linearity – higher is better.\nBoth training and test datasets perform similarly, which is a good sign the model is not overfitting.\nPearson’s correlation coefficient r\n\n\nCode\ncor(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nCode\ncor(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nR2\nCan either square the correlation coefficient or use the R2() function from the caret package.\n\n\nCode\nR2(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.67\n\n\nCode\nR2(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.68\n\n\nLin’s concordance correlation coefficient (CCC) (epiR package)\n\n\nCode\nepi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.8\n\n\nCode\nepi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.81\n\n\n\n\nConclusions\n\n\nCode\n# put all data into a tible and kable it\ntibble(\n    Dataset = c(\"Training\", \"Test\"),\n    ME = c(\n        mean(loyn_train$ABUND - loyn_train$pred),\n        mean(loyn_test$ABUND - loyn_test$pred)\n    ),\n    MAE = c(\n        mean(abs(loyn_train$ABUND - loyn_train$pred)),\n        mean(abs(loyn_test$ABUND - loyn_test$pred))\n    ),\n    RMSE = c(\n        RMSE(loyn_train$ABUND, loyn_train$pred),\n        RMSE(loyn_test$ABUND, loyn_test$pred)\n    ),\n    cor = c(\n        cor(loyn_train$ABUND, loyn_train$pred),\n        cor(loyn_test$ABUND, loyn_test$pred)\n    ),\n    R2 = c(\n        R2(loyn_train$ABUND, loyn_train$pred),\n        R2(loyn_test$ABUND, loyn_test$pred)\n    ),\n    LCCC = c(\n        epi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est,\n        epi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est\n    )\n) %&gt;%\n    knitr::kable(digits = 2)\n\n\n\n\n\nDataset\nME\nMAE\nRMSE\ncor\nR2\nLCCC\n\n\n\n\nTraining\n0.00\n4.43\n6.02\n0.82\n0.67\n0.80\n\n\nTest\n-1.65\n5.21\n6.72\n0.82\n0.68\n0.81\n\n\n\n\n\n\nThe model fit for the training dataset is marginally better (slight overfitting, but not a concern)\nSmall differences are also expected due to the small sample size or the chosen set.seed()\nThe model predicts bird abundance in forest patches in SE Victoria well (r = 0.82, LCCC = 0.81, MAE = 5.21 birds/patch, RMSE = 6.72 birds/patch).\n\n\n\nThanks!\nQuestions? Comments?\nSlides made with Quarto"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting-1",
    "href": "lectures/L09/Lecture-09.html#data-splitting-1",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\n\n\n\n\n\n\nOnly possible for larger datasets (hundreds of observations)\nSplit the existing dataset into calibration, validation and test datasets (70:15:15, etc.)\n\nCalibration set: used to fit the model.\nValidation set: used to test model development (prevent overfitting).\nTest set: used to assess how well the model predicts new data.\n\n\n\nData splitting\nPros\n\nCompared to collecting new data, it is cheaper and faster to split existing data.\n\nCons\n\nWe have less data to fit the model and less data to validate the model.\nHow do we split the data? Randomly? By time? By location?\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1\nIteration 2\nIteration 3\nAnd so on…\n\n\nk-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1 Iteration 2 Iteration 3 3-fold cross-validation Fold 1 Fold 2 Fold 3\n\nLike data splitting, where existing data is split into two parts:\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\nThe difference is that the splitting is done multiple times, and the model is fit and validated multiple times.\nEach iteration or fold is used for testing once.\n\n\n\nk-fold cross-validation\nPros\n\nSame as data splitting, but also:\n\nThe model is fit and validated multiple times, so we can get a better estimate of how well the model predicts new data.\nGreatly reduces overfitting as the model’s performance is not just a result of the particular way the data was split.\n\n\nCons\n\nBias in small datasets: each fold may contain too little data to provide a representative sample.\nEach fold fits a new model so it is not used for interpretation, only for prediction quality.\n\nComputationally more expensive.\n\n\n\n\nCross-validation\nk-fold cross-validation splits data in each fold randomly.\nIf there is some underlying structure to the data, consider:\n\nSpatial cross-validation (e.g. fields on a farm, each fold is one field)\nTemporal cross-validation (e.g. time series data, each fold is a time period)\nStratified cross-validation (e.g. each fold has the same proportion of each category)\n\n\n\nAssessing prediction quality\nHow ‘good’ are the predictions? Observed vs predicted.\n\n\nVisually\n\nPlot: observed (y_i) vs predicted (\\hat y) values\n\n\n\nCode\nggplot(data = airquality, aes(x = log(Ozone), y = pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\n\n\n\nWe can also use metrics to quantify how well the model predicts new data:\n\nHow close points are to the 1:1 line\nHow linear the relationship is between observed and predicted values\n\n\n\nError\nThe smaller the error, the better the model.\nMean error: the average difference between observed and predicted values.\n\nCan be positive or negative to indicate over- or under-estimation (a measure of bias)\n\nME = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\hat{y}_i (in y units)\nMean absolute error: the average (absolute) difference between observed and predicted values (residual).\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| (in y units)\nMean squared error: the average of the squared residuals\n\nSquared so positive and negative errors do not cancel each other out\nPenalises poor predictions more\n\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\nRoot mean squared error: the standard deviation of the residuals\n\nSquaring the error penalises poor predictions more\n\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} (in y units)\n\n\nLinearity\nThe more linear the relationship between observed and predicted values, the better the model.\nPearson’s correlation coefficient (r):\n\nA measure of the strength and direction of a linear relationship between two variables.\nRanges from -1 to 1, with 0 indicating no relationship and 1 indicating a perfect positive linear relationship.\n\nr = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{\\hat{y}})^2}}\nR2:\n\nThe proportion of variance explained by the variables in the model.\nWhen two variables are compared (e.g. observed vs predicted) it is the same as correlation squared.\nA value of 1 indicates a perfect linear relationship.\n\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} Lin’s concordance correlation coefficient (\\rho_c):\n\nA measure of agreement between two variables (based on covariance, variances, and difference in means).\nHow well the points fit the 1:1 line.\nA value of 0 indicates no agreement and 1 indicating perfect agreement.\n\nLCCC = \\frac{2\\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) + (\\mu_X - \\mu_Y)^2}\n\n\nLCCC vs r and R2\nLCCC combines error and linearity so it better measures the fit to the 1:1 line.\n\n\nCode\ndf &lt;- tibble(y = seq(0, 100, 5),\n  \"45 degree line | CCC = 1\" = seq(0, 100, 5)) %&gt;%\n  mutate(\"Location shift | CCC = 0.89\" = `45 degree line | CCC = 1` - 15) %&gt;%\n  mutate(\"Scale shift | CCC = 0.52\" = y / 2) %&gt;%\n  mutate(\"Location and scale shift | CCC = 0.67\" = y * 2 - 20)\n\n# pivot\ndf_long &lt;- df %&gt;%\n  pivot_longer(-1, values_to = \"x\") %&gt;%\n  mutate(name = factor(name, \n    levels = c(\"45 degree line | CCC = 1\",\n      \"Location shift | CCC = 0.89\",\n      \"Scale shift | CCC = 0.52\",\n      \"Location and scale shift | CCC = 0.67\")))\n\nggplot(df_long, aes(x, y)) +\n  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = \"grey\") +\n  facet_wrap(~name) +\n  geom_point() +\n  xlim(0, 100) +\n  labs(x = \"\", y = \"\") +\n  theme_classic() +\n  geom_blank() \n\n\n\n\n\nLimitations\nNo one metric is perfect.\nEach prediction below has an LCCC of 0.6.\nWadoux and Minasny 2024  :::{.callout-tip}\nUse multiple metrics to test prediction quality, and always plot the predicted vs observed."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting-2",
    "href": "lectures/L09/Lecture-09.html#data-splitting-2",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\nPros\n\nCompared to collecting new data, it is cheaper and faster to split existing data.\n\nCons\n\nWe have less data to fit the model and less data to validate the model.\nHow do we split the data? Randomly? By time? By location?"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-1",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-1",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-2",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-2",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-3",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-3",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1\nIteration 2\nIteration 3\nAnd so on…"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-4",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-4",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1 Iteration 2 Iteration 3 3-fold cross-validation Fold 1 Fold 2 Fold 3\n\nLike data splitting, where existing data is split into two parts:\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\nThe difference is that the splitting is done multiple times, and the model is fit and validated multiple times.\nEach iteration or fold is used for testing once."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-5",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-5",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation\nPros\n\nSame as data splitting, but also:\n\nThe model is fit and validated multiple times, so we can get a better estimate of how well the model predicts new data.\nGreatly reduces overfitting as the model’s performance is not just a result of the particular way the data was split.\n\n\nCons\n\nBias in small datasets: each fold may contain too little data to provide a representative sample.\nEach fold fits a new model so it is not used for interpretation, only for prediction quality.\n\nComputationally more expensive."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#cross-validation",
    "href": "lectures/L09/Lecture-09.html#cross-validation",
    "title": "Regression: predictive modelling",
    "section": "Cross-validation",
    "text": "Cross-validation\nk-fold cross-validation splits data in each fold randomly.\nIf there is some underlying structure to the data, consider:\n\nSpatial cross-validation (e.g. fields on a farm, each fold is one field)\nTemporal cross-validation (e.g. time series data, each fold is a time period)\nStratified cross-validation (e.g. each fold has the same proportion of each category)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#visually",
    "href": "lectures/L09/Lecture-09.html#visually",
    "title": "Regression: predictive modelling",
    "section": "Visually",
    "text": "Visually\n\nPlot: observed (y_i) vs predicted (\\hat y) values\n\n\n\nCode\nggplot(data = airquality, aes(x = log(Ozone), y = pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\n\n\n\nWe can also use metrics to quantify how well the model predicts new data:\n\nHow close points are to the 1:1 line\nHow linear the relationship is between observed and predicted values"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#error",
    "href": "lectures/L09/Lecture-09.html#error",
    "title": "Regression: predictive modelling",
    "section": "Error",
    "text": "Error\nThe smaller the error, the better the model.\nMean error: the average difference between observed and predicted values.\n\nCan be positive or negative to indicate over- or under-estimation (a measure of bias)\n\nME = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\hat{y}_i (in y units)\nMean absolute error: the average (absolute) difference between observed and predicted values (residual).\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| (in y units)\nMean squared error: the average of the squared residuals\n\nSquared so positive and negative errors do not cancel each other out\nPenalises poor predictions more\n\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\nRoot mean squared error: the standard deviation of the residuals\n\nSquaring the error penalises poor predictions more\n\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} (in y units)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#linearity",
    "href": "lectures/L09/Lecture-09.html#linearity",
    "title": "Regression: predictive modelling",
    "section": "Linearity",
    "text": "Linearity\nThe more linear the relationship between observed and predicted values, the better the model.\nPearson’s correlation coefficient (r):\n\nA measure of the strength and direction of a linear relationship between two variables.\nRanges from -1 to 1, with 0 indicating no relationship and 1 indicating a perfect positive linear relationship.\n\nr = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{\\hat{y}})^2}}\nR2:\n\nThe proportion of variance explained by the variables in the model.\nWhen two variables are compared (e.g. observed vs predicted) it is the same as correlation squared.\nA value of 1 indicates a perfect linear relationship.\n\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} Lin’s concordance correlation coefficient (\\rho_c):\n\nA measure of agreement between two variables (based on covariance, variances, and difference in means).\nHow well the points fit the 1:1 line.\nA value of 0 indicates no agreement and 1 indicating perfect agreement.\n\nLCCC = \\frac{2\\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) + (\\mu_X - \\mu_Y)^2}"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#lccc-vs-r-and-r2",
    "href": "lectures/L09/Lecture-09.html#lccc-vs-r-and-r2",
    "title": "Regression: predictive modelling",
    "section": "LCCC vs r and R2",
    "text": "LCCC vs r and R2\nLCCC combines error and linearity so it better measures the fit to the 1:1 line.\n\n\nCode\ndf &lt;- tibble(y = seq(0, 100, 5),\n  \"45 degree line | CCC = 1\" = seq(0, 100, 5)) %&gt;%\n  mutate(\"Location shift | CCC = 0.89\" = `45 degree line | CCC = 1` - 15) %&gt;%\n  mutate(\"Scale shift | CCC = 0.52\" = y / 2) %&gt;%\n  mutate(\"Location and scale shift | CCC = 0.67\" = y * 2 - 20)\n\n# pivot\ndf_long &lt;- df %&gt;%\n  pivot_longer(-1, values_to = \"x\") %&gt;%\n  mutate(name = factor(name, \n    levels = c(\"45 degree line | CCC = 1\",\n      \"Location shift | CCC = 0.89\",\n      \"Scale shift | CCC = 0.52\",\n      \"Location and scale shift | CCC = 0.67\")))\n\nggplot(df_long, aes(x, y)) +\n  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = \"grey\") +\n  facet_wrap(~name) +\n  geom_point() +\n  xlim(0, 100) +\n  labs(x = \"\", y = \"\") +\n  theme_classic() +\n  geom_blank()"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#limitations",
    "href": "lectures/L09/Lecture-09.html#limitations",
    "title": "Regression: predictive modelling",
    "section": "Limitations",
    "text": "Limitations\nNo one metric is perfect.\nEach prediction below has an LCCC of 0.6.\nWadoux and Minasny 2024  :::{.callout-tip}\nUse multiple metrics to test prediction quality, and always plot the predicted vs observed."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#about",
    "href": "lectures/L09/Lecture-09.html#about",
    "title": "Regression: predictive modelling",
    "section": "About",
    "text": "About\nData on the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.\nThe predictor variables are:\n\nALT Altitude (m)\nYR.ISOL Year when the patch was isolated (years)\nGRAZE Grazing (coded 1-5 which is light to heavy)\nAREA Patch area (ha)\nDIST Distance to nearest patch (km)\nLDIST Distance to largest patch (km)\n\n\n\nCode\nloyn &lt;- read_csv(\"images/loyn.csv\")"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#dataset-splitting",
    "href": "lectures/L09/Lecture-09.html#dataset-splitting",
    "title": "Regression: predictive modelling",
    "section": "Dataset splitting",
    "text": "Dataset splitting\nWe will split the data into training and test sets.\nAs the dataset is quite small, we will use a 80:20 split.\n\n\nCode\nset.seed(100)\nindexes &lt;- sample(1:nrow(loyn), size = 0.2 * nrow(loyn)) # randomly sample 20% of rows in the dataset\nloyn_train &lt;- loyn[-indexes, ] # remove the 20% - training dataset\nloyn_test &lt;- loyn[indexes, ] # select the 20% - test dataset"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#checking-the-split",
    "href": "lectures/L09/Lecture-09.html#checking-the-split",
    "title": "Regression: predictive modelling",
    "section": "Checking the split",
    "text": "Checking the split\nCheck out the str() of the data to see if the split worked (number of observations).\n\n\nCode\nstr(loyn_train)\n\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ AREA   : num [1:45] 0.1 0.5 0.5 1 1 1 1 1 2 2 ...\n $ YR.ISOL: num [1:45] 1968 1920 1900 1966 1918 ...\n $ DIST   : num [1:45] 39 234 104 66 246 467 284 156 66 93 ...\n $ LDIST  : num [1:45] 39 234 311 66 246 ...\n $ GRAZE  : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT    : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n\n\nCode\nstr(loyn_test)\n\n\ntibble [11 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:11] 3 29.5 26 39.6 34.4 19.5 14.6 28.3 15.8 5 ...\n $ AREA   : num [1:11] 1 973 18 49 96 6 2 34 5 4 ...\n $ YR.ISOL: num [1:11] 1900 1970 1966 1972 1976 ...\n $ DIST   : num [1:11] 311 337 40 1427 39 ...\n $ LDIST  : num [1:11] 571 1323 3188 1557 519 ...\n $ GRAZE  : num [1:11] 5 1 2 1 2 3 1 1 3 5 ...\n $ ALT    : num [1:11] 130 190 190 180 175 170 210 110 130 120 ..."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#exploratory-data-analysis",
    "href": "lectures/L09/Lecture-09.html#exploratory-data-analysis",
    "title": "Regression: predictive modelling",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nThe next step is to visualise the data.\nExpore relationships between the predictors and the response via histograms, scatterplots, boxplots, correlations etc.\n\nIn this lecture we will just look at histograms."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#histograms",
    "href": "lectures/L09/Lecture-09.html#histograms",
    "title": "Regression: predictive modelling",
    "section": "Histograms",
    "text": "Histograms\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n\n\n\n\nLooks like AREA LDIST and DIST are skewed – we will transform them so that they are more normally distributed."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#transforming-predictors",
    "href": "lectures/L09/Lecture-09.html#transforming-predictors",
    "title": "Regression: predictive modelling",
    "section": "Transforming predictors",
    "text": "Transforming predictors\nWe will use log10() to transform the predictors. The mutate() function from the dplyr package is useful for this as it can create new columns in the data frame with the transformed values.\n\n\nCode\nloyn_train &lt;- loyn_train %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    )\n\n\nThen, remove the untransformed variables from the dataset. Here we can use the select() function from the dplyr package to “delselect” columns by using the - sign.\n\n\nCode\nloyn_train &lt;- loyn_train %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nstr(loyn_train)\n\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND    : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ YR.ISOL  : num [1:45] 1968 1920 1900 1966 1918 ...\n $ GRAZE    : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT      : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n $ AREA_L10 : num [1:45] -1 -0.301 -0.301 0 0 ...\n $ LDIST_L10: num [1:45] 1.59 2.37 2.49 1.82 2.39 ...\n $ DIST_L10 : num [1:45] 1.59 2.37 2.02 1.82 2.39 ..."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#final-inspection",
    "href": "lectures/L09/Lecture-09.html#final-inspection",
    "title": "Regression: predictive modelling",
    "section": "Final inspection",
    "text": "Final inspection\nView the histograms again to check that the transformation worked.\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#full-model",
    "href": "lectures/L09/Lecture-09.html#full-model",
    "title": "Regression: predictive modelling",
    "section": "Full model",
    "text": "Full model\nWe start with a full model that includes all the predictors.\n\n\nCode\nfull_fit &lt;- lm(ABUND ~ ., data = loyn_train)\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#assumptions---round-1",
    "href": "lectures/L09/Lecture-09.html#assumptions---round-1",
    "title": "Regression: predictive modelling",
    "section": "Assumptions - Round 1",
    "text": "Assumptions - Round 1\nAs usual, we should check the assumptions of the model (CLINE + outliers). We will use the check_model() function from the perfomance package (because it looks nice and has interpretation instructions).\n\n\nCode\nperformance::check_model(full_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\"))"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#assumptions---round-1-1",
    "href": "lectures/L09/Lecture-09.html#assumptions---round-1-1",
    "title": "Regression: predictive modelling",
    "section": "Assumptions - Round 1",
    "text": "Assumptions - Round 1\nWe check multicollinearity with variable inflation factors (VIF) - VIFs are all &lt; 10, so there is no multicollinearity. All assumptions are thus met.\n\n\nCode\ncheck_model(full_fit, check = c(\"vif\"))"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#backwards-stepwise-selection",
    "href": "lectures/L09/Lecture-09.html#backwards-stepwise-selection",
    "title": "Regression: predictive modelling",
    "section": "Backwards stepwise selection",
    "text": "Backwards stepwise selection\nUse the step() function perform backwards stepwise selection. This function uses AIC to select the best model.\nDepending on the dataset splitting, the best model may be different each time we randomly sample the data. In this case we should all have the same results as we set the seed.\nIf we compare to the full model, the adjusted r-squared is slightly higher, and the AIC is lower.\n\n\nCode\nstep_fit &lt;- step(full_fit, direction = \"backward\")\n\n\nStart:  AIC=174.81\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- ALT        1     14.67 1618.5 173.22\n- LDIST_L10  1     16.92 1620.8 173.28\n- DIST_L10   1     69.18 1673.0 174.71\n&lt;none&gt;                   1603.9 174.81\n- GRAZE      1     78.00 1681.9 174.94\n- YR.ISOL    1    126.48 1730.3 176.22\n- AREA_L10   1    867.44 2471.3 192.26\n\nStep:  AIC=173.22\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1     10.76 1629.3 171.52\n&lt;none&gt;                   1618.5 173.22\n- DIST_L10   1     85.56 1704.1 173.54\n- GRAZE      1     98.23 1716.8 173.87\n- YR.ISOL    1    117.80 1736.3 174.38\n- AREA_L10   1   1088.05 2706.6 194.35\n\nStep:  AIC=171.52\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n&lt;none&gt;                  1629.3 171.52\n- GRAZE     1     93.97 1723.3 172.04\n- YR.ISOL   1    107.73 1737.0 172.40\n- DIST_L10  1    114.60 1743.9 172.57\n- AREA_L10  1   1161.66 2791.0 193.74"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#the-selected-model",
    "href": "lectures/L09/Lecture-09.html#the-selected-model",
    "title": "Regression: predictive modelling",
    "section": "The selected model",
    "text": "The selected model\n\n\n\n\nCode\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n\n\n\n\n\nCode\nsummary(step_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10, data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1683  -3.1961   0.3374   3.4834  14.2021 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -135.17508  102.72850  -1.316    0.196    \nYR.ISOL        0.08323    0.05118   1.626    0.112    \nGRAZE         -1.50496    0.99082  -1.519    0.137    \nAREA_L10       8.61888    1.61392   5.340 3.98e-06 ***\nDIST_L10      -4.87726    2.90770  -1.677    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.382 on 40 degrees of freedom\nMultiple R-squared:  0.6701,    Adjusted R-squared:  0.6371 \nF-statistic: 20.31 on 4 and 40 DF,  p-value: 3.365e-09\n\n\n\nAssumptions - Round 2\n\n\nCode\ncheck_model(step_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\", \"vif\"))\n\n\n\n\nModel validation\nIt looks like the model is good, so let’s bring in the test set to see how it performs!\n\n\nPrepare the test data\nSince the test data has not been transformed, we need to do that first.\nWe then predict onto the training and test dataset using the reduced stepwise model.\n\n\nCode\nloyn_test &lt;- loyn_test %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    ) %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nloyn_train$pred &lt;- predict(step_fit, newdata = loyn_train)\nloyn_test$pred &lt;- predict(step_fit, newdata = loyn_test)\n\n\n\n\nPlotting observed vs predicted\n\n\nCode\np1 &lt;- ggplot(loyn_train, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Training\") +\n  theme_classic()\n\np2 &lt;- ggplot(loyn_test, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Test\") +\n  theme_classic()\n\np1 + p2\n\n\n\n\n\nCalculating metrics - error\nGenerally the training dataset will have lower error and higher linearity than the test dataset. If this difference is very large – it suggests the model is not applicable to the test data and overfitting.\nWith error – lower is better. The following all measure error in the same units as the response variable (number of birds in each forest patch).\nMean error\n\n\nCode\nmean(loyn_train$ABUND - loyn_train$pred) |&gt; round(2)\n\n\n[1] 0\n\n\nCode\nmean(loyn_test$ABUND - loyn_test$pred) |&gt; round(2)\n\n\n[1] -1.65\n\n\nWe expect the ME for the training dataset to be near 0 – a well-fitted linear regression model will have positive and negative residuals balance each other out.\nMean absolute error\n\n\nCode\nmean(abs(loyn_train$ABUND - loyn_train$pred)) |&gt; round(2)\n\n\n[1] 4.43\n\n\nCode\nmean(abs(loyn_test$ABUND - loyn_test$pred)) |&gt; round(2)\n\n\n[1] 5.21\n\n\nRoot mean squared error (caret package)\n\n\nCode\nRMSE(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 6.02\n\n\nCode\nRMSE(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 6.72\n\n\n\n\nCalculating metrics – linearity\nWith linearity – higher is better.\nBoth training and test datasets perform similarly, which is a good sign the model is not overfitting.\nPearson’s correlation coefficient r\n\n\nCode\ncor(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nCode\ncor(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nR2\nCan either square the correlation coefficient or use the R2() function from the caret package.\n\n\nCode\nR2(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.67\n\n\nCode\nR2(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.68\n\n\nLin’s concordance correlation coefficient (CCC) (epiR package)\n\n\nCode\nepi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.8\n\n\nCode\nepi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.81\n\n\n\n\nConclusions\n\n\nCode\n# put all data into a tible and kable it\ntibble(\n    Dataset = c(\"Training\", \"Test\"),\n    ME = c(\n        mean(loyn_train$ABUND - loyn_train$pred),\n        mean(loyn_test$ABUND - loyn_test$pred)\n    ),\n    MAE = c(\n        mean(abs(loyn_train$ABUND - loyn_train$pred)),\n        mean(abs(loyn_test$ABUND - loyn_test$pred))\n    ),\n    RMSE = c(\n        RMSE(loyn_train$ABUND, loyn_train$pred),\n        RMSE(loyn_test$ABUND, loyn_test$pred)\n    ),\n    cor = c(\n        cor(loyn_train$ABUND, loyn_train$pred),\n        cor(loyn_test$ABUND, loyn_test$pred)\n    ),\n    R2 = c(\n        R2(loyn_train$ABUND, loyn_train$pred),\n        R2(loyn_test$ABUND, loyn_test$pred)\n    ),\n    LCCC = c(\n        epi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est,\n        epi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est\n    )\n) %&gt;%\n    knitr::kable(digits = 2)\n\n\n\n\n\nDataset\nME\nMAE\nRMSE\ncor\nR2\nLCCC\n\n\n\n\nTraining\n0.00\n4.43\n6.02\n0.82\n0.67\n0.80\n\n\nTest\n-1.65\n5.21\n6.72\n0.82\n0.68\n0.81\n\n\n\n\n\n\nThe model fit for the training dataset is marginally better (slight overfitting, but not a concern)\nSmall differences are also expected due to the small sample size or the chosen set.seed()\nThe model predicts bird abundance in forest patches in SE Victoria well (r = 0.82, LCCC = 0.81, MAE = 5.21 birds/patch, RMSE = 6.72 birds/patch).\n\n\n\nThanks!\nQuestions? Comments?\nSlides made with Quarto"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#assumptions---round-2",
    "href": "lectures/L09/Lecture-09.html#assumptions---round-2",
    "title": "Regression: predictive modelling",
    "section": "Assumptions - Round 2",
    "text": "Assumptions - Round 2\n\n\nCode\ncheck_model(step_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\", \"vif\"))"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#prepare-the-test-data",
    "href": "lectures/L09/Lecture-09.html#prepare-the-test-data",
    "title": "Regression: predictive modelling",
    "section": "Prepare the test data",
    "text": "Prepare the test data\nSince the test data has not been transformed, we need to do that first.\nWe then predict onto the training and test dataset using the reduced stepwise model.\n\n\nCode\nloyn_test &lt;- loyn_test %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    ) %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nloyn_train$pred &lt;- predict(step_fit, newdata = loyn_train)\nloyn_test$pred &lt;- predict(step_fit, newdata = loyn_test)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#plotting-observed-vs-predicted",
    "href": "lectures/L09/Lecture-09.html#plotting-observed-vs-predicted",
    "title": "Regression: predictive modelling",
    "section": "Plotting observed vs predicted",
    "text": "Plotting observed vs predicted\n\n\nCode\np1 &lt;- ggplot(loyn_train, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Training\") +\n  theme_classic()\n\np2 &lt;- ggplot(loyn_test, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Test\") +\n  theme_classic()\n\np1 + p2"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#calculating-metrics---error",
    "href": "lectures/L09/Lecture-09.html#calculating-metrics---error",
    "title": "Regression: predictive modelling",
    "section": "Calculating metrics - error",
    "text": "Calculating metrics - error\nGenerally the training dataset will have lower error and higher linearity than the test dataset. If this difference is very large – it suggests the model is not applicable to the test data and overfitting.\nWith error – lower is better. The following all measure error in the same units as the response variable (number of birds in each forest patch).\nMean error\n\n\nCode\nmean(loyn_train$ABUND - loyn_train$pred) |&gt; round(2)\n\n\n[1] 0\n\n\nCode\nmean(loyn_test$ABUND - loyn_test$pred) |&gt; round(2)\n\n\n[1] -1.65\n\n\nWe expect the ME for the training dataset to be near 0 – a well-fitted linear regression model will have positive and negative residuals balance each other out.\nMean absolute error\n\n\nCode\nmean(abs(loyn_train$ABUND - loyn_train$pred)) |&gt; round(2)\n\n\n[1] 4.43\n\n\nCode\nmean(abs(loyn_test$ABUND - loyn_test$pred)) |&gt; round(2)\n\n\n[1] 5.21\n\n\nRoot mean squared error (caret package)\n\n\nCode\nRMSE(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 6.02\n\n\nCode\nRMSE(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 6.72"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#calculating-metrics-linearity",
    "href": "lectures/L09/Lecture-09.html#calculating-metrics-linearity",
    "title": "Regression: predictive modelling",
    "section": "Calculating metrics – linearity",
    "text": "Calculating metrics – linearity\nWith linearity – higher is better.\nBoth training and test datasets perform similarly, which is a good sign the model is not overfitting.\nPearson’s correlation coefficient r\n\n\nCode\ncor(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nCode\ncor(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.82\n\n\nR2\nCan either square the correlation coefficient or use the R2() function from the caret package.\n\n\nCode\nR2(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n\n[1] 0.67\n\n\nCode\nR2(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n\n[1] 0.68\n\n\nLin’s concordance correlation coefficient (CCC) (epiR package)\n\n\nCode\nepi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.8\n\n\nCode\nepi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est |&gt; round(2)\n\n\n[1] 0.81"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#conclusions",
    "href": "lectures/L09/Lecture-09.html#conclusions",
    "title": "Regression: predictive modelling",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nCode\n# put all data into a tible and kable it\ntibble(\n    Dataset = c(\"Training\", \"Test\"),\n    ME = c(\n        mean(loyn_train$ABUND - loyn_train$pred),\n        mean(loyn_test$ABUND - loyn_test$pred)\n    ),\n    MAE = c(\n        mean(abs(loyn_train$ABUND - loyn_train$pred)),\n        mean(abs(loyn_test$ABUND - loyn_test$pred))\n    ),\n    RMSE = c(\n        RMSE(loyn_train$ABUND, loyn_train$pred),\n        RMSE(loyn_test$ABUND, loyn_test$pred)\n    ),\n    cor = c(\n        cor(loyn_train$ABUND, loyn_train$pred),\n        cor(loyn_test$ABUND, loyn_test$pred)\n    ),\n    R2 = c(\n        R2(loyn_train$ABUND, loyn_train$pred),\n        R2(loyn_test$ABUND, loyn_test$pred)\n    ),\n    LCCC = c(\n        epi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est,\n        epi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est\n    )\n) %&gt;%\n    knitr::kable(digits = 2)\n\n\n\n\n\nDataset\nME\nMAE\nRMSE\ncor\nR2\nLCCC\n\n\n\n\nTraining\n0.00\n4.43\n6.02\n0.82\n0.67\n0.80\n\n\nTest\n-1.65\n5.21\n6.72\n0.82\n0.68\n0.81\n\n\n\n\n\n\nThe model fit for the training dataset is marginally better (slight overfitting, but not a concern)\nSmall differences are also expected due to the small sample size or the chosen set.seed()\nThe model predicts bird abundance in forest patches in SE Victoria well (r = 0.82, LCCC = 0.81, MAE = 5.21 birds/patch, RMSE = 6.72 birds/patch)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#workflow",
    "href": "lectures/L08/Lecture-08.html#workflow",
    "title": "Regression: model development",
    "section": "Workflow",
    "text": "Workflow\n\nModel development\n\nExplore: visualise, summarise\nTransform predictors: linearise, reduce skewness/leverage\nModel: fit, check assumptions, interpret, transform. Repeat.\n\nVariable selection\n\nVIF: remove predictors with high variance inflation factor\nModel selection: stepwise selection, AIC, principle of parsimony, assumption checks\n\nPredictive modelling\n\nPredict: Use the model to predict new data\nValidate: Evaluate the model’s performance"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#previously-on-envx2001",
    "href": "lectures/L08/Lecture-08.html#previously-on-envx2001",
    "title": "Regression: model development",
    "section": "Previously on ENVX2001…",
    "text": "Previously on ENVX2001…\nWe fitted a multiple linear regression model to the data.\n\n\nCode\nfull_fit &lt;- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#question",
    "href": "lectures/L08/Lecture-08.html#question",
    "title": "Regression: model development",
    "section": "Question",
    "text": "Question\n\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind\nAre all the variables/predictors needed?\nPrinciples\nA good model:\n\nHas only useful predictors: principle of parsimony\nHas no redundant predictors: principle of orthogonality (no multicollinearity)\nIs interpretable (principle of transparency; last week), or predicts well (principle of accuracy; next week)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#on-the-principle-of-parsimony",
    "href": "lectures/L08/Lecture-08.html#on-the-principle-of-parsimony",
    "title": "Regression: model development",
    "section": "On the principle of parsimony",
    "text": "On the principle of parsimony\n\nOckham’s razor: “Entities should not be multiplied unnecessarily.”\nOne should prefer the simplest explanation that fits the data if multiple explanations are equally good.\n\n\n“It is vain to do with more what can be done with fewer.”\n\n– William of Ockham (1287–1347)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#what-happens-when-we-add-more-predictors-to-a-model",
    "href": "lectures/L08/Lecture-08.html#what-happens-when-we-add-more-predictors-to-a-model",
    "title": "Regression: model development",
    "section": "What happens when we add more predictors to a model?",
    "text": "What happens when we add more predictors to a model?\nA simple example using polynomial regression.\n\nThe more predictors we include, the more variance we can explain.\nHowever, the more predictors and complexity we include, the more overfitted the model becomes.\n\n\n\nCode\nset.seed(1030)\nxsquared &lt;- function(x) {\n  x ^ 2\n}\n# Generate xy data\nsim_data &lt;- function(xsquared, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1) \n  y = rnorm(n = sample_size, mean = xsquared(x), sd = 0.05)\n  data.frame(x, y)\n}\n# Generate predicted data (model)\ndf = sim_data(xsquared, sample_size = 60)\nfit &lt;- lm(y ~ 1, data = df)\nfit_1 &lt;- lm(y ~ poly(x, degree = 1), data = df)\nfit_2 &lt;- lm(y ~ poly(x, degree = 2), data = df)\nfit_many &lt;- lm(y ~ poly(x, degree = 20), data = df)\ntruth &lt;- seq(from = 0, to = 1, by = 0.01)\n# Combine the data and model fits into a single data frame\ndf &lt;- data.frame(\n  x = df$x,\n  y = df$y,\n  fit = predict(fit),\n  fit_1 = predict(fit_1),\n  fit_2 = predict(fit_2),\n  fit_many = predict(fit_many)\n)\n\n# Reshape the data frame into long format\ndf_long &lt;- pivot_longer(\n  df, \n  cols = starts_with(\"fit_\"),\n  names_to = \"model\",\n  values_to = \"value\"\n) %&gt;% \n  mutate(\n    model = case_when(\n      model == \"fit\" ~ \"y = b\",\n      model == \"fit_1\" ~ \"y = b + mx\",\n      model == \"fit_2\" ~ \"y = b + mx + nx^2\",\n      model == \"fit_many\" ~ \"y = b + mx + nx^2 + ... + zx^20\",\n      TRUE ~ model\n    )\n  )\n# Plot\np &lt;- ggplot(df_long, aes(x = x, y = value, color = model)) +\n  facet_wrap(~ model, ncol = 2, scales = \"free\") +\n  geom_point(aes(y = y), alpha = .4, size = 2) +\n  geom_line(linewidth = .9, linetype = 1) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  geom_blank()\np"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#variance-bias-trade-off",
    "href": "lectures/L08/Lecture-08.html#variance-bias-trade-off",
    "title": "Regression: model development",
    "section": "Variance-bias trade-off",
    "text": "Variance-bias trade-off\n\nIn concept…\n\nAs complexity increases, bias (\\sum{O-\\bar{P}}) decreases (the mean of a model’s predictions is closer to the true mean).\nAs complexity increases, prediction variance (\\frac{\\sum{P-\\bar{P}}}{n}) decreases.\nThe goal is to find a model that isn’t too simple or complex with a good balance between bias and variance.\n\n \\text{Mean Squared Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Immeasureable Error}  In practice, the math and relationships are a bit more irregular."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#how-do-we-determine-the-best-model",
    "href": "lectures/L08/Lecture-08.html#how-do-we-determine-the-best-model",
    "title": "Regression: model development",
    "section": "How do we determine the best model?",
    "text": "How do we determine the best model?\nSome model quality measures you should be familiar with:\n\nR2: variance explained by the model with a maximum value of 1 = 100%\nResidual standard error: the mean error of the observed values from the predicted/fitted values (i.e. line of best fit).\nPartial F-test: compare the full model to a reduced model, works well when the number of predictors is small and simple models.\n\nSome other commonly used measures:\n\nInformation criteria: AIC, BIC, etc. (more on this later).\nError measures: useful when the aim for the model is to predict. The best model has the smallest residual error (or other similar metrics)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#what-models-do-we-try",
    "href": "lectures/L08/Lecture-08.html#what-models-do-we-try",
    "title": "Regression: model development",
    "section": "What models do we try?",
    "text": "What models do we try?\n\nEverything: all possible combinations of predictors.\n\nEach variable can either be included or excluded so the number of possible combinations is 2^n\ne.g. 3 predictors (x_1, x_2, x_3) could have 8 models\n\nNo variables i.e. mean y the null hypothesis\n1 variable: x_1; x_2; x_3\n2 variables: x_1 + x_2; x_1 + x_3; x_2 + x_3\n3 variables: x_1 + x_2 + x_3\n\nSo…not recommended\n\nStepwise regression: add/remove predictors one at a time until removing a variable makes the model worse.\nSelect meaningful predictors based on domain knowledge, correlation, or significance.\nMore complex approaches are available for big data and machine learning."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#air-quality-can-we-reduce-the-number-of-predictors",
    "href": "lectures/L08/Lecture-08.html#air-quality-can-we-reduce-the-number-of-predictors",
    "title": "Regression: model development",
    "section": "Air quality: can we reduce the number of predictors?",
    "text": "Air quality: can we reduce the number of predictors?\nFull model:\n\n\nCode\nsummary(full_fit)\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\nWind has the highest p-value, can we remove it?\nFull model: Multiple R-squared = 0.66, Adjusted R-squared = 0.66"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#section",
    "href": "lectures/L08/Lecture-08.html#section",
    "title": "Regression: model development",
    "section": "",
    "text": "Full model: multiple R-squared = 0.66, adjusted R-squared = 0.66\n\nReduced model: take out Wind\n\n\nCode\nreduced_fit &lt;- lm(log(Ozone) ~ Temp + Solar.R, data = airquality)\nsummary(reduced_fit)\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83864 -0.33727  0.03444  0.29877  1.38210 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.7646527  0.4249016  -4.153 6.58e-05 ***\nTemp         0.0607386  0.0056663  10.719  &lt; 2e-16 ***\nSolar.R      0.0024651  0.0005924   4.161 6.38e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5413 on 108 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6163,    Adjusted R-squared:  0.6092 \nF-statistic: 86.73 on 2 and 108 DF,  p-value: &lt; 2.2e-16\n\n\n\nReduced model: multiple R-squared = 0.62, adjusted R-squared = 0.61\nAdjusted R-squared is lower, but is a 4% difference “worth it”? Is it significant?"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#the-r2-value",
    "href": "lectures/L08/Lecture-08.html#the-r2-value",
    "title": "Regression: model development",
    "section": "The R2 value",
    "text": "The R2 value\nThe R-squared value is the proportion of variance explained by the model.\n R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \nThe adjusted R-squared value is the proportion of variance explained by the model, adjusted for the number of predictors.\nR^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} \nwhere n is the number of observations and p is the number of predictors."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#partial-f-test-1",
    "href": "lectures/L08/Lecture-08.html#partial-f-test-1",
    "title": "Regression: model development",
    "section": "Partial F-test",
    "text": "Partial F-test\nHow much of an improvement in adjusted R^2 is worth having an extra variable / more complex model?\n\nWe can perform a hypothesis test to determine whether the improvement is significant.\nThe F-test measures the full model against an intercept only model in terms of explained variance (residual sum of squares).\nThe partial F-test compares the full model to a reduced model in terms of the trade-off between model complexity and variance explained (i.e. adjusted R^2).\n\nH_0: no significant difference between the full and reduced models\nH_1: the full model is significantly better than the reduced model\nCalculating the F-stat:\n\n\nF = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full}"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#partial-f-test-calculation",
    "href": "lectures/L08/Lecture-08.html#partial-f-test-calculation",
    "title": "Regression: model development",
    "section": "Partial F-test: calculation",
    "text": "Partial F-test: calculation\nF = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full} where:\n\nSS_{reg,full} is the sum of squares of the full model (total of predictors)\nSS_{reg,reduced} is the sum of squares of the reduced model (total of predictors)\ndf_{res,full} is the degrees of freedom of the residuals of the full model\ndf_{res,reduced} is the degrees of freedom of the residuals of the reduced model\nMS_{res, full} is the mean square of the residuals of the full model\n\n\n\n\n\nCode\nfull &lt;- anova(full_fit) %&gt;% broom::tidy()\nfull\n\n\n# A tibble: 4 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Temp          1 45.8  45.8       177.   2.07e-24\n2 Solar.R       1  5.07  5.07       19.6  2.29e- 5\n3 Wind          1  3.97  3.97       15.4  1.58e- 4\n4 Residuals   107 27.7   0.259      NA   NA       \n\n\n\n\n\nCode\nreduced &lt;- anova(reduced_fit) %&gt;% broom::tidy()\nreduced\n\n\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Temp          1 45.8  45.8       156.   1.05e-22\n2 Solar.R       1  5.07  5.07       17.3  6.38e- 5\n3 Residuals   108 31.6   0.293      NA   NA       \n\n\n\nEach row is the individual effect of each predictor on the response log(Ozone) (whilst holding all other predictors constant)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#by-hand",
    "href": "lectures/L08/Lecture-08.html#by-hand",
    "title": "Regression: model development",
    "section": "By hand",
    "text": "By hand\nF = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full}\n\nSS_{reg,full} = 45.8 + 5.07 + 3.97 = 54.84\nSS_{reg,reduced} = 45.8 + 5.07 = 50.87\ndf_{res,full} = 107\ndf_{res,reduced} = 108\nMS_{res, full} = 0.259\n\nF = |\\frac{54.84 - 50.87}{(107-108)}| \\div 0.259 = 15.33"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#in-r-manually",
    "href": "lectures/L08/Lecture-08.html#in-r-manually",
    "title": "Regression: model development",
    "section": "In R (manually)",
    "text": "In R (manually)\n\n\nCode\nss_full &lt;- sum(full$sumsq[1:3])\nss_reduced &lt;- sum(reduced$sumsq[1:2])\ndf_full &lt;- full$df[4]\ndf_reduced &lt;- reduced$df[3]\nms_full &lt;- full$meansq[4]\nF &lt;- abs((ss_full - ss_reduced) / ((df_full - df_reduced))) / ms_full\nF     # F-statistic\n\n\n[1] 15.35026\n\n\nCode\npf(F, df1 = 1, df2 = df_full, lower.tail = FALSE) # corresponding p-value\n\n\n[1] 0.0001576806\n\n\n(There is a slight difference due to rounding)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#in-r-with-functions",
    "href": "lectures/L08/Lecture-08.html#in-r-with-functions",
    "title": "Regression: model development",
    "section": "In R with functions",
    "text": "In R with functions\n\n\nCode\nanova(full_fit, reduced_fit)\n\n\nAnalysis of Variance Table\n\nModel 1: log(Ozone) ~ Temp + Solar.R + Wind\nModel 2: log(Ozone) ~ Temp + Solar.R\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    107 27.675                                 \n2    108 31.645 -1   -3.9703 15.35 0.0001577 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe partial F-test is significant (p-value &lt; 0.05), so we can reject the null hypothesis and conclude that the full model is significantly better, even if adjusted R2 improves by 4%."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#but-wait",
    "href": "lectures/L08/Lecture-08.html#but-wait",
    "title": "Regression: model development",
    "section": "But wait…",
    "text": "But wait…\nLooking back at the original model, we can see that the partial regression coefficients are the same as the partial F-test results!\n\n\nCode\nanova(full_fit, reduced_fit) # partial F-test\n\n\nAnalysis of Variance Table\n\nModel 1: log(Ozone) ~ Temp + Solar.R + Wind\nModel 2: log(Ozone) ~ Temp + Solar.R\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    107 27.675                                 \n2    108 31.645 -1   -3.9703 15.35 0.0001577 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nfull &lt;- anova(full_fit) %&gt;% broom::tidy()\nfull # partial regression coefficients\n\n\n# A tibble: 4 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Temp          1 45.8  45.8       177.   2.07e-24\n2 Solar.R       1  5.07  5.07       19.6  2.29e- 5\n3 Wind          1  3.97  3.97       15.4  1.58e- 4\n4 Residuals   107 27.7   0.259      NA   NA       \n\n\nThis is because the reduced model is nested within the full model so the partial F-test is equivalent to a partial regression coefficient test."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#nested-models",
    "href": "lectures/L08/Lecture-08.html#nested-models",
    "title": "Regression: model development",
    "section": "Nested models",
    "text": "Nested models\n\nPrevious example is a simple example of a nested model.\nA model is nested within another model if the predictors in the first model are a subset of the predictors in the second model.\nThis makes comparing the two models easier, as we can compare the regression coefficients of the two models.\n\nExample\n\nIf the original model is y ~ a + b + c:\n\nNested: y ~ a + b\nNested: y ~ a\nNot nested: y ~ a + b + d – because d is not in the full model\n\n\n\n\n\n\n\n\nImportant\n\n\nPartial F-tests will only make sense/work for nested models!"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#about",
    "href": "lectures/L08/Lecture-08.html#about",
    "title": "Regression: model development",
    "section": "About",
    "text": "About\n\n\nCode\nloyn &lt;- read.csv(\"images/loyn.csv\")\nstr(loyn)\n\n\n'data.frame':   56 obs. of  7 variables:\n $ ABUND  : num  5.3 2 1.5 17.1 13.8 14.1 3.8 2.2 3.3 3 ...\n $ AREA   : num  0.1 0.5 0.5 1 1 1 1 1 1 1 ...\n $ YR.ISOL: int  1968 1920 1900 1966 1918 1965 1955 1920 1965 1900 ...\n $ DIST   : int  39 234 104 66 246 234 467 284 156 311 ...\n $ LDIST  : int  39 234 311 66 246 285 467 1829 156 571 ...\n $ GRAZE  : int  2 5 5 3 5 3 5 5 4 5 ...\n $ ALT    : int  160 60 140 160 140 130 90 60 130 130 ...\n\n\n\nCan we predict the abundance of birds in forest patches cleared for agriculture, based on patch size, area, grazing and other variables?\nLoyn (1987)\n\nDIST: Distance to nearest patch (km)\nLDIST: Distance to a larger patch (km)\nAREA: Patch area (ha)\nGRAZE: Grazing pressure 1 (light) – 5 (heavy) – ALT: Altitude (m)\nYR.ISOL: Years since isolation (years)\nABUND: Density of forest birds in a forest patch (birds/patch)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#data-exploration",
    "href": "lectures/L08/Lecture-08.html#data-exploration",
    "title": "Regression: model development",
    "section": "Data exploration",
    "text": "Data exploration\n\n\nCode\nloyn %&gt;%\n  pivot_longer(-ABUND) %&gt;%\n  ggplot(aes(x = value, y = ABUND)) +\n  geom_point() +\n  facet_wrap(~name, scales = \"free\") +\n  labs(y = \"ABUND\")\n\n\n\n\nThe predictors are on very different scales, which can cause problems for the model.\nThe relationships don’t look particularly linear…and outliers.\nWe will perform log10 transforms of AREA, LDIST, and DIST."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#log10-transformation",
    "href": "lectures/L08/Lecture-08.html#log10-transformation",
    "title": "Regression: model development",
    "section": "Log10 transformation",
    "text": "Log10 transformation\n\n\nCode\n# perform transformations\nloyn &lt;- loyn %&gt;%\n  mutate(AREA_L10 = log10(AREA),\n         LDIST_L10 = log10(LDIST),\n         DIST_L10 = log10(DIST))\n\n# View distributions again\nloyn %&gt;%\n  select(-ALT, -GRAZE, -YR.ISOL) %&gt;%\n  pivot_longer(-ABUND) %&gt;%\n  mutate(name = factor(name, levels = unique(name))) %&gt;%  # Preserve original order\n  ggplot(aes(x = value, y = ABUND)) +\n  geom_point() +\n  facet_wrap(~name, scales = \"free\") +\n  labs(y = \"ABUND\")"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#checking-assumptions---no-transformation",
    "href": "lectures/L08/Lecture-08.html#checking-assumptions---no-transformation",
    "title": "Regression: model development",
    "section": "Checking assumptions - no transformation",
    "text": "Checking assumptions - no transformation\n\n\nCode\nloyn_fit &lt;- lm(ABUND ~ YR.ISOL + GRAZE + ALT + AREA + LDIST + DIST, data = loyn)\nsummary(loyn_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + ALT + AREA + LDIST + DIST, \n    data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.6638  -4.6409  -0.0883   4.2858  20.1042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -1.097e+02  1.133e+02  -0.968  0.33791   \nYR.ISOL      6.693e-02  5.684e-02   1.177  0.24472   \nGRAZE       -3.447e+00  1.107e+00  -3.114  0.00308 **\nALT          4.772e-02  3.089e-02   1.545  0.12878   \nAREA         8.866e-04  4.657e-03   0.190  0.84980   \nLDIST        1.418e-03  1.310e-03   1.082  0.28451   \nDIST         3.811e-03  5.418e-03   0.703  0.48514   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.947 on 49 degrees of freedom\nMultiple R-squared:  0.5118,    Adjusted R-squared:  0.452 \nF-statistic: 8.561 on 6 and 49 DF,  p-value: 2.24e-06\n\n\nCode\nperformance::check_model(loyn_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#checking-assumptions---transformation",
    "href": "lectures/L08/Lecture-08.html#checking-assumptions---transformation",
    "title": "Regression: model development",
    "section": "Checking assumptions - transformation",
    "text": "Checking assumptions - transformation\n\n\nCode\nloyn_fit &lt;- lm(ABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10, data = loyn)\nsummary(loyn_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + \n    DIST_L10, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6506  -2.9390   0.5289   2.5353  15.2842 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -125.69725   91.69228  -1.371   0.1767    \nYR.ISOL        0.07387    0.04520   1.634   0.1086    \nGRAZE         -1.66774    0.92993  -1.793   0.0791 .  \nALT            0.01951    0.02396   0.814   0.4195    \nAREA_L10       7.47023    1.46489   5.099 5.49e-06 ***\nLDIST_L10     -0.64842    2.12270  -0.305   0.7613    \nDIST_L10      -0.90696    2.67572  -0.339   0.7361    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.384 on 49 degrees of freedom\nMultiple R-squared:  0.6849,    Adjusted R-squared:  0.6464 \nF-statistic: 17.75 on 6 and 49 DF,  p-value: 8.443e-11\n\n\nCode\nperformance::check_model(loyn_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#leverage",
    "href": "lectures/L08/Lecture-08.html#leverage",
    "title": "Regression: model development",
    "section": "Leverage",
    "text": "Leverage\n\nThe leverage plot shows the influence of each observation (i.e. point) on the model.\nPoints with high leverage can have a large effect on the model when removed.\nIdentified by the Cook’s distance statistic – named after the American statistician R. Dennis Cook, who introduced the concept in 1977.\n\n\n\n\n\n\n\nTip\n\n\nThe leverage plot is a useful tool for identifying outliers and influential points, but can also be used to check for other issues such as heteroskedasticity (equal variances) and non-linearity!"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#reading-the-leverage-plot",
    "href": "lectures/L08/Lecture-08.html#reading-the-leverage-plot",
    "title": "Regression: model development",
    "section": "Reading the leverage plot",
    "text": "Reading the leverage plot\n\n\nCode\npar(mfrow = c(1,2))\nplot(loyn_fit, which = c(4,5))\n\n\n\n\nVisually, points with Cook’s distance &gt; 0.5 are considered influential by default, but this is a somewhat arbitrary threshold.\nIn practice, you should use a threshold that is appropriate for your data and model."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#outlier-detection-using-performance",
    "href": "lectures/L08/Lecture-08.html#outlier-detection-using-performance",
    "title": "Regression: model development",
    "section": "Outlier detection using performance",
    "text": "Outlier detection using performance\n\n\nCode\nperformance::check_model(loyn_fit, check = c(\"outliers\", \"pp_check\"))\n\n\n\n\n\nCode\nperformance::check_outliers(loyn_fit)\n\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.919).\n- For variable: (Whole model)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#collinearity",
    "href": "lectures/L08/Lecture-08.html#collinearity",
    "title": "Regression: model development",
    "section": "Collinearity",
    "text": "Collinearity\n\nTwo predictors that have a perfect linear relationship (i.e. r = 1 or -1) breaks the assumption of collinearity\nEven strong correlations between predictors can lead to unstable estimates and large standard errors.\nVariance inflation factors (VIFs) are a measure of collinearity in the model.\n\n\n\nCode\ncorrplot::corrplot(cor(loyn), method = \"number\")"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#calculating-vif",
    "href": "lectures/L08/Lecture-08.html#calculating-vif",
    "title": "Regression: model development",
    "section": "Calculating VIF",
    "text": "Calculating VIF\n\n\nCode\ncar::vif(loyn_fit) |&gt; round(2) # numbers\n\n\n  YR.ISOL     GRAZE       ALT  AREA_L10 LDIST_L10  DIST_L10 \n     1.80      2.52      1.47      1.91      2.01      1.65 \n\n\nCode\nplot(performance::check_collinearity(loyn_fit)) # visual\n\n\n\n\n1 = no correlation with other predictors.\n&gt;10 is a sign for high, not tolerable correlation of model predictors (which need to be removed and the model refitted)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#the-best-model",
    "href": "lectures/L08/Lecture-08.html#the-best-model",
    "title": "Regression: model development",
    "section": "The best model?",
    "text": "The best model?\nIf we remove the least significant variable…\n\n\nCode\nfull6 &lt;- loyn_fit\npart5 &lt;- update(full6, . ~ . - LDIST_L10)\npart4 &lt;- update(part5, . ~ . - DIST_L10)\npart3 &lt;- update(part4, . ~ . - ALT)\npart2 &lt;- update(part3, . ~ . - YR.ISOL)\npart1 &lt;- update(part2, . ~ . - GRAZE)\n\nformulas &lt;- c(part1$call$formula, \n              part2$call$formula, \n              part3$call$formula, \n              part4$call$formula, \n              part5$call$formula, \n              loyn_fit$call$formula)\nformulas &lt;-\n  c(\"ABUND ~ AREA_L10\",\n    \"ABUND ~ AREA_L10 + GRAZE\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10 + LDIST_L10\")\n\nrs &lt;- bind_rows(glance(part1), \n          glance(part2), \n          glance(part3), \n          glance(part4),\n          glance(part5), \n          glance(full6)) %&gt;%\n        mutate(Model = formulas) %&gt;%\n        select(Model, r.squared, adj.r.squared)\n\nknitr::kable(rs, digits = 2)\n\n\n\n\n\n\n\n\n\n\nModel\nr.squared\nadj.r.squared\n\n\n\n\nABUND ~ AREA_L10\n0.55\n0.54\n\n\nABUND ~ AREA_L10 + GRAZE\n0.65\n0.64\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL\n0.67\n0.65\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT\n0.68\n0.66\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10\n0.68\n0.65\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10 + LDIST_L10\n0.68\n0.65\n\n\n\n\n\n\nR-squared increases with addition of predictors.\nAdj. R-squared varies with addition of predictors."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#the-problem",
    "href": "lectures/L08/Lecture-08.html#the-problem",
    "title": "Regression: model development",
    "section": "The problem",
    "text": "The problem\n\nOther combinations of predictors exist but are not shown.\nNeed automated way to select the best model – 6 predictors gives us 2^6 = 64 models to choose from!\nTwo options:\n\nBackward elimination\nForward selection (not covered in this course)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#steps-for-backward-elimination",
    "href": "lectures/L08/Lecture-08.html#steps-for-backward-elimination",
    "title": "Regression: model development",
    "section": "Steps for backward elimination",
    "text": "Steps for backward elimination\n\nStart with full model.\nFor each predictor, test the effect of its removal on the model fit.\nRemove the predictor that has the least effect on the model fit i.e. the least informative predictor, unless it is nonetheless supplying significant information about the response.\nRepeat steps 2 and 3 until no predictors can be removed without significantly affecting the model fit.\n\nIn backward selection, the model fit is assessed using the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). Here we focus on the AIC."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#about-aic",
    "href": "lectures/L08/Lecture-08.html#about-aic",
    "title": "Regression: model development",
    "section": "About AIC",
    "text": "About AIC\n\nMost popular model selection criterion.\nDeveloped by Hirotsugu Akaike under the name of “an information criterion” (AIC)\nFounded on information theory which is concerned with the transmission, processing, utilization, and extraction of information.\n\nAIC = 2k - 2\\ln(L) where k is the number of parameters in the model and L is the maximum value of the likelihood function."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#interpretation",
    "href": "lectures/L08/Lecture-08.html#interpretation",
    "title": "Regression: model development",
    "section": "Interpretation",
    "text": "Interpretation\nWhen used in linear regression, the AIC can also be defined as:\nAIC = n\\log(\\frac{RSS}{n}) + 2k\nwhere RSS is the residual sum of squares and 2k is the number of parameters in the model (i.e. model complexity).\n\nThe AIC is a measure of the relative quality or goodness of fit of a statistical model for a given set of data.\nIt estimates the relative amount of information lost by a given model when it is used to approximate the true underlying process that generated the data.\nThe smaller the AIC, the better the model fits the data.\nA relative measure and unitless, so it is not worth trying to interpret alone."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#back-to-our-example",
    "href": "lectures/L08/Lecture-08.html#back-to-our-example",
    "title": "Regression: model development",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nCode\nback_step &lt;- step(loyn_fit, direction = \"backward\")\n\n\nStart:  AIC=214.14\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1      3.80 2000.7 212.25\n- DIST_L10   1      4.68 2001.5 212.27\n- ALT        1     27.02 2023.9 212.90\n&lt;none&gt;                   1996.8 214.14\n- YR.ISOL    1    108.83 2105.7 215.11\n- GRAZE      1    131.07 2127.9 215.70\n- AREA_L10   1   1059.75 3056.6 235.98\n\nStep:  AIC=212.25\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n- DIST_L10  1     12.64 2013.3 210.60\n- ALT       1     35.12 2035.8 211.22\n&lt;none&gt;                  2000.7 212.25\n- YR.ISOL   1    121.64 2122.3 213.55\n- GRAZE     1    132.44 2133.1 213.84\n- AREA_L10  1   1193.04 3193.7 236.44\n\nStep:  AIC=210.6\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n- ALT       1     57.84 2071.1 210.19\n&lt;none&gt;                  2013.3 210.60\n- GRAZE     1    123.48 2136.8 211.94\n- YR.ISOL   1    134.89 2148.2 212.23\n- AREA_L10  1   1227.11 3240.4 235.25\n\nStep:  AIC=210.19\nABUND ~ YR.ISOL + GRAZE + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n&lt;none&gt;                  2071.1 210.19\n- YR.ISOL   1    129.81 2200.9 211.59\n- GRAZE     1    188.45 2259.6 213.06\n- AREA_L10  1   1262.97 3334.1 234.85"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#section-1",
    "href": "lectures/L08/Lecture-08.html#section-1",
    "title": "Regression: model development",
    "section": "",
    "text": "Printing back_step reveals the final model:\n\n\nCode\nback_step\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10, data = loyn)\n\nCoefficients:\n(Intercept)      YR.ISOL        GRAZE     AREA_L10  \n -134.26065      0.07835     -1.90216      7.16617"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#backward-elimination-coefficients",
    "href": "lectures/L08/Lecture-08.html#backward-elimination-coefficients",
    "title": "Regression: model development",
    "section": "Backward elimination: coefficients",
    "text": "Backward elimination: coefficients\nFull model\n\n\nCode\nsjPlot::tab_model(\n  loyn_fit, back_step, \n  show.ci = FALSE, \n  show.aic = TRUE,\n  dv.labels = c(\"Full model\",\n                \"Reduced model\")\n)\n\n\n\n\n\n \nFull model\nReduced model\n\n\nPredictors\nEstimates\np\nEstimates\np\n\n\n(Intercept)\n-125.70\n0.177\n-134.26\n0.126\n\n\nYR ISOL\n0.07\n0.109\n0.08\n0.077\n\n\nGRAZE\n-1.67\n0.079\n-1.90\n0.034\n\n\nALT\n0.02\n0.419\n\n\n\n\nAREA L10\n7.47\n&lt;0.001\n7.17\n&lt;0.001\n\n\nLDIST L10\n-0.65\n0.761\n\n\n\n\nDIST L10\n-0.91\n0.736\n\n\n\n\nObservations\n56\n56\n\n\nR2 / R2 adjusted\n0.685 / 0.646\n0.673 / 0.654\n\n\nAIC\n375.064\n371.109\n\n\n\n\n\n\n\nThe reduced model retains more explanatory power than the full model!"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#model-selection",
    "href": "lectures/L08/Lecture-08.html#model-selection",
    "title": "Regression: model development",
    "section": "Model selection",
    "text": "Model selection\nModel development\n\nStart with full model and check assumptions (e.g. normality, homoscedasticity, linearity, etc.).\nLook for additional issues (e.g. multicollinearity, outliers, etc.) – correlations, leverage, VIF plots.\nConsider transformations (e.g. log, sqrt, etc.).\nTest assumptions again.\n\nModel selection\n\nUse VIF as an initial step to get rid of highly correlated predictors.\nPerform variable selection using backward elimination (good and fast), because:\n\nUsing R2 as a criterion is not recommended (it is not a good measure of model fit, only a good measure of variance explained).\nUsing partial F-test is good, but slow."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#next-lecture-model-training-and-prediction",
    "href": "lectures/L08/Lecture-08.html#next-lecture-model-training-and-prediction",
    "title": "Regression: model development",
    "section": "Next lecture: model training and prediction",
    "text": "Next lecture: model training and prediction\n\nHow to incorporate calibration and validation into your workflow\nDetermining prediction intervals and performance metrics"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#about-me",
    "href": "lectures/L07/Lecture-07.html#about-me",
    "title": "Regression: modelling",
    "section": "About me",
    "text": "About me\n\n\n\n\n\n\n\nResearch topics: spatial modelling and mapping, precision agriculture, winter grains\nTimeline at USYD\n\nBSc (Hons) in Agricultural Science\nPhD in Digital Agriculture\nPostdoc in Spatial Modelling\nAssociate Lecturer in Agricultural Data Science\n\n\n\n\n\n\n\nFaba beans at Trangie"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#learning-outcomes",
    "href": "lectures/L07/Lecture-07.html#learning-outcomes",
    "title": "Regression: modelling",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nLO1. demonstrate proficiency in designing sample schemes and analysing data from them using using R\nLO2. describe and identify the basic features of an experimental design; replicate, treatment structure and blocking structure\nLO3. demonstrate proficiency in the use or the statistical programming language R to an ANOVA and fit regression models to experimental data\nLO4. demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data\nLO5. interpret the output and understand conceptually how its derived of a regression, ANOVA and multivariate analysis that have been calculated by R\nLO6. write statistical and modelling results as part of a scientific report\nLO7. appraise the validity of statistical analyses used publications."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#refresher-from-envx1002",
    "href": "lectures/L07/Lecture-07.html#refresher-from-envx1002",
    "title": "Regression: modelling",
    "section": "Refresher from ENVX1002",
    "text": "Refresher from ENVX1002\n\nRegression modelling is for one continuous numerical response (y) and one or more numerical predictors (x_1, x_2, x_n)\nCan be for linear or nonlinear relationships – focus on linear in ENVX2001\nTo help us:\n\nUnderstand the relationship between variables\nPredict new values of y based on x\nTest hypotheses about the relationship between variables\n\nFit a ‘line of best fit’ that minimises the sum of the squared residuals (least-squares)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#workflow",
    "href": "lectures/L07/Lecture-07.html#workflow",
    "title": "Regression: modelling",
    "section": "Workflow",
    "text": "Workflow\n\nModel development\n\nExplore: visualise, summarise\nModel: fit, check assumptions, interpret – (transform, repeat).\nTransform predictors\n\nVariable selection\n\nVIF: remove predictors with high variance inflation factor\nModel selection: stepwise selection, AIC, principle of parsimony, assumption checks\n\nPredictive modelling\n\nPredict: Use the model to predict new data\nValidate: Evaluate the model’s performance"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#brief-history",
    "href": "lectures/L07/Lecture-07.html#brief-history",
    "title": "Regression: modelling",
    "section": "Brief history",
    "text": "Brief history\n  \nAdrien-Marie Legendre, Carl Friedrich Gauss, Francis Galton\n\n\n\n\n\n\nNote\n\n\nMany other people contributed to the development of regression analysis, but these three are the most well-known."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#brief-history-1",
    "href": "lectures/L07/Lecture-07.html#brief-history-1",
    "title": "Regression: modelling",
    "section": "Brief history",
    "text": "Brief history\n\nMethod of least squares first theorised by Adrien-Marie Legendre in 1805\nTechnique of least squares first used by Carl Friedrich Gauss in 1809 (to fit a parabola to the orbit of the asteroid Ceres)\nModel fitting first published by Francis Galton in 1886 (predicting the height of a child from the height of the parents)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#example-child-vs-parent-height",
    "href": "lectures/L07/Lecture-07.html#example-child-vs-parent-height",
    "title": "Regression: modelling",
    "section": "Example: child vs parent height",
    "text": "Example: child vs parent height\nGalton, F. (1886). Regression Towards Mediocrity in Hereditary Stature Journal of the Anthropological Institute, 15, 246-263\n\n\n\n\nCode\nlibrary(HistData)\ndata(Galton)\nstr(Galton)\n\n\n'data.frame':   928 obs. of  2 variables:\n $ parent: num  70.5 68.5 65.5 64.5 64 67.5 67.5 67.5 66.5 66.5 ...\n $ child : num  61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 ...\n\n\n\n928 children of 205 pairs of parents\nAverage height of both parents and their child’s height measured in inches\nSize classes were binned (hence data looks discrete)\n\n\n\n\nCode\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) + \n  geom_smooth(method = \"lm\") +\n  labs(subtitle = paste(\"Correlation:\", round(cor(Galton$parent, Galton$child), 2)))"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#defining-a-linear-relationship",
    "href": "lectures/L07/Lecture-07.html#defining-a-linear-relationship",
    "title": "Regression: modelling",
    "section": "Defining a linear relationship",
    "text": "Defining a linear relationship\n\nPearson correlation coefficient (r) measures the linear correlation between two variables (ranges from -1 to 1)\nUseful for distinguishing strength (weak/moderate/strong) and direction (positive/negative) of the association\nDoes not distinguish different patterns – i.e. is the relationship actually linear?\n\n r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \n\n\nCode\ncor(Galton$parent, Galton$child) |&gt; round(2)\n\n\n[1] 0.46"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#anscombes-quartet",
    "href": "lectures/L07/Lecture-07.html#anscombes-quartet",
    "title": "Regression: modelling",
    "section": "Anscombe’s quartet",
    "text": "Anscombe’s quartet\n\n\nCode\nlibrary(tidyverse)\nanscombe %&gt;%\n  pivot_longer(everything(), cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"), names_pattern = \"(.)(.)\") %&gt;%\n  ggplot(aes(x = x, y = y)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    facet_wrap(~set, ncol = 4)\n\n\n\nAll of these data have a correlation coefficient of about 0.8 – always visualise your data."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#simple-linear-regression-model",
    "href": "lectures/L07/Lecture-07.html#simple-linear-regression-model",
    "title": "Regression: modelling",
    "section": "Simple linear regression model",
    "text": "Simple linear regression model\nWe want to predict a response Y based on a predictor x for i number of observations:\nY_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}\nwhere\n\\epsilon_i \\sim N(0, \\sigma^2)\n\nY_i, the response, is an observed value of the dependent variable.\n\\beta_0, the constant, is the population intercept and is fixed.\n\\beta_1 is the population slope parameter, and like \\beta_0, is also fixed.\n\\epsilon_i is the error associated with predictions of y_i, and unlike \\beta_0 or \\beta_1, it is not fixed.\n\nBecause \\epsilon_i is the only part of the equation that is not fixed, we associate it with the residual error (observed-predicted). It would also cover other aspects of error (e.g. sampling error, parallax error) but these are hard to discern."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#fitting-the-model",
    "href": "lectures/L07/Lecture-07.html#fitting-the-model",
    "title": "Regression: modelling",
    "section": "Fitting the model",
    "text": "Fitting the model\n\n\\color{royalblue}{\\hat{y}_i} is the predicted value of y_i:\n\n\\color{royalblue}{\\hat{y}_i} = \\beta_0 + \\beta_1 x_i\n\nThe residual is the difference between the observed value of the response and the predicted value:\n\n\\hat\\epsilon_i = y_i - \\color{royalblue}{\\hat{y}_i}\n\nTherefore:\n\n\\hat\\epsilon_i = y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)}\n\nWe use the method of least squares and minimise the sum of the squared residuals (SS):\n\n\\sum_{i=1}^n \\hat\\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#section",
    "href": "lectures/L07/Lecture-07.html#section",
    "title": "Regression: modelling",
    "section": "",
    "text": "Finding the minimum SS requires solving the following problem:\n\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2\nWe can find \\beta_0 and \\beta_1 analytically. We first find \\beta_1:\n \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}}  And then substitute \\beta_1 into the equation for \\beta_0:\n \\beta_0 = \\bar{y} - \\beta_1 \\bar{x}"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#numerical-fitting",
    "href": "lectures/L07/Lecture-07.html#numerical-fitting",
    "title": "Regression: modelling",
    "section": "Numerical fitting",
    "text": "Numerical fitting\nComputers use “random guesses” to find set of parameters that minimises objective function (SS) – more computationally efficient and applies beyond linear regression.\n\nsource"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#fitting-a-model-in-r-is-easy-with-lm",
    "href": "lectures/L07/Lecture-07.html#fitting-a-model-in-r-is-easy-with-lm",
    "title": "Regression: modelling",
    "section": "Fitting a model in R is easy with lm()",
    "text": "Fitting a model in R is easy with lm()\n\n\nCode\nfit &lt;- lm(child ~ parent, data = Galton)\n\n\nThat’s it – the model has been fitted.\nBut there is a process similar to HATPC (hypothesis, assumptions, test, p-value, conclusions)."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#define-the-hypothesis",
    "href": "lectures/L07/Lecture-07.html#define-the-hypothesis",
    "title": "Regression: modelling",
    "section": "Define the hypothesis",
    "text": "Define the hypothesis\nH_0: \\beta_1=0\nH_1: \\beta_1 \\neq 0\nThe null model is a line with no slope (i.e. flat or horizontal) at the mean of the child height (\\bar{y} = 68 inches).\n\n\nCode\nlibrary(dplyr)\nnull_model &lt;- Galton %&gt;%\n  lm(child ~ 1, data = .) %&gt;%\n  broom::augment(Galton)\nlin_model &lt;- Galton %&gt;%\n  lm(child ~ parent, data = .) %&gt;%\n  broom::augment(Galton)\nmodels &lt;- bind_rows(null_model, lin_model) %&gt;%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 1\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 1\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\") +\n  theme_classic()"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions",
    "href": "lectures/L07/Lecture-07.html#assumptions",
    "title": "Regression: modelling",
    "section": "Assumptions",
    "text": "Assumptions\nThe data must meet certain criteria, which we often call assumptions. They can be remembered using LINE:\n\nLinearity. The relationship between y and x is linear.\nIndependence. The errors \\epsilon are independent.\nNormal. The errors \\epsilon are normally distributed.\nEqual Variance. At each value of x, the variance of y is the same i.e. homoskedasticity, or constant variance.\n\n\n\n\n\n\n\nTip\n\n\nAll but the independence assumption can be assessed using diagnostic plots."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-with-base-r-plot",
    "href": "lectures/L07/Lecture-07.html#assumptions-with-base-r-plot",
    "title": "Regression: modelling",
    "section": "Assumptions with base R plot()",
    "text": "Assumptions with base R plot()\n\n\nCode\npar(mfrow= c(2, 2)) # plots combined into 2x2 grid\nplot(fit)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-with-ggfortify-package-and-autoplot",
    "href": "lectures/L07/Lecture-07.html#assumptions-with-ggfortify-package-and-autoplot",
    "title": "Regression: modelling",
    "section": "Assumptions with ggfortify package and autoplot()",
    "text": "Assumptions with ggfortify package and autoplot()\n\n\nCode\nlibrary(ggfortify)\nautoplot(fit)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-using-performance",
    "href": "lectures/L07/Lecture-07.html#assumptions-using-performance",
    "title": "Regression: modelling",
    "section": "Assumptions using performance",
    "text": "Assumptions using performance\n(Also provides a guide on what to check for in the assumption plot)\n\n\nCode\nlibrary(performance)\nperformance::check_model(fit) # check all assumptions\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-linearity",
    "href": "lectures/L07/Lecture-07.html#assumption-linearity",
    "title": "Regression: modelling",
    "section": "Assumption: Linearity",
    "text": "Assumption: Linearity\n\n\nPrior knowledge and visual inspection comes into play. Does the relationship look approximately linear?\n\n\nCode\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nThe linearity assumption can be checked again by looking at a plot of the residuals against x (i.e. parent height).\n\n\nCode\nperformance::check_model(fit, check = \"linearity\")\n\n\n\n\n\n\n\n\n\n\n\nWhere the green reference line is &gt; 0, the model underestimates, and where it is &lt; 0, it overestimates.\nIf the linearity assumption is violated, we should not be fitting a linear model – transform or use a nonlinear model."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-independence",
    "href": "lectures/L07/Lecture-07.html#assumption-independence",
    "title": "Regression: modelling",
    "section": "Assumption: Independence",
    "text": "Assumption: Independence\nThis assumption is addressed during experimental design, but issues like correlation between errors and patterns occurring due to time are possible if:\n\nObservations of the same subject are related i.e. multicollinearity\nTime-series data, if the same subjects are sampled i.e. autocorrelation"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-normality",
    "href": "lectures/L07/Lecture-07.html#assumption-normality",
    "title": "Regression: modelling",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\n\nFor a given value of x, the residuals should be normally distributed. In a scatterplot of x and y, the points would appear evenly distributed (linear and no fanning).\n\n\n\n\nCode\nperformance::check_model(fit, check = c(\"normality\", \"qq\"))\n\n\n\n\n\n\n\n\n\n\n\nHow to interpret a QQ plot\nQQ plot interpretation"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assessing-normality-using-residuals",
    "href": "lectures/L07/Lecture-07.html#assessing-normality-using-residuals",
    "title": "Regression: modelling",
    "section": "Assessing normality using residuals",
    "text": "Assessing normality using residuals\n\nLight-tailed: small variance in residuals, resulting in a narrow distribution\nHeavy-tailed: many extreme positive and negative residuals, resulting in a wide distribution\nLeft-skewed (n shape): more data falls to the left of the mean\nRight-skewed (u shape): more data falls to the right of the mean\n\n\n\nHeavy-tailed, left-skewed.\n\n\nCode\nset.seed(1028)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x + rchisq(100, df = 3) * -1\ndf &lt;- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"qq\")))\n\n\n\n\n\n\n\n\n\n\nLight-tailed, right-skewed.\n\n\nCode\nset.seed(1028)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x + rnbinom(100, 10, .5)\ndf &lt;- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"qq\")))"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#asumption-equal-variances",
    "href": "lectures/L07/Lecture-07.html#asumption-equal-variances",
    "title": "Regression: modelling",
    "section": "Asumption: Equal variances",
    "text": "Asumption: Equal variances\n\n\nCode\nperformance::check_model(fit, check = c(\"homogeneity\", \"outliers\"))\n\n\n\nOutliers are not a strict assumption, but they will affect the model fit."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#what-is-a-standardised-residual",
    "href": "lectures/L07/Lecture-07.html#what-is-a-standardised-residual",
    "title": "Regression: modelling",
    "section": "What is a standardised residual?",
    "text": "What is a standardised residual?\n\nThe standardised residual is the residual divided by the standard error of the residual (normalised).\n\nStandardised\\ residual = \\frac{Residual}{Standard\\ error\\ of\\ the\\ residual}\n\nThe mean of the residuals is 0 in linear regression\nA standardised residual of 2 or above suggests the point is an outlier (far from the regression line)\nSpread should be random i.e. no pattern (fanning, W), which indicates equal variances"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#anova-and-linear-regression",
    "href": "lectures/L07/Lecture-07.html#anova-and-linear-regression",
    "title": "Regression: modelling",
    "section": "ANOVA and linear regression",
    "text": "ANOVA and linear regression\nANOVA is a variation of linear regression – both partition variance into sum of squares for residuals (variance explained) and sum of squares for error (variance not explained) aka the components of the F-statistic.\n\n\nANOVA Output\n\n\nCode\nfit &lt;- lm(child ~ parent, data = Galton)\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: child\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nparent      1 1236.9 1236.93  246.84 &lt; 2.2e-16 ***\nResiduals 926 4640.3    5.01                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nparent Sum Sq: the variation that parent explains in the child variable\nResiduals Mean Sq: variation (per degree of freedom) that the model does not explain\nThe F-value is the ratio, i.e. does parent explain enough variation in child to be considered significant?\n\n\\text{F-value} = \\frac{\\text{parent Sum Sq}}{\\text{Residuals Mean Sq}} = \\frac{1236.9}{5.01} = 246.84 \n\nRegression Output\n\nfit &lt;- lm(child ~ parent, data = Galton)\nsummary(fit)\n\n# F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#anova-and-linear-regression-1",
    "href": "lectures/L07/Lecture-07.html#anova-and-linear-regression-1",
    "title": "Regression: modelling",
    "section": "ANOVA and linear regression",
    "text": "ANOVA and linear regression\nANOVA is a variation of linear regression – both partition variance into sum of squares for residuals (variance explained) and sum of squares for error (variance not explained) aka the components of the F-statistic.\nANOVA Output\nThe ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p &lt; .001)\nRegression Output\nWe fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R2 = 0.21, F(1, 926) = 246.84, p &lt; .001). Within this model, the effect of parent is statistically significant and positive (\\beta_1 = 0.65, 95% CI [0.57, 0.73], t(926) = 15.71, p &lt; .001).\n\n\n\n\n\n\nNote\n\n\nFor simple linear regression, the significance of the predictor (i.e. child) is the same as the model significance."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#model-fit-1",
    "href": "lectures/L07/Lecture-07.html#model-fit-1",
    "title": "Regression: modelling",
    "section": "Model fit",
    "text": "Model fit\n\n\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\widehat{child} = 23.9 + 0.65 \\cdot parent\nFor every unit change in parent (i.e. 1 inch), we expect a 0.65 unit change in child.\nHow much variation is explained? R2 = 0.21 = 21%\n\nMultiple R2: proportion of variance in the response variable explained by the model.\nAdjusted R2: as above but adjusted for the number of predictors in the model.\n\nFor multiple linear regression\nIt only increases if the new term improves the model more than would be expected by chance\nAlways lower than multiple R2"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#making-predictions",
    "href": "lectures/L07/Lecture-07.html#making-predictions",
    "title": "Regression: modelling",
    "section": "Making predictions",
    "text": "Making predictions\nWhat is the predicted child height for a parent height of 70 inches?\n\n\nCode\nchild &lt;- 23.9 + 0.65 * 70\nchild\n\n\n[1] 69.4\n\n\nWe use predict() to make predictions – it takes in the lm() model, recreates the equation and applies it to new data.\n\n\nCode\npredict(fit, data.frame(parent = 70)) # using 70 as this is the value we want to sub in and predict\n\n\n       1 \n69.18187 \n\n\n\n\n\n\n\n\nNote\n\n\nHow good is our prediction actually? What if we had more parents and children, would the equation still hold up? We cover this in Week 9."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#what-if-assumptions-are-not-met",
    "href": "lectures/L07/Lecture-07.html#what-if-assumptions-are-not-met",
    "title": "Regression: modelling",
    "section": "What if assumptions are not met?",
    "text": "What if assumptions are not met?\nViolations of…\n\nLinearity can cause systematically wrong predictions\nHomoskedasticity makes it difficult to estimate “true” standard deviation of errors (i.e. noisy estimates)\nNormality can compromise inferences and hypothesis testing"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#how-do-we-solve-these-problems",
    "href": "lectures/L07/Lecture-07.html#how-do-we-solve-these-problems",
    "title": "Regression: modelling",
    "section": "How do we solve these problems?",
    "text": "How do we solve these problems?\n\nUse less restrictive (but more complicated) models, e.g. generalised linear models, non-parametric techniques (ENVX3002)\nPerform variance corrections (complicated)\nTransform the response variable (y) to stabilise variance and correct normality\nTransform the predictor variable (x) if issues still exist in the diagnostics\n\n\n\n\n\n\n\nNote\n\n\nWe can also perform transformations to improve the model fit, but beware of overfitting – we want to make reasonable predictions, not fit the data!"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#example-air-quality",
    "href": "lectures/L07/Lecture-07.html#example-air-quality",
    "title": "Regression: modelling",
    "section": "Example: air quality",
    "text": "Example: air quality\n\n\nDaily air quality measurements in New York, May to September 1973.\n\n\nCode\nstr(airquality)\n\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nWe start with one variable: is ozone concentration influenced by temperature?\n\n\nCode\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-checks",
    "href": "lectures/L07/Lecture-07.html#assumption-checks",
    "title": "Regression: modelling",
    "section": "Assumption checks",
    "text": "Assumption checks\n\n\nCode\nfit &lt;- lm(Ozone ~ Temp, data = airquality)\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n\n\n\nIs a simple linear model appropriate? Depends on your threshold for what is acceptable."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#backtransforming-fyi",
    "href": "lectures/L07/Lecture-07.html#backtransforming-fyi",
    "title": "Regression: modelling",
    "section": "Backtransforming – FYI",
    "text": "Backtransforming – FYI\nA log transformation (natural or a base) is relatively easy to back-transform.\n\\widehat{log(Ozone)}=\\color{royalblue}{-1.8380 + 0.0675 \\times Temp} \\widehat{Ozone}=e^{-1.8380 + 0.0675 \\times Temp}=e^{-1.8380} \\times e^{0.0675 \\times Temp} But given we are focused on a 1-unit change of Temp, \\widehat{Ozone} changes by e^{0.0675} = 1.07 times.\nIf this had been a sqrt() transformation…\n\\widehat{\\sqrt{Ozone}}=-1.8380 + 0.0675 \\times Temp \\widehat{Ozone}=(-1.8380 + 0.0675 \\times Temp)^2 = 3.3782−(0.2481×Temp)+(0.0675×Temp)^2"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#interpreting-log-transformations-fyi",
    "href": "lectures/L07/Lecture-07.html#interpreting-log-transformations-fyi",
    "title": "Regression: modelling",
    "section": "Interpreting log transformations – FYI",
    "text": "Interpreting log transformations – FYI\n\n\nLog-linear: Log(Y)=\\beta_0+\\beta_1x\n\nAn increase of x by 1 unit corresponds to a \\beta_1 unit increase in log(Y)\nAn increase of x by 1 unit corresponds to approximately a \\beta_1 \\times 100\\% increase in Y\n\n\n\n\n\nLinear-log: Y=\\beta_0+\\beta_1log(x)\n\nAn increase of 1\\% in x corresponds to a \\frac{\\beta_1}{100} increase in Y\n\n\n\n\n\nLog-log: Log(Y)=\\beta_0+\\beta_1log(x)\n\nAn increase of 1\\% in x corresponds to a \\beta_1\\% increase in Y"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#percent-change-with-ln-transformation-fyi",
    "href": "lectures/L07/Lecture-07.html#percent-change-with-ln-transformation-fyi",
    "title": "Regression: modelling",
    "section": "Percent change with ln transformation – FYI",
    "text": "Percent change with ln transformation – FYI\nInterpreting as a percent change can be more meaningful - it can be done with any log transformation (substitute e below for 10 or any other base), but the quick approximation only works with natural log transformations.\nIf y has been transformed with a natural log (log(y)), for a one-unit increase in x the percent change in y (not log(y)) is calculated with:\n\\Delta y \\% = 100 \\cdot (e^{\\beta_1}-1)\nIf \\beta_1 is small (i.e. -0.25 &lt; \\beta_1 &lt; 0.25), then: e^{\\beta_1} \\approx 1 + \\beta_1. So \\Delta y \\% \\approx 100 \\cdot \\beta_1.\n\n\n\n\n\n\n\n\n\n\nβ\nExact (e^{\\beta} - 1)%\nApproximate 100 \\cdot \\beta\n\n\n\n\n-0.25\n-22.13\n-25\n\n\n-0.1\n-9.52\n-10\n\n\n0.01\n1.01\n1\n\n\n0.1\n10.52\n10\n\n\n0.25\n28.41\n25\n\n\n0.5\n64.87\n50\n\n\n2\n638.91\n200\n\n\n\n\n\ny transformed: a one-unit increase in x is approximately a \\beta_1% change in y.\nx transformed: a 1% increase in x is approximately a 0.01 \\cdot \\beta_1 change in y.\nBoth x and y transformed: a 1% increase in x is approximately a \\beta_1% change in y."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#transforming-ozone",
    "href": "lectures/L07/Lecture-07.html#transforming-ozone",
    "title": "Regression: modelling",
    "section": "Transforming Ozone",
    "text": "Transforming Ozone\nLet’s transform Ozone using the natural log (log()).\n\n\nCode\nfit_log &lt;- lm(log(Ozone) ~ Temp, data = airquality)\n\n\n\n\n\nBefore\n\n\nCode\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (ppb)\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(subtitle = paste(\"Correlation:\", round(cor(airquality$Temp, airquality$Ozone), 2)))\n\n\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nggplot(airquality, aes(x = Temp, y = log(Ozone))) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"log(Ozone) (ppb)\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(subtitle = paste(\"Correlation:\", round(cor(airquality$Temp, log(airquality$Ozone)), 2)))"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-linearity-1",
    "href": "lectures/L07/Lecture-07.html#assumption-linearity-1",
    "title": "Regression: modelling",
    "section": "Assumption: Linearity",
    "text": "Assumption: Linearity\n\n\nBefore\n\n\nCode\nautoplot(fit, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nautoplot(fit_log, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-normality-1",
    "href": "lectures/L07/Lecture-07.html#assumption-normality-1",
    "title": "Regression: modelling",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\n\nBefore\n\n\nCode\nautoplot(fit, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nautoplot(fit_log, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-equal-variances",
    "href": "lectures/L07/Lecture-07.html#assumption-equal-variances",
    "title": "Regression: modelling",
    "section": "Assumption: Equal variances",
    "text": "Assumption: Equal variances\n\n\nBefore\n\n\nCode\nautoplot(fit, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nautoplot(fit_log, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#is-transforming-better",
    "href": "lectures/L07/Lecture-07.html#is-transforming-better",
    "title": "Regression: modelling",
    "section": "Is transforming better?",
    "text": "Is transforming better?\n\n\nBefore\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\nAfter\n\n\nCode\nsummary(fit_log)\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.14469 -0.33095  0.02961  0.36507  1.49421 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.83797    0.45100  -4.075 8.53e-05 ***\nTemp         0.06750    0.00575  11.741  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5848 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5473,    Adjusted R-squared:  0.5434 \nF-statistic: 137.8 on 1 and 114 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#can-we-use-more-predictors",
    "href": "lectures/L07/Lecture-07.html#can-we-use-more-predictors",
    "title": "Regression: modelling",
    "section": "Can we use more predictors?",
    "text": "Can we use more predictors?\n\n\nCode\npsych::pairs.panels(airquality)\n\n\n\nCan we improve the current model by adding wind and solar radiation as additional predictors?"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-1",
    "href": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-1",
    "title": "Regression: modelling",
    "section": "Can we use more predictors?",
    "text": "Can we use more predictors?\nFrom:\nlog(size)_i = \\beta_0 + \\beta_1Temp_i + \\epsilon_i\nTo:\nlog(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-2",
    "href": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-2",
    "title": "Regression: modelling",
    "section": "Can we use more predictors?",
    "text": "Can we use more predictors?\nlog(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i\n\n\nCode\nmulti_fit &lt;- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-1",
    "href": "lectures/L07/Lecture-07.html#assumptions-1",
    "title": "Regression: modelling",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nCode\nperformance::check_model(multi_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n\n\n\nThere is one additional assumption for multiple linear regression. Collinearity is when two or more predictors are very highly correlated. If the predictors are basically identical, the model cannot distinguish how much variability each explains. (Correlations in previous slides look fine)."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#hypothesis",
    "href": "lectures/L07/Lecture-07.html#hypothesis",
    "title": "Regression: modelling",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor multiple linear regression, there are two hypothesis tests:\n\nIndividual predictors, where the significance of each predictor is tested via t-tests\n\nH_0: \\beta_k = 0 H_1: \\beta_k \\neq 0\n\nThe overall model, which is tested with an F-test (to get F-stat). H_0 is an intercept-only model (i.e. the mean), so if at least one predictor is useful, the model is better than the intercept-only model.\n\nH_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0 H_1: \\text{At least one } \\beta_k \\neq 0"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#model-fit-2",
    "href": "lectures/L07/Lecture-07.html#model-fit-2",
    "title": "Regression: modelling",
    "section": "Model Fit",
    "text": "Model Fit\n\n\nCode\nsummary(multi_fit)\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\nModel equation:\n\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#interpretation",
    "href": "lectures/L07/Lecture-07.html#interpretation",
    "title": "Regression: modelling",
    "section": "Interpretation",
    "text": "Interpretation\n\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind\nHolding all other variables constant:\n\nA one degree (°F) increase in Temp is associated with a 4.9% increase in Ozone concentration.\nA one unit increase in Solar.R is associated with a 0.25% increase in Ozone concentration.\nA one unit increase in Wind is associated with a 6.2% decrease in Ozone concentration.\n\nAutomating extracting the model equation into latex using extract_eq() from the package equatiomatic:\n\n\nCode\nequatiomatic::extract_eq(multi_fit, use_coefs = TRUE, coef_digits = 3) |&gt; print()\n\n\n$$\n\\operatorname{\\widehat{log(Ozone)}} = -0.262 + 0.049(\\operatorname{Temp}) + 0.003(\\operatorname{Solar.R}) - 0.062(\\operatorname{Wind})\n$$"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#is-mlr-model-better",
    "href": "lectures/L07/Lecture-07.html#is-mlr-model-better",
    "title": "Regression: modelling",
    "section": "Is MLR model better?",
    "text": "Is MLR model better?\n\n\nCode\nsjPlot::tab_model(fit_log, multi_fit, digits = 4, show.ci = FALSE)\n\n\n\n\n\n \nlog(Ozone)\nlog(Ozone)\n\n\nPredictors\nEstimates\np\nEstimates\np\n\n\n(Intercept)\n-1.8380\n&lt;0.001\n-0.2621\n0.637\n\n\nTemp\n0.0675\n&lt;0.001\n0.0492\n&lt;0.001\n\n\nSolar R\n\n\n0.0025\n&lt;0.001\n\n\nWind\n\n\n-0.0616\n&lt;0.001\n\n\nObservations\n116\n111\n\n\nR2 / R2 adjusted\n0.547 / 0.543\n0.664 / 0.655\n\n\n\n\n\n\n\n\nThe adjusted R^2 is higher for the MLR model…\nInterpretation of R^2 is the same as for simple linear regression: how much of the variation in the response variable is explained by the model\nAre all the variables/predictors needed? (next week)"
  },
  {
    "objectID": "lectures/L05/index.html",
    "href": "lectures/L05/index.html",
    "title": "Lecture 05",
    "section": "",
    "text": "Important\n\n\n\nLecture 05 is not available in Quarto. Please refer to the Canvas site to access the lecture material."
  },
  {
    "objectID": "lectures/L03/index.html",
    "href": "lectures/L03/index.html",
    "title": "Lecture 03",
    "section": "",
    "text": "Lecture 03a – t-tests Full Screen | PDF\nLecture 03b – One-way ANOVA Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1: Designed studies**",
      "L03 -- 1-way ANOVA"
    ]
  },
  {
    "objectID": "lectures/L02/index.html",
    "href": "lectures/L02/index.html",
    "title": "Lecture 02",
    "section": "",
    "text": "Lecture 02a – Sampling designs Full Screen | PDF\n\nLecture 02a – Sampling designs II Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1: Designed studies**",
      "L02 -- Sampling designs"
    ]
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#key-concepts-we-covered",
    "href": "lectures/L02/Lecture-02a.html#key-concepts-we-covered",
    "title": "Lecture 02a – Sampling designs",
    "section": "Key concepts we covered",
    "text": "Key concepts we covered\n\nPopulation vs. sample\n\nPopulation: The complete set of all items we’re studying\nSample: A subset of the population we actually measure\n\nParameters (population) and statistics (sample)\n\nCentral tendency: how data clusters around a middle value\n\nmean (average), median (middle value), mode (most common)\n\nSpread/dispersion: how data points vary from each other\n\nvariance (average squared deviation), standard deviation (square root of variance)\n\n\nConfidence intervals – we’ll explore this further today!"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#overview",
    "href": "lectures/L02/Lecture-02a.html#overview",
    "title": "Lecture 02a – Sampling designs",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\nAspect\nObservational study\nControlled experiment\n\n\n\n\nControl\nNo control over the variables of interest: mensurative and absolute\nControl over the variables of interest: comparative and manipulative\n\n\nCausation\nCannot establish causation, but perhaps association\nCan establish causation\n\n\nFeasibility\nCan be done in many cases\nMay be destructive and thus cannot always be done\n\n\nExamples\nSurveys, monitoring studies, correlational studies, case-control studies, cohort studies\nClinical trials, A/B testing, laboratory experiments, field experiments\n\n\nStatistical Tests\nCorrelation, regression, chi-squared tests, t-tests, one-way ANOVA, time series analysis\nT-tests, one-way ANOVA, factorial ANOVA, regression\n\n\n\n\n\n\n\nWe will focus on the fundamentals behind observational studies this week.\n\n\n\n\n\n\nTip\n\n\nMensurative studies involve measuring without manipulating variables.\nAbsolute studies measure actual values rather than comparing between treatments."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#observational-studies-two-common-types",
    "href": "lectures/L02/Lecture-02a.html#observational-studies-two-common-types",
    "title": "Lecture 02a – Sampling designs",
    "section": "Observational studies: two common types",
    "text": "Observational studies: two common types\n\nSurveys\n\nEstimate a statistic (e.g. mean, variance), but\nno temporal change during estimate.\nE.g. measuring species richness in a forest.\nThink of it as a snapshot at one point in time.\n\n\n\nMonitoring studies\n\nEstimate a change in statistic (same as above), and\ntemporal change across observations, i.e. before and after.\nE.g. measuring species richness in a forest before and after a fire.\nThink of it as taking multiple snapshots over time to see changes."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#sampling-designs",
    "href": "lectures/L02/Lecture-02a.html#sampling-designs",
    "title": "Lecture 02a – Sampling designs",
    "section": "Sampling designs",
    "text": "Sampling designs\n\nSimple random sampling:\n\nEach unit has an equal chance of being selected.\nRandomly sample units from the entire population.\nLike putting all names in a hat and drawing some out randomly.\n\n\n\n\n\n\n\n\nStratified random sampling\n\nThe population is first divided into strata (groups with similar characteristics).\nRandomly sample units within each strata by simple random sampling.\nLike separating students by year level, then randomly selecting some from each year."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#what-is-random-sampling",
    "href": "lectures/L02/Lecture-02a.html#what-is-random-sampling",
    "title": "Lecture 02a – Sampling designs",
    "section": "What is “random” sampling?",
    "text": "What is “random” sampling?\n\nRandom selection of finite or infinite population units.\n\n\n\nWhat does random mean?\n\n\n\nWithin a population, all units have a &gt; 0 probability of being selected i.e. everything has a chance to be selected.\n\n\nThis chance is called the inclusion probability (\\pi_i):\n\n\\pi_i is equal within a population unit – i.e. all units have the same chance of being selected.\n\\pi_i not necessarily equal between different population units – i.e. units from different groups may have different chances of being selected - more on this later.\n\n\n\n\n\nHow do we perform random sampling in real life?\n\nRandom number generator (RNG) – e.g. R’s sample() function.\nRandom number table – e.g. Random number table by the National Institute of Standards and Technology (NIST).\nThink of it like rolling dice or drawing names from a hat, but using mathematics to ensure true randomness."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#we-know-that",
    "href": "lectures/L02/Lecture-02a.html#we-know-that",
    "title": "Lecture 02a – Sampling designs",
    "section": "We know that…",
    "text": "We know that…\nFrom the previous lecture\n\nSample mean is a good measure of central tendency (the “middle” of our data).\nSample variance is a good measure of dispersion (how spread out our data is).\nSample size affects the precision of the sample mean (more samples = more precise).\n\n\nCan we combine all of the above in a single statistic?\nYes! That’s where confidence intervals come in."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#combining-an-estimate-with-its-precision",
    "href": "lectures/L02/Lecture-02a.html#combining-an-estimate-with-its-precision",
    "title": "Lecture 02a – Sampling designs",
    "section": "Combining an estimate with its precision",
    "text": "Combining an estimate with its precision\nA confidence interval (CI) is:\n\nA range of values that likely contains the true population value\nLike saying “We’re 95% confident that out of 100 samples, 95 of them will be within this range”\nCrucial for hypothesis testing and estimation, the basis of statistical inference\n\nYou will often see CIs in scientific papers, reports, and news articles."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#calculating-confidence-intervals",
    "href": "lectures/L02/Lecture-02a.html#calculating-confidence-intervals",
    "title": "Lecture 02a – Sampling designs",
    "section": "Calculating confidence intervals",
    "text": "Calculating confidence intervals\nWhat we need\n\nEstimate of the population parameter, i.e. the sample mean (\\bar{x}) - our best guess of the true value\nCritical value (t_{n-1}) - a number that represents our chosen confidence level (like 95%)\nStandard error of the estimate, SE of the mean (SE_{\\bar{x}}) - tells us how precise our mean is\n\n\n\n\n\n\n\nNote\n\n\nThink of standard error as “how much our sample means would vary if we took many different samples and calculated the mean each time”\n\n\n\nAll these (mean, standard error and critical value) are combined to form the confidence interval."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#breakdown",
    "href": "lectures/L02/Lecture-02a.html#breakdown",
    "title": "Lecture 02a – Sampling designs",
    "section": "Breakdown",
    "text": "Breakdown\nIn general, a CI has the form: \\text{estimate} \\pm \\text{margin of error}\nwhere the margin of error is a function of the standard error of the estimate:\n\\text{estimate} \\pm (\\text{critical value} \\times \\text{standard error (estimate)})\nwhere the critical value is based on the sampling distribution of the estimate i.e. the t-distribution.\n\n\n\n\n\n\nTip\n\n\nThink of margin of error like the “plus or minus” value you often see in survey results (±3%)"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#formula-for-95-confidence-interval-ci",
    "href": "lectures/L02/Lecture-02a.html#formula-for-95-confidence-interval-ci",
    "title": "Lecture 02a – Sampling designs",
    "section": "Formula for 95% Confidence Interval (CI)",
    "text": "Formula for 95% Confidence Interval (CI)\n\n\\bar{x} \\pm \\left(t_{n-1} \\times \\frac{s}{\\sqrt{n}}\\right)\n\nStep-by-step calculation by hand\n\nCalculate the sample mean, \\bar{x} (add all values and divide by number of samples)\nCalculate the sample standard deviation, s (measure of spread in your data)\nDetermine the standard error of the mean, SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}} (how precise your mean estimate is)\nLook up the t-value, t_{n-1}, from the t-distribution table for the 95% confidence level and n-1 degrees of freedom.\n\nThis is a specific number based on how many samples you have\n\nCompute the margin of error: \\text{Margin of Error} = t_{n-1} \\times SE_{\\bar{x}}\nFinally, the 95% CI is: \\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})\n\nYou need to be able to calculate this by hand/calculator."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions",
    "href": "lectures/L02/Lecture-02a.html#more-definitions",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nDegrees of freedom (df)\nThe number of values in a sample that are free to vary while still maintaining the same statistic.\n\n\\text{df} = n - 1\n\nwhere n is the number of samples.\n\nExample\nImagine you have 4 numbers with a mean of 5:\n\nYou can choose the first three numbers freely: 3, 10, and 7\nBut the fourth number MUST be 0 to make the mean = 5\n\n(3 + 10 + 7 + 0) ÷ 4 = 5\n\nSo only 3 numbers (n-1) can be freely chosen = 3 degrees of freedom"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions-1",
    "href": "lectures/L02/Lecture-02a.html#more-definitions-1",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nt-critical value\nA number that helps determine how wide to make your confidence interval.\n\nBased on your confidence level (e.g. 95%) and sample size\nLarger t-values = wider intervals = more confidence but less precision\nSmaller sample sizes = larger t-values (because we’re less certain)"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions-2",
    "href": "lectures/L02/Lecture-02a.html#more-definitions-2",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nt-distribution\nA probability distribution that accounts for the uncertainty when estimating from small samples.\n\nSimilar to the normal “bell curve” distribution, but with heavier tails.\nWith few samples (small n), the t-distribution is wider, reflecting greater uncertainty.\nAs sample size increases, the t-distribution gets closer to the normal distribution.\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(gganimate)\n\n# Variable to control animation speed (higher values = slower transitions)\nanim_speed &lt;- 1\n\n# Create a sequence of x values for the plot\nx &lt;- seq(-4, 4, length.out = 400)\n\n# Degrees of freedom: 1 through 5 then even numbers from 6 to 30\ndfs &lt;- c(1:5, seq(6, 30, by = 2))\n\n# Prepare data for t-distribution curves with a new \"df\" column\nt_curves &lt;- do.call(rbind, lapply(dfs, function(df) {\n  data.frame(\n    x = x,\n    density = dt(x, df),\n    df = df\n  )\n}))\n\n# Create data for the standard normal distribution (static)\nnormal_curve &lt;- data.frame(\n  x = x,\n  density = dnorm(x)\n)\n\n# Plot with gganimate: animate t-distribution curves (each frame shows one df)\np &lt;- ggplot() +\n  # Static normal distribution curve\n  geom_line(\n    data = normal_curve, aes(x = x, y = density),\n    color = \"black\", linetype = \"dashed\", size = 1\n  ) +\n  # t-distribution curve that will animate\n  geom_line(\n    data = t_curves, aes(x = x, y = density, color = factor(df)),\n    size = 1\n  ) +\n  labs(\n    title = \"Degrees of Freedom: {closest_state}\",\n    x = \"x\",\n    y = \"Density\",\n    subtitle = \"Dashed line = Normal distribution; Solid line = t-distribution\"\n  ) +\n  theme(legend.position = \"none\") +\n  transition_states(states = df, transition_length = anim_speed, state_length = anim_speed)\n\np\n\n\n\n\n\n\n\n\n\nNote\n\n\nNotice how the t-distribution (solid line) gets closer to the normal distribution (dashed line) as the degrees of freedom increase!"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#interpreting-confidence-intervals",
    "href": "lectures/L02/Lecture-02a.html#interpreting-confidence-intervals",
    "title": "Lecture 02a – Sampling designs",
    "section": "Interpreting confidence intervals",
    "text": "Interpreting confidence intervals\n\nConfidence intervals depend on a specified confidence level (e.g. 95%, 99%) with higher confidence levels producing wider intervals (i.e. more conservative).\nAnother way to think of it: a range of values that we are fairly sure contains the true value of the population parameter.\n\n\nFishing analogy\nA confidence interval is like a fishing net:\n\nA wider net (interval) is more likely to catch the fish (true value)\nA spear (single point estimate) is less likely to catch the fish\nThe net width represents our uncertainty about the true value\n\n\n\n\n\n\n\nImportant\n\n\nCommon misunderstanding: A 95% CI does NOT mean there’s a 95% chance the true value is inside the interval. It means if you took 100 different samples and made 100 different CIs, about 95 of them would contain the true value."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#soil-carbon",
    "href": "lectures/L02/Lecture-02a.html#soil-carbon",
    "title": "Lecture 02a – Sampling designs",
    "section": "Soil carbon",
    "text": "Soil carbon\n\nSoil carbon content was measured at 7 locations across the area. The amount at each location was 48, 56, 90, 78, 86, 71, 42 tonnes per hectare (t/ha).\n\n\nCode\nsoil &lt;- c(48, 56, 90, 78, 86, 71, 42)\nsoil\n\n\n[1] 48 56 90 78 86 71 42\n\n\nWhat is the mean soil carbon content and how confident are we in this estimate? How this is calculated depends on whether we used simple random sampling or stratified random sampling."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#mean-and-95-ci",
    "href": "lectures/L02/Lecture-02a.html#mean-and-95-ci",
    "title": "Lecture 02a – Sampling designs",
    "section": "Mean and 95% CI",
    "text": "Mean and 95% CI\nStep-by-step calculation\n\n\nMean: \\bar{x} = \\frac{48+56+90+78+86+71+42}{7} \\approx 67.3 t/ha\nStandard deviation: s \\approx 18.84 t/ha\n\nThis tells us how much individual measurements vary from the mean\n\nStandard error: SE = \\frac{s}{\\sqrt{7}} \\approx 7.12 t/ha\n\nThis tells us how precise our estimate of the mean is\n\nt-value (95% CI, df = 6): t_{0.975,6} \\approx 2.447\n\nThis is the critical value for 95% confidence with 6 degrees of freedom\n\nMargin of error: t_{0.975,6} \\times SE \\approx 17.43 t/ha\n\nThis is the “plus or minus” value for our interval\n\nWhich gives: (67.3 - 17.43, 67.3 + 17.43) = (49.87, 84.73) t/ha\n\n\n\nAnd so we report the mean soil carbon content as 67.3 t/ha with a 95% CI of (49.87, 84.73) t/ha or 67.3 ± 17.43 t/ha.\nNote: Our confidence interval is quite wide relative to our mean (about ±26% of the mean value), suggesting moderate uncertainty in our estimate."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#implementation-in-r",
    "href": "lectures/L02/Lecture-02a.html#implementation-in-r",
    "title": "Lecture 02a – Sampling designs",
    "section": "Implementation in R",
    "text": "Implementation in R\nManual calculation\n\n# Step 1: Calculate the sample mean of soil carbon content\nmean_soil &lt;- mean(soil)\n\n# Step 2: Calculate the sample standard deviation of soil carbon content\nsd_soil &lt;- sd(soil)\n\n# Step 3: Calculate the standard error of the mean (SE)\nse_soil &lt;- sd_soil / sqrt(length(soil))\n\n# Step 4: Calculate the t-critical value for a 95% confidence interval\nt_crit &lt;- qt(0.975, df = length(soil) - 1)\n\n# Step 5: Calculate the margin of error (t_crit * SE)\n# and then determine the lower and upper bounds of the confidence interval\nci &lt;- mean_soil + c(-1, 1) * (t_crit * se_soil)\n\n# Step 6: View the calculated 95% confidence interval\nci\n\n[1] 49.84627 84.72516\n\n\nThere are ways to calculate this in R quickly, but it is important to understand the manual calculation.\n\n\n\n\n\n\nUnderstanding the R code\n\n\n\nmean() calculates the average value\nsd() calculates the standard deviation\nqt(0.975, df=6) gives the t-critical value (we use 0.975 because we want 95% in the middle, with 2.5% in each tail)\nc(-1, 1) creates a vector to subtract and add the margin of error"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#questions",
    "href": "lectures/L02/Lecture-02a.html#questions",
    "title": "Lecture 02a – Sampling designs",
    "section": "Questions",
    "text": "Questions\n\nHow precise is our estimate?\n\nOur margin of error is about 17.43 t/ha, which is roughly 26% of our mean value.\n\nHow big a change must there be to estimate a statistically significant change?\n\nChanges larger than about 17.43 t/ha would likely be statistically significant.\n\nCan we sample more efficiently?\n\nYes! This is where stratified sampling becomes valuable.\n\n\nTo answer these questions, we need to compare simple random sampling with a hypothetical stratified random sampling design (i.e. what if we had considered stratification before sampling?)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#assumed-knowledge",
    "href": "lectures/L01/Lecture-01b.html#assumed-knowledge",
    "title": "Lecture 01b – Revision",
    "section": "Assumed knowledge",
    "text": "Assumed knowledge\n\nSamples, populations and statistical inference\nProbability distributions\nParameter estimation\n\ncentral tendency\nspread or variability\n\nSampling distribution of the mean\n\nStandard error\nConfidence intervals\nCentral Limit Theorem"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#why-is-this-important",
    "href": "lectures/L01/Lecture-01b.html#why-is-this-important",
    "title": "Lecture 01b – Revision",
    "section": "Why is this important?",
    "text": "Why is this important?\n\nUnderstanding sampling informs experimental design (Week 4 onward). How many samples do we need and are our samples representative?\nRecognising sample (not sampling) distributions helps us choose the right statistical model – e.g. t-test to compare two means that are normally distributed.\nMost statistical techniques use sample statistics for interpretation, e.g. the t-test can be explained using confidence intervals, and the ANOVA test can be interpreted in part using means and standard errors.\n\nAll of these concepts will make more sense as we go through the course, but if you do not try to understand them now, you will struggle."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#populations-and-samples",
    "href": "lectures/L01/Lecture-01b.html#populations-and-samples",
    "title": "Lecture 01b – Revision",
    "section": "Populations and samples",
    "text": "Populations and samples\nPopulations\n\nAll the possible units and their associated observations of interest\nScientists are often interested in making inferences about populations, but measuring every unit is impractical\n\nSamples\n\nA collection of observations from any population is a sample, and the number of observations in it is the sample size\nWe assume samples that we collect can be used to make inferences about the population\nNEW: Samples need to be representative of the population"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#statistics-vs-parameters",
    "href": "lectures/L01/Lecture-01b.html#statistics-vs-parameters",
    "title": "Lecture 01b – Revision",
    "section": "Statistics vs parameters",
    "text": "Statistics vs parameters\n\nCharacteristics of the population are called parameters (e.g. population mean or population regression slope)\nCharacteristics of the sample are called statistics (e.g. sample mean or sample regression slope) – they are used to estimate the population parameters\nStatistics are what we use to help us understand the population\nFormal statistical methods can help us make inferences about the population based on the sample – statistical inference\nNot all statistical techniques are inferential, but many are"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sample-data",
    "href": "lectures/L01/Lecture-01b.html#sample-data",
    "title": "Lecture 01b – Revision",
    "section": "Sample data",
    "text": "Sample data\nSample data are usually collected as variables, which are the characteristics we measure or record from each object.\n\nVariables can be:\nCategorical Variables\n\nNominal: categories without a natural order (e.g. colors, names)\nOrdinal: categories with a natural order (e.g. ratings, rankings)\n\nNumerical Variables\n\nContinuous: can take any value within a range (e.g. height, weight)\nDiscrete: can take only specific values (e.g. counts, presence/absence)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#you-decide-on-what-a-variable-represents",
    "href": "lectures/L01/Lecture-01b.html#you-decide-on-what-a-variable-represents",
    "title": "Lecture 01b – Revision",
    "section": "YOU decide on what a variable represents",
    "text": "YOU decide on what a variable represents\nA numerical, continuous variable can be treated as a categorical variable if you decide to categorise it.\nExamples\n\n\nheight (in cm) – a numerical, continuous variable, can be treated as a categorical variable if you group it into categories (short, medium, tall)\nage (in years) – a numerical, discrete variable, can be treated as a continuous variable (if we allow for certain issues)\ntreatment (A, B, C) – a categorical variable, can be treated as a numerical variable if we assign numbers to the treatments (1, 2, 3) and assume they are ordered e.g. effect of 1 &lt; 2 &lt; 3 – the basis of non-parametric tests"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#types-of-probability-distributions",
    "href": "lectures/L01/Lecture-01b.html#types-of-probability-distributions",
    "title": "Lecture 01b – Revision",
    "section": "Types of probability distributions",
    "text": "Types of probability distributions\nPopulations can be described by probability distributions, and by now, you should be familiar with these distributions and their properties\n\n\nNormal Distribution: Bell-shaped curve, symmetric around the mean. Data is continuous\nBinomial Distribution: Models success/failure outcomes in a fixed number of trials. Data is discrete\nPoisson Distribution: Models count data when events occur at a constant rate. Data is discrete\n\n\n\nKnowing the distribution of your data is important for choosing the right statistical model – although it is not always necessary."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#section",
    "href": "lectures/L01/Lecture-01b.html#section",
    "title": "Lecture 01b – Revision",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nset.seed(908)\nnormal_data &lt;- data.frame(x = rnorm(10000, mean = 0, sd = 1))\nbinomial_data &lt;- data.frame(x = rbinom(10000, size = 10, prob = 0.5))\npoisson_data &lt;- data.frame(x = rpois(500, lambda = 3))\n\n# normal\np1 &lt;- ggplot(normal_data, aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  ggtitle(\"Normal Distribution\")\n\n# binomial\np2 &lt;- ggplot(binomial_data, aes(x = x)) +\n  geom_bar(fill = \"lightgreen\", color = \"black\") +\n  ggtitle(\"Binomial Distribution\")\n\n# poisson\np3 &lt;- ggplot(poisson_data, aes(x = x)) +\n  geom_bar(fill = \"lightpink\", color = \"black\") +\n  ggtitle(\"Poisson Distribution\")\n\n# Arrange plots\nlibrary(patchwork)\np1 | p2 / p3"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-central-tendency",
    "href": "lectures/L01/Lecture-01b.html#measures-of-central-tendency",
    "title": "Lecture 01b – Revision",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nMean \\bar{x}\n\nThe arithmetic average of all values in a dataset\nSum of all values divided by number of observations\nSensitive to extreme values (outliers)\n\n\n\n\n\n\n\n\nFormula\n\n\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\n\nwhere n is the number of observations\nx_i represents each individual value\n\\sum means we add up all values from i=1 to n\nExample: for data {2,4,6,8}, n=4 and \\bar{x} = \\frac{2+4+6+8}{4} = 5"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#section-1",
    "href": "lectures/L01/Lecture-01b.html#section-1",
    "title": "Lecture 01b – Revision",
    "section": "",
    "text": "Median\n\nMiddle value when data is ordered\n50th percentile of the data\nMore robust to outliers than mean\nFor even n, average of two middle values\n\n\nMode\n\nMost frequently occurring value\nCan have multiple modes\nOnly measure of central tendency for categorical data\nNot always meaningful for continuous data"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#how-they-compare",
    "href": "lectures/L01/Lecture-01b.html#how-they-compare",
    "title": "Lecture 01b – Revision",
    "section": "How they compare",
    "text": "How they compare\n\nDepending on the distribution of the data, the mean and median can be different, and this can tell you something about the data."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nVariance s^2\n\nMeasures how spread out the data is from the mean\nCalculated as average squared deviations from the mean\nSquared units make it harder to interpret\nSensitive to outliers (squares large deviations)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion-1",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion-1",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\n\n\n\n\n\n\nFormula\n\n\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\n\nwhere n is the number of observations\nx_i represents each individual value\n\\bar{x} is the sample mean\nFor data {2,4,6,8}:\n\nmean = 5\ndifferences = (-3,-1,1,3), squares = (9,1,1,9), sum = 20\nTherefore, s^2 = \\frac{20}{3} \\approx 6.67"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion-2",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion-2",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nStandard Deviation s\n\nSquare root of variance\nSame units as original data\nMore interpretable than variance\nEmpirical rule for normal distributions:\n\n≈68% of data within ±1 SD\n≈95% of data within ±2 SD\n≈99.7% of data within ±3 SD\n\n\n\n\n\n\n\n\n\nFormula\n\n\ns = \\sqrt{s^2}\n\nSimply the square root of variance\nFor our example data {2,4,6,8}:\n\ns = \\sqrt{6.67} \\approx 2.58\n\nInterpretation: On average, values deviate about 2.58 units from the mean"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#visualising-standard-deviation",
    "href": "lectures/L01/Lecture-01b.html#visualising-standard-deviation",
    "title": "Lecture 01b – Revision",
    "section": "Visualising standard deviation",
    "text": "Visualising standard deviation"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#population-parameters-vs-sample-statistics",
    "href": "lectures/L01/Lecture-01b.html#population-parameters-vs-sample-statistics",
    "title": "Lecture 01b – Revision",
    "section": "Population parameters vs sample statistics",
    "text": "Population parameters vs sample statistics\nFor those of you interested:\nMean\n\n\n\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\n\n\n\nVariance\n\n\n\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\n\n\n\nStandard Deviation\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\sigma = \\sqrt{\\sigma^2}\ns = \\sqrt{s^2}\n\n\n\n\n\n\n\n\n\nNote\n\n\nNotice the use of n-1 in sample variance and standard deviation"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#why-n-1",
    "href": "lectures/L01/Lecture-01b.html#why-n-1",
    "title": "Lecture 01b – Revision",
    "section": "Why n-1?",
    "text": "Why n-1?\n\nWhen calculating sample variance, we use n-1 instead of n in the denominator\nThis is called “Bessel’s correction”\nWhy? Because we lose one “degree of freedom” when we estimate the mean:\n\nIf you know the sample mean (\\bar{x})\nAnd you know all but one value in your sample\nThe last value is constrained - it must make the mean equal \\bar{x}"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#what-is-a-sampling-distribution",
    "href": "lectures/L01/Lecture-01b.html#what-is-a-sampling-distribution",
    "title": "Lecture 01b – Revision",
    "section": "What is a sampling distribution?",
    "text": "What is a sampling distribution?\n\nDistribution of a statistic (e.g., mean) calculated from repeated samples\nShows how sample statistics vary from sample to sample\nImportant for understanding sampling variability and making inferences"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sampling-distribution-of-the-mean",
    "href": "lectures/L01/Lecture-01b.html#sampling-distribution-of-the-mean",
    "title": "Lecture 01b – Revision",
    "section": "Sampling distribution of the mean",
    "text": "Sampling distribution of the mean"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#central-limit-theorem",
    "href": "lectures/L01/Lecture-01b.html#central-limit-theorem",
    "title": "Lecture 01b – Revision",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nI know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the Central Limit Theorem. The law would have been personified by the Greeks and deified, if they had known of it.”\n\n– Sir Francis Galton, 1889, Natural Inheritance\nThe Central Limit Theorem (CLT) states that for sufficiently large samples:\n\nThe sampling distribution of the mean follows a normal distribution\nThe mean of the sampling distribution equals the population mean\nThe standard deviation of the sampling distribution (standard error) = \\frac{\\sigma}{\\sqrt{n}}"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#clt-in-action",
    "href": "lectures/L01/Lecture-01b.html#clt-in-action",
    "title": "Lecture 01b – Revision",
    "section": "CLT in action",
    "text": "CLT in action\n\n\nCode\n# Create a skewed population\nset.seed(456)\nskewed_pop &lt;- exp(rnorm(10000, mean = 0, sd = 0.5))\n\n# Sample means for different sample sizes (ordered small to large)\nsample_sizes &lt;- c(5, 30, 100)\nsample_labels &lt;- factor(paste(\"n =\", sample_sizes),\n  levels = paste(\"n =\", sample_sizes)\n) # preserve order\nsample_dist_data &lt;- lapply(sample_sizes, function(n) {\n  means &lt;- replicate(1000, mean(sample(skewed_pop, size = n)))\n  data.frame(means = means, size = factor(paste(\"n =\", n), levels = levels(sample_labels)))\n})\nsample_dist_df &lt;- do.call(rbind, sample_dist_data)\n\n# Plot\nggplot() +\n  geom_histogram(aes(x = means, y = ..density..),\n    data = sample_dist_df,\n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  geom_density(aes(x = means), data = sample_dist_df, color = \"blue\") +\n  facet_wrap(~size, scales = \"free_x\") +\n  ggtitle(\"Sampling distributions for different sample sizes\")"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#example",
    "href": "lectures/L01/Lecture-01b.html#example",
    "title": "Lecture 01b – Revision",
    "section": "Example",
    "text": "Example\n\n\nCode\nset.seed(239)\n# Generate a skewed distribution\nskewed &lt;- tibble(\n  x = rgamma(1000, shape = 2, scale = 1)\n)\n\n# plot in ggplot2\nggplot(data = skewed, aes(x = x)) +\n  geom_histogram(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlab(\"Height (m)\")\n\n\n\n\nSkewed population distribution for tree heights.\nWe want to estimate the mean height of the trees in the forest."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sample-no-summary-statistic",
    "href": "lectures/L01/Lecture-01b.html#sample-no-summary-statistic",
    "title": "Lecture 01b – Revision",
    "section": "1 sample (no summary statistic)",
    "text": "1 sample (no summary statistic)\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 1,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\nWith only one sample, we are not really seeing a sampling distribution – we are just replicating the same population distribution. A sampling distribution emerges when we take multiple samples and calculate their means."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-1",
    "href": "lectures/L01/Lecture-01b.html#samples-1",
    "title": "Lecture 01b – Revision",
    "section": "2 samples",
    "text": "2 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 2,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nWe sample 2 trees and calculate the mean height, and repeat this 1000 times.\nThe distribution of sample means is starting to look more like a normal distribution."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-2",
    "href": "lectures/L01/Lecture-01b.html#samples-2",
    "title": "Lecture 01b – Revision",
    "section": "5 samples",
    "text": "5 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 5,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nFive random samples per calculated mean, repeated 1000 times.\nThe distribution is becoming more normal, and the spread is decreasing: estimate is getting more precise."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-3",
    "href": "lectures/L01/Lecture-01b.html#samples-3",
    "title": "Lecture 01b – Revision",
    "section": "30 samples",
    "text": "30 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 30,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nThirty random samples per calculated mean, repeated 1000 times.\nThe distribution of sample means is very close to a normal distribution."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-4",
    "href": "lectures/L01/Lecture-01b.html#samples-4",
    "title": "Lecture 01b – Revision",
    "section": "50 samples",
    "text": "50 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nFifty random samples per calculated mean, repeated 1000 times.\nHow many samples is enough?"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#are-50-samples-normal-enough",
    "href": "lectures/L01/Lecture-01b.html#are-50-samples-normal-enough",
    "title": "Lecture 01b – Revision",
    "section": "Are 50 samples “normal” enough?",
    "text": "Are 50 samples “normal” enough?\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = 2, # population mean for gamma(2,1)\n      sd = sqrt(2) / sqrt(50) # theoretical SE for gamma(2,1)\n    ),\n    linewidth = 1,\n    color = \"blue\",\n    linetype = \"dashed\"\n  ) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nFifty random samples per calculated mean, repeated 1000 times.\nHow many samples is enough?"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#effect-of-sample-size",
    "href": "lectures/L01/Lecture-01b.html#effect-of-sample-size",
    "title": "Lecture 01b – Revision",
    "section": "Effect of sample size",
    "text": "Effect of sample size\n\n\nCode\nlibrary(tidymodels)\nlibrary(patchwork)\nset.seed(642)\n\nheights &lt;- tibble(heights = rnorm(1000, 1.99, 1))\npopmean &lt;- mean(heights$heights)\nsample_sizes &lt;- c(2, 5, 25, 100)\nn &lt;- length(sample_sizes)\n\nheights &lt;- tibble(heights = rgamma(1000, shape = 2, scale = 1))\nsample_sizes &lt;- c(2, 5, 25, 100)\nn &lt;- length(sample_sizes)\n\nplots &lt;- lapply(sample_sizes, function(size) {\n  df &lt;- heights |&gt;\n    rep_sample_n(size = size, reps = 2000) |&gt;\n    group_by(replicate) |&gt;\n    summarise(xbar = mean(heights))\n\n  mean_xbar &lt;- mean(df$xbar)\n\n  ggplot(df, aes(x = xbar)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    geom_vline(aes(xintercept = mean_xbar), color = \"blue\", linetype = \"dashed\") +\n    geom_text(aes(x = mean_xbar, label = sprintf(\"%.2f\", mean_xbar), y = Inf), hjust = -0.1, vjust = 2, color = \"blue\") +\n    ggtitle(paste0(\"Sample Size: \", size)) +\n    xlab(\"Mean height (m)\") +\n    xlim(-3, 8)\n})\nwrap_plots(plots)\n\n\n\nIncreased sample size leads to a more accurate estimate of the population mean, reflected by the narrower distribution of the sample mean, which is captured by the standard error."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#effect-of-variability",
    "href": "lectures/L01/Lecture-01b.html#effect-of-variability",
    "title": "Lecture 01b – Revision",
    "section": "Effect of variability",
    "text": "Effect of variability\n\n\nCode\nset.seed(1221)\n\n# Define a function to generate ggplot objects\ngenerate_plot &lt;- function(sd) {\n  data &lt;- rnorm(500, 1.99, sd)\n  p &lt;- ggplot(data = tibble(x = data), aes(x = x)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    ggtitle(paste(\"SD =\", sd)) +\n    xlim(-100, 100)\n  return(p)\n}\n\n# Apply the function to a list of standard deviations\nsds &lt;- c(3, 6, 15, 25)\nplots &lt;- lapply(sds, generate_plot)\n\n# Wrap the plots\nwrap_plots(plots)\n\n\n\nIncreased variability (i.e. wide range of tree heights) leads to a wider distribution of the sample mean (i.e. less precision), which is also reflected by the standard error."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#clt-drives-statistical-inference",
    "href": "lectures/L01/Lecture-01b.html#clt-drives-statistical-inference",
    "title": "Lecture 01b – Revision",
    "section": "CLT drives statistical inference",
    "text": "CLT drives statistical inference\nBecause of how predictable the CLT applies to sample means, we can use this to make reasonably accurate inferences about the population mean, even if we do not know the population distribution.\n\nA sampling distribution of the mean will be normally distributed for sufficiently large samples – how large is “sufficient” depends on the population distribution\nThe mean of the sampling distribution trends towards the population mean with increasing sample size\nTo determine how well the sample mean estimates the population mean, we use the standard error of the mean – basically a standard deviation of the sampling distribution"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#standard-error-of-the-mean",
    "href": "lectures/L01/Lecture-01b.html#standard-error-of-the-mean",
    "title": "Lecture 01b – Revision",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\n\nMeasures the precision of a sample mean\nDescribes variation in sample means – around the true population mean\nDecreases as sample size increases, because we become more “confident” in our estimate\n\n\n\n\n\n\n\n\nFormula\n\n\nSE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n\nwhere s is the sample standard deviation\nn is the sample size"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#when-to-report-sd-or-se",
    "href": "lectures/L01/Lecture-01b.html#when-to-report-sd-or-se",
    "title": "Lecture 01b – Revision",
    "section": "When to report SD or SE",
    "text": "When to report SD or SE\nStandard Deviation (SD)\n\nDescribes variability in your data\nStays constant regardless of sample size\n\nStandard Error (SE)\n\nDescribes precision of your mean estimate\nDecreases with larger sample size (SE = \\frac{SD}{\\sqrt{n}})\n\nWhen reporting statistics:\n\nUse mean ± SE to show precision of your estimate\nUse mean ± SD to show spread of your raw data\nSE can appear deceptively small with large sample sizes – always report sample size!"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#confidence-intervals",
    "href": "lectures/L01/Lecture-01b.html#confidence-intervals",
    "title": "Lecture 01b – Revision",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWhat is a confidence interval?\n\nRange of values likely to contain the true population parameter\nLevel of confidence (usually 95%) indicates reliability\nWider intervals = less precise estimates\n\n\n\n\n\n\n\n\nFormula for 95% CI\n\n\n\\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#visualising-confidence-intervals",
    "href": "lectures/L01/Lecture-01b.html#visualising-confidence-intervals",
    "title": "Lecture 01b – Revision",
    "section": "Visualising confidence intervals",
    "text": "Visualising confidence intervals\n\n\nCode\n# Generate sample data\nset.seed(253)\nsample_data &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  value = c(\n    rnorm(30, 100, 15),\n    rnorm(30, 110, 15),\n    rnorm(30, 105, 15)\n  )\n)\n\n# Calculate means and CIs\nci_data &lt;- sample_data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    mean = mean(value),\n    se = sd(value) / sqrt(n()),\n    ci_lower = mean - qt(0.975, n() - 1) * se,\n    ci_upper = mean + qt(0.975, n() - 1) * se\n  )\n\n# Plot\nggplot(ci_data, aes(x = group, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  ggtitle(\"Means with 95% Confidence Intervals\")\n\n\n\nWe will learn more about confidence intervals in the next lecture."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX2001 – Applied Statistical Methods",
    "section": "",
    "text": "This site contains ONLY SOME of the lecture material for ENVX2001. It is meant for lecturers to publish their lecture content in a structured way and has not been designed for student use. Most of the content is still in development and will be updated throughout the semester. If you happen to stumble upon this site, feel free to have a look around… but be aware that the content is not final and may contain errors.\nENVX2001 students should always navigate to Canvas to access the ENVX2001 page and view the course material in the right context. If you are looking for a specific lecture, please refer to the Canvas page for the course.\nModule 1: Designed studies\n\nLecture 01 – Introduction\nLecture 02 – Sampling designs\nLecture 03 – 1-way ANOVA\nLecture 04 – Residual diagnostics & post hoc tests\nLecture 05 – Experimental design\nLecture 06 – ANOVA with blocking\n\nModule 2: Finding patterns in data\n\nLecture 07 – Regression modelling\nLecture 08 – Regression model development\nLecture 09 – Regression model assessment\nLecture 10 – Principle component analysis\nLecture 11 – Clustering\nLecture 12 – Multidimensional scaling",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Home**"
    ]
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#staff",
    "href": "lectures/L01/Lecture-01a.html#staff",
    "title": "Lecture 01a – Welcome",
    "section": "Staff",
    "text": "Staff\n\n\n\n\n\n\n\n\n\nA. Prof Aaron Greenville\n\n\n\n\n\n\n\nDr Si Yang Han\n\n\n\n\n\n\n\nDr Januar Harianto\n\n\n\n\n\n\n\nProf Mathew Crowther"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#structure",
    "href": "lectures/L01/Lecture-01a.html#structure",
    "title": "Lecture 01a – Welcome",
    "section": "Structure",
    "text": "Structure\nThis unit includes lectures, self-guided tutorials, labs, discussions, and drop-in sessions.\n\nLectures: Tuesdays 10 AM, Wednesdays 11 AM, Chemistry Lecture Theatre 3\nTutorials: Self-guided (1 hour), complete before each week’s lab.\nLabs: South Eveleigh Precinct, Thursday 9 am – 12 pm, Friday 10 am – 1 pm, 2 pm – 5 pm\nDiscussion: Via Ed discussion, we usually respond the same day unless it is the weekend.\nDrop-in sessions: Scheduled as necessary (Zoom or in person). Email us to arrange a session."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#attendance",
    "href": "lectures/L01/Lecture-01a.html#attendance",
    "title": "Lecture 01a – Welcome",
    "section": "Attendance",
    "text": "Attendance\n\nLectures: Highly recommended but not compulsory. Lectures are recorded, capturing slides and audio only, which may miss important discussions.\nLabs: Mandatory, 80% minimum attendance required. Attendance will be taken by QR code. If you miss a lab, you may attend another session that week – send us an email!\nTutorials: Self-guided (1 hour), complete before each week’s lab."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#assessments",
    "href": "lectures/L01/Lecture-01a.html#assessments",
    "title": "Lecture 01a – Welcome",
    "section": "Assessments",
    "text": "Assessments\nCheck Unit Outline\n\n\n\n\n\n\n\n\n\nWeek\nAssessment\nWeight\nType\n\n\n\n\n4\nEarly Feedback Task\n1%\nIndividual\n\n\n5\nProject 1: Describing data\n10%\nIndividual\n\n\n10\nProject 2: Analysing experimental data\n20%\nIndividual\n\n\n13\nProject 3: Presentation (multivariate)\n20%\nGroup\n\n\n-\nQuizzes (weekly, multiple due dates)\n4%\nIndividual\n\n\n-\nExam (2 hours, MCQs + Short Answers)\n45%\nIndividual"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#biomedical-building",
    "href": "lectures/L01/Lecture-01a.html#biomedical-building",
    "title": "Lecture 01a – Welcome",
    "section": "Biomedical Building",
    "text": "Biomedical Building\n\nCredit: Michael Wheatland"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#directions",
    "href": "lectures/L01/Lecture-01a.html#directions",
    "title": "Lecture 01a – Welcome",
    "section": "Directions",
    "text": "Directions\nBuses\nCourtesy buses are available:\n\nThe best option is to take the bus from Fisher Library to Redfern Station, then walk to the precinct (through the new station platform as “local traffic”).\nAlternatively, direct buses are available – but less frequent.\n\nDriving\nFree parking is available around Henderson Road, but it is extremely crowded. We do not recommend driving to the precinct."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#walking",
    "href": "lectures/L01/Lecture-01a.html#walking",
    "title": "Lecture 01a – Welcome",
    "section": "Walking",
    "text": "Walking\nWalking to the South Eveleigh Precinct takes about 20 minutes. However, you can save approximately 5 minutes by using Redfern station’s community access gates, where you don’t need to use an Opal card to get through.\nIf the map does not load, click here"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#statistics-in-action",
    "href": "lectures/L01/Lecture-01a.html#statistics-in-action",
    "title": "Lecture 01a – Welcome",
    "section": "Statistics in Action",
    "text": "Statistics in Action\nModern science and decision-making are driven by data:\n\n\nResearch: From lab experiments to field studies\nPolicy: Environmental management decisions\nIndustry: Business analytics and optimisation\nInnovation: AI and machine learning – foundations"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#why-statistics-matters",
    "href": "lectures/L01/Lecture-01a.html#why-statistics-matters",
    "title": "Lecture 01a – Welcome",
    "section": "Why Statistics Matters?",
    "text": "Why Statistics Matters?\nStatistics empowers you to:\n\n\nTurn raw data into meaningful insights\nMake evidence-based decisions\nCommunicate findings effectively\nSolve complex real-world problems\n\n\n\n\n\n\nStatistics helps avoid misinterpreting data. Source: Anchorman (2004)"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#real-world-applications",
    "href": "lectures/L01/Lecture-01a.html#real-world-applications",
    "title": "Lecture 01a – Welcome",
    "section": "Real-world Applications",
    "text": "Real-world Applications\n\nSource: NASA’s Global Temperature Index"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#sports-analytics",
    "href": "lectures/L01/Lecture-01a.html#sports-analytics",
    "title": "Lecture 01a – Welcome",
    "section": "Sports Analytics",
    "text": "Sports Analytics\n\nThe 10 highest-seeded players averaged 3.48 rounds won in the Australian Open since 2011, compared to just 3.03 at Wimbledon. Source: fivethirtyeight"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#your-path-ahead",
    "href": "lectures/L01/Lecture-01a.html#your-path-ahead",
    "title": "Lecture 01a – Welcome",
    "section": "Your Path Ahead",
    "text": "Your Path Ahead\nThis course will develop your:\n\nTechnical Skills\n\nR programming proficiency\nData visualization expertise\nStatistical analysis methods\n\n\n\nProfessional Skills\n\nCritical thinking\nScientific communication\nProblem-solving abilities"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#lecture-attendance-options",
    "href": "lectures/L01/Lecture-01a.html#lecture-attendance-options",
    "title": "Lecture 01a – Welcome",
    "section": "Lecture attendance options",
    "text": "Lecture attendance options\nIn-person vs. online recordings\n\nIn-person benefits:\n\nReal-time interaction with peers and lecturers\nImmediate feedback and clarification of concepts\nActive participation in discussions and polls\nBuilding connections with classmates\n\nOnline recording benefits:\n\nFlexibility to manage other commitments\nAbility to pause and review complex concepts\nLearn at your own pace\nConvenient for those with long commutes"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#on-campus-or-online",
    "href": "lectures/L01/Lecture-01a.html#on-campus-or-online",
    "title": "Lecture 01a – Welcome",
    "section": "On-campus or online?",
    "text": "On-campus or online?\nChoose the option that best suits your learning style and circumstances. If watching online, try to:\n\nStay up to date with recordings to avoid falling behind\nUse Ed discussion board actively for questions\nAttend some lectures in person when possible for key topics\n\nThere is a strong positive correlation between lecture attendance and final grades – but it’s not the only factor. It may just be the case that students who attend lectures are more likely to keep up with the course material."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#put-in-the-hours",
    "href": "lectures/L01/Lecture-01a.html#put-in-the-hours",
    "title": "Lecture 01a – Welcome",
    "section": "Put in the hours",
    "text": "Put in the hours\n\nThis is a 6 credit point unit, which means that you are expected to spend 120 – 150 hours in total, including exam prep time (~10 h per week)!\nPractice makes perfect. Tutorials and Labs help you apply the concepts you learn in lectures – complete all the exercises, and practice with the bonus questions provided."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#ask-questions",
    "href": "lectures/L01/Lecture-01a.html#ask-questions",
    "title": "Lecture 01a – Welcome",
    "section": "Ask questions",
    "text": "Ask questions\n\nEd is the best place to ask questions. We are way more responsive on Ed than on email.\nWe are open to the use of AI tools (including LLMs like ChatGPT) to help you answer questions about code… but don’t use them to cheat yourself out of learning.\nWe have drop-in sessions, where you can jump in and have a chat on Zoom. We will announce the schedule on Ed."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#by-the-end-of-this-course-we-want-you-to-be-able-to",
    "href": "lectures/L01/Lecture-01a.html#by-the-end-of-this-course-we-want-you-to-be-able-to",
    "title": "Lecture 01a – Welcome",
    "section": "By the end of this course, we want you to be able to:",
    "text": "By the end of this course, we want you to be able to:\n\nLO1 demonstrate proficiency in designing sample schemes and analysing data from them using R.\nLO2 describe and identify the basic features of an experimental design: replicate, treatment structure and blocking structure.\nLO3 demonstrate proficiency in the use or the statistical programming language R to apply an ANOVA and fit regression models to experimental data.\nLO4 demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data.\nLO5 interpret the output and understand conceptually how its derived of a regression, ANOVA and multivariate analysis that have been calculated by R.\nLO6 write statistical and modelling results as part of a scientific report.\nLO7 appraise the validity of statistical analyses used publications."
  },
  {
    "objectID": "lectures/L01/index.html",
    "href": "lectures/L01/index.html",
    "title": "Lecture 01",
    "section": "",
    "text": "Lecture 01a – Welcome Full Screen | PDF\n\nLecture 01b – The beginning is the end: a revision Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1: Designed studies**",
      "L01 -- Introduction"
    ]
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling",
    "text": "Simple random sampling\n\n\nEach unit has an equal chance of being selected.\n\n\nNot always the case, but still a good technique."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-1",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-1",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling",
    "text": "Simple random sampling\nEach unit has an equal chance of being selected.\nNot always the case, but still a good technique."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-potential-problems",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-potential-problems",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling: potential problems",
    "text": "Simple random sampling: potential problems\n\nImagine tossing 10 random points onto a landscape.\n\n\nBy pure chance…\n\nWe might miss some important areas entirely\nOr sample some areas too much\n\n\n\nThis is more likely when:\n\nSample size is small\nThe landscape has distinct zones"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-theoretical-example",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-theoretical-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling: theoretical example",
    "text": "Simple random sampling: theoretical example\n\nIf an area has:\n\n80% grassland\n20% wetland\n\n\n\nWith simple random sampling:\n\nWe expect ~8 samples in grassland, ~2 in wetland\nBut by chance, we might get:\n\n10 grassland, 0 wetland!\nOr 6 grassland, 4 wetland\n\n\n\n\nBut what if we have more information about the population?"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#soil-carbon-example",
    "href": "lectures/L02/Lecture-02b.html#soil-carbon-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Soil carbon example",
    "text": "Soil carbon example\n\nSoil carbon\n\n\n\nDifferent land types\n\nLand type A covers 62% of the area, land type B covers 38%\nType A has a higher chance of being selected with simple random sampling\nCan we use this information to our advantage?"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Stratified random sampling",
    "text": "Stratified random sampling\n3 steps\n\n\nDivide the population into homogeneous subgroups (strata).\nSample from each stratum using simple random sampling.\nPool (or combine) the estimates from each stratum to get an overall population estimate.\n\n\n\nReal-world example\nIf studying plant biodiversity in a national park:\n\nStep 1: Divide park into strata (e.g., forest, grassland, wetland)\nStep 2: Take random samples within each habitat type\nStep 3: Combine data to estimate overall biodiversity, giving proper weight to each habitat’s area"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#strata-rules",
    "href": "lectures/L02/Lecture-02b.html#strata-rules",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Strata rules",
    "text": "Strata rules\nStrata are…\n\n\nMutually exclusive and collectively exhaustive (simple explanation: every sample belongs to exactly one stratum – no overlaps, no leftovers)\nHomogeneous - Samples within a stratum should be similar to each other (less variable than the overall population)\nEach stratum must be sampled - The goal is to ensure every important group is represented"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#good-vs.-poor-stratification-choices",
    "href": "lectures/L02/Lecture-02b.html#good-vs.-poor-stratification-choices",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Good vs. poor stratification choices",
    "text": "Good vs. poor stratification choices\n\nEveryday examples\n\n\nGood strata\n\nUniversity students: Undergrad, Masters, PhD\nForest types: Deciduous, Coniferous, Mixed\nIncome levels: Low, Medium, High\n\n\nPoor strata choices\n\nInterests: Sports fans, Music lovers, Foodies (a person can be in multiple groups)\nWater quality: Clean, Somewhat polluted (too subjective, not clearly defined)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#advantages",
    "href": "lectures/L02/Lecture-02b.html#advantages",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Advantages",
    "text": "Advantages\nWe address:\n\nBias. Each stratum is sampled, so the sample is representative of the population.\nAccuracy. Each stratum is represented by a minimum number of sampling units.\nInsight. We can compare strata and make inferences about the population.\n\n\nDoes this make simple random sampling obsolete?\n\nNo. Still a good technique.\nWith large enough samples, the two methods will converge.\nChance of not selecting a unit from a stratum is always there, but reduces as the sample size increases."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#what-are-we-trying-to-achieve-with-our-calculations",
    "href": "lectures/L02/Lecture-02b.html#what-are-we-trying-to-achieve-with-our-calculations",
    "title": "Lecture 02b – Sampling designs II",
    "section": "What are we trying to achieve with our calculations?",
    "text": "What are we trying to achieve with our calculations?\nThe statistical journey\n\nOnce we have our stratified sample, we need to:\n\nEstimate the population central tendency: Calculate the pooled mean\nQuantify our uncertainty: Calculate the pooled standard error\nCreate an inference tool: Build a confidence interval\nMake decisions: Compare estimates, test hypotheses\n\nAll of these steps must account for our stratified design."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#the-statistical-workflow-for-stratified-sampling",
    "href": "lectures/L02/Lecture-02b.html#the-statistical-workflow-for-stratified-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "The statistical workflow for stratified sampling",
    "text": "The statistical workflow for stratified sampling\nFour key steps:\n\nPooled Mean (\\bar{y}_{s}): Sum of (stratum weight × stratum mean)\n\nBest estimate of the population parameter\n\nPooled Standard Error: SE(\\bar{y}_{s}) = \\sqrt{\\sum w_i^2 \\times \\frac{s_i^2}{n_i}}\n\nAccounts for stratum weights and within-stratum variability\n\nt-Critical Value: Based on df = n - L and α = 0.05\n\nAccounts for sample size in uncertainty estimates\n\nConfidence Interval: \\text{Pooled mean} \\pm (t-\\text{critical} \\times SE(\\bar{y}_{s}))\n\nRange likely containing true population mean"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#accounting-for-strata-using-weight",
    "href": "lectures/L02/Lecture-02b.html#accounting-for-strata-using-weight",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Accounting for strata using “weight”",
    "text": "Accounting for strata using “weight”\nWeighted estimates\n\nWe need to “weigh” the estimates from each stratum to account for the different stratum sizes and inclusion probabilities.\nMost of the time, we use the stratum size as the weight to calculate weighted estimates.\nThe overall population estimate is the sum of the weighted estimates from each stratum, i.e. we pool the individual strata information into a single, overall population estimate.\n\n\nExample\n\n\nA forest contains two types of trees: A and B, with 60% and 40% of the population, respectively.\nWe want to estimate the mean height of the trees.\nTake 10 height measurements, of which 7 are randomly selected from type A and 3 are randomly selected from type B.\nThe pooled estimate for the mean height of the trees is: 0.6 \\times \\text{average height of A} + 0.4 \\times \\text{average height of B}"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#soil-carbon-data",
    "href": "lectures/L02/Lecture-02b.html#soil-carbon-data",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Soil carbon data",
    "text": "Soil carbon data\nOur case study\nSoil carbon content was measured at 7 locations across the area. The amounts were: 48, 56, 90, 78, 86, 71, 42 tonnes per hectare (t/ha).\n\n\nSetting up the data in R\nWe know which land type each sample came from:\n\nlandA &lt;- c(90, 78, 86, 71)  # stratum A samples (62% of the area)\nlandB &lt;- c(48, 56, 42)      # stratum B samples (38% of the area)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#pooled-mean-bar-y_s",
    "href": "lectures/L02/Lecture-02b.html#pooled-mean-bar-y_s",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Pooled mean \\bar y_{s}",
    "text": "Pooled mean \\bar y_{s}\n\nThe pooled mean is our best estimate of the overall population mean, taking into account the different stratum sizes.\n\n\n\\bar{y}_{s} = \\sum_{i=1}^L \\bar{y}_i \\times w_i\nIn simple terms:\n\nWe calculate the mean for each stratum separately (\\bar{y}_i)\nWe multiply each stratum’s mean by its weight (w_i)\nWe add these weighted means together to get the overall pooled mean"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#calculating-pooled-mean-soil-carbon-example",
    "href": "lectures/L02/Lecture-02b.html#calculating-pooled-mean-soil-carbon-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Calculating pooled mean: soil carbon example",
    "text": "Calculating pooled mean: soil carbon example\n\nWe first define the weights w_i for each stratum based on their area:\n\n\nCode\nweight &lt;- c(0.62, 0.38)  # 62% of area is land type A, 38% is land type B\n\n\n\n\nThen we calculate the weighted mean:\n\n\nCode\nweighted_mean &lt;- mean(landA) * weight[1] + mean(landB) * weight[2]\nweighted_mean\n\n\n[1] 68.86833\n\n\nThis is like saying: “62% of our land has soil carbon like land type A, and 38% has soil carbon like land type B, so our overall estimate takes both into account in these proportions.”"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#pooled-standard-error-of-the-mean-sebar-y_s",
    "href": "lectures/L02/Lecture-02b.html#pooled-standard-error-of-the-mean-sebar-y_s",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Pooled standard error of the mean SE(\\bar y_{s})",
    "text": "Pooled standard error of the mean SE(\\bar y_{s})\nThe formula looks similar to a standard error…\nSE(\\bar y_{s}) = \\sqrt{\\color{blue}{{\\sum_{i=1}^L w_i^2}} \\times \\frac{s_i^2}{n_i}}\n\n\n\n\n\n\nWhat’s different?\n\n\n\nInstead of a single variance term, we use the sum of weighted variances from each stratum\nThe \\color{blue}{w_i^2} term ensures we account for the relative size of each stratum\nEach stratum contributes its own variance (s_i^2) and sample size (n_i)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#t-critical-value",
    "href": "lectures/L02/Lecture-02b.html#t-critical-value",
    "title": "Lecture 02b – Sampling designs II",
    "section": "t-critical value",
    "text": "t-critical value\nDegrees of freedom df\ndf = n - L\nwhere n is the total number of samples and L is the number of strata.\n\n\nThe degrees of freedom tells us how much “free information” we have for making estimates\nFor stratified sampling, we lose one degree of freedom for each stratum\nExample: If we have 12 samples in 3 strata:\n\nThe degrees of freedom is 12 - 3 = 9\nThink of it this way: 9 samples can be placed anywhere, but we must have at least 1 sample in each of the 3 strata\n\n\n\n\nIn R\n\n\nCode\ndf &lt;- length(landA) + length(landB) - 2\nt_crit &lt;- qt(0.975, df)\nt_crit\n\n\n[1] 2.570582"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "95 % Confidence interval for stratified random sampling",
    "text": "95 % Confidence interval for stratified random sampling\nThe formula\n95\\%\\ CI = \\bar y_{s} \\pm t^{0.025}_{n-L} \\times SE(\\bar y_{s})\nwhere L is the number of strata, n is the total number of samples, and \\bar y_{s} is the weighted mean of the strata.\nIn simple terms:\n\n\nWe’re creating a range where we’re 95% confident the true population mean lies\nWe start with our best estimate (the pooled mean \\bar y_{s})\nWe add and subtract a “margin of error” (which depends on our sample size and variability)\nThe margin of error = t-critical value × standard error\n\n\nVisualising this:\nLower bound ← [Pooled mean - Margin of error] ... [Pooled mean + Margin of error] → Upper bound"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling-1",
    "href": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling-1",
    "title": "Lecture 02b – Sampling designs II",
    "section": "95 % Confidence interval for stratified random sampling",
    "text": "95 % Confidence interval for stratified random sampling\nPutting it all together\n\n\nCode\nvarA &lt;- var(landA) / length(landA)  # variance of the mean for A\nvarB &lt;- var(landB) / length(landB)  # variance of the mean for B\nweighted_var &lt;- weight[1]^2 * varA + weight[2]^2 * varB\nweighted_se &lt;- sqrt(weighted_var)\nci &lt;- c(\n  L95 = weighted_mean - t_crit * weighted_se,\n  u95 = weighted_mean + t_crit * weighted_se\n)\nci\n\n\n     L95      u95 \n61.04864 76.68803"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-vs.-stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#simple-random-vs.-stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random vs. stratified random sampling",
    "text": "Simple random vs. stratified random sampling\nWhat if we had used stratified random sampling instead of simple random sampling (and collected the same amount of data)?\nWhat differences can you see?\n\n\nCode\nlibrary(tidyverse)\n# Manually printing the results below as SRS data is in previous lecture\ncompare &lt;- tibble(\n  Design = c(\"Simple Random\", \"Stratified Random\"),\n  Mean = c(67.29, 68.9), \n  `Var (mean)` = c(50.83, 9.30),\n  L95 = c(49.85, 61), \n  U95 = c(84.73, 76.7), \n  df = c(6, 5))\nknitr::kable(compare)\n\n\n\n\n\nDesign\nMean\nVar (mean)\nL95\nU95\ndf\n\n\n\n\nSimple Random\n67.29\n50.83\n49.85\n84.73\n6\n\n\nStratified Random\n68.90\n9.30\n61.00\n76.70\n5"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#visual-comparison-of-95-confidence-intervals",
    "href": "lectures/L02/Lecture-02b.html#visual-comparison-of-95-confidence-intervals",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Visual comparison of 95% confidence intervals",
    "text": "Visual comparison of 95% confidence intervals\n\n\nCode\n# Creating a visual comparison of confidence intervals\nggplot(compare, aes(x = Design, y = Mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = L95, ymax = U95), width = 0.2, size = 1) +\n  labs(title = \"95% Confidence Intervals by Sampling Design\",\n       y = \"Soil Carbon (tonnes/ha)\",\n       x = \"\") +\n  theme_minimal(base_size = 14) +\n  annotate(\"text\", x = 2, y = 55, \n           label = \"Stratified sampling gives a\\nnarrower confidence interval\\n(more precise estimate)\", \n           color = \"blue\")\n\n\n\n\nKey insights:\n\nBoth methods give similar estimates of the mean\nStratified sampling produces a much narrower confidence interval\nThe variance of the mean is about 5 times smaller with stratified sampling\nThis means stratified sampling is much more precise with the same number of samples"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#efficiency",
    "href": "lectures/L02/Lecture-02b.html#efficiency",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Efficiency",
    "text": "Efficiency\nWhat is sampling efficiency?\n\nA measure of how much “bang for your buck” you get with different sampling methods\nCalculated as a ratio: \\text{Efficiency} = \\frac{\\text{Variance of SRS}}{\\text{Variance of Stratified}}\n\n\nIn simple terms:\n\nEfficiency &gt; 1: Stratified sampling is better (more precise with same sample size)\nEfficiency = 5 means: You’d need 5 times as many samples with simple random sampling to get the same precision as stratified sampling\n\n\n\nIn R\n\n\nCode\nefficiency &lt;- 50.83 / 9.30\nefficiency\n\n\n[1] 5.465591\n\n\nHow many samples would we have had to collect using simple random sampling to achieve the same precision as our stratified sample?\n\n\nCode\nround(7 * efficiency, 0)\n\n\n[1] 38\n\n\nSo we would need about 38 samples with simple random sampling to get the same precision that we achieved with just 7 samples using stratified sampling!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#tips-on-implementation",
    "href": "lectures/L02/Lecture-02b.html#tips-on-implementation",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Tips on implementation",
    "text": "Tips on implementation\n\nThe most difficult part is to identify the strata and assign the sampling units to the strata\nCommon stratification variables in environmental science:\n\nSpatial: elevation bands, soil types, vegetation zones\nTemporal: seasons, time of day, growth stages\nManagement: treatment types, land-use history\n\nStrata sampling size: allocate samples to strata based on the size of the strata, either proportional to:\n\nthe size of the strata (e.g. 60% of area = 60% of samples)\nthe variance of the strata (more samples where variation is higher)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#the-change-in-mean-delta-bar-y",
    "href": "lectures/L02/Lecture-02b.html#the-change-in-mean-delta-bar-y",
    "title": "Lecture 02b – Sampling designs II",
    "section": "The change in mean \\Delta \\bar y",
    "text": "The change in mean \\Delta \\bar y\nImportant considerations\n\n\nWe want to measure change in soil carbon over time\nKey question: How do we select sites for the second measurement?\n\nReturn to the same sites?\nSelect completely new sites?\n\nThis choice affects our statistical analysis (covariance)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#monitoring-estimates",
    "href": "lectures/L02/Lecture-02b.html#monitoring-estimates",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Monitoring estimates",
    "text": "Monitoring estimates\nChange in mean \\Delta \\bar y\n\nThe difference between the means of the two sets of measurements.\n\n\\Delta \\bar y = \\bar y_2 - \\bar y_1\nwhere \\bar y_2 and \\bar y_1 are the means of the second and first set of measurements, respectively."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#uncertainty-in-change-estimates",
    "href": "lectures/L02/Lecture-02b.html#uncertainty-in-change-estimates",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Uncertainty in change estimates",
    "text": "Uncertainty in change estimates\n\nVariance of the change in mean Var(\\Delta{\\bar y})\nThis tells us how precise our estimate of the change is. It depends on:\nVar(\\Delta{\\bar y}) = Var(\\bar y_2) + Var(\\bar y_1) - 2 \\times Cov(\\bar y_2, \\bar y_1)\nIn simple terms:\n\nThe uncertainty in our change estimate comes from the uncertainties in both measurements\nHowever, if we sample the same sites twice, they are related to each other (covariance)\nThis relationship usually reduces the overall uncertainty in our change estimate\n\n\n\nImportant: Visiting the same sites twice (paired sampling) usually gives more precise estimates of change than visiting different sites each time!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#covariance-and-site-selection",
    "href": "lectures/L02/Lecture-02b.html#covariance-and-site-selection",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Covariance and site selection",
    "text": "Covariance and site selection\nQuick decision guide\n\nSame sites? Use paired approach:\n\nSites are the same in both visits\nUse paired t-test\nAccount for covariance between visits\n\nDifferent sites? Use independent approach:\n\nNew random sites in second visit\nUse two-sample t-test\nNo covariance between visits"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#what-is-covariance",
    "href": "lectures/L02/Lecture-02b.html#what-is-covariance",
    "title": "Lecture 02b – Sampling designs II",
    "section": "What is covariance?",
    "text": "What is covariance?\n\nCovariance measures how two measurements relate to each other:\nExample with soil carbon:\n\nSite 1: First visit = 90 t/ha, Second visit = 95 t/ha\nSite 2: First visit = 48 t/ha, Second visit = 52 t/ha\nSite 3: First visit = 71 t/ha, Second visit = 75 t/ha\n\n\n\nWhat do you notice? Sites with high carbon in the first measurement still have high carbon in the second measurement (positive covariance).\nWhy this matters: Knowing the first measurement helps us predict the second one, reducing uncertainty in our estimate of change.\n\n\nPractical takeaway: When measuring change over time, returning to the same sites usually gives more precise results because it removes site-to-site variation."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#calculating-the-95-ci-for-the-change-in-mean",
    "href": "lectures/L02/Lecture-02b.html#calculating-the-95-ci-for-the-change-in-mean",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Calculating the 95% CI for the change in mean",
    "text": "Calculating the 95% CI for the change in mean\nThe formula looks similar to before:\n95\\%\\ CI = \\Delta \\bar y \\pm t^{0.025}_{n-1} \\times SE(\\Delta \\bar y)\n\nIn plain language:\n\nWe have our best estimate of the change (the difference between the two means)\nWe add and subtract a margin of error to create a range\nWe’re 95% confident that the true change falls within this range\n\n\n\nThe standard error of the change SE(\\Delta \\bar y)\n\nThis tells us how precise our estimate of the change is\nIt’s complicated to calculate by hand, especially when we visit the same sites twice\nIf we visit the same sites twice, we need to account for their relationship (covariance)\n\n\n\nGood news! You don’t need to calculate this by hand!\n\nR can do these calculations for you using the t.test() function\nFor same sites: use paired = TRUE option\nFor different sites: use paired = FALSE option\nWe’ll practice this in the lab!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#thanks",
    "href": "lectures/L02/Lecture-02b.html#thanks",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Thanks!",
    "text": "Thanks!\nQuestions?\nThis presentation is based on the SOLES Quarto reveal.js template and is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html",
    "href": "lectures/L03/Lecture-03a.html",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "",
    "text": "William Gosset (1908)\n\n\nFormulated the \\(t\\)-distribution and \\(t\\)-tests.\n\n\n\nRonald FIsher\n\n\nFormulated the \\(t\\)-distribution and \\(t\\)-tests."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#data",
    "href": "lectures/L03/Lecture-03a.html#data",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Data",
    "text": "Data\n\nWeights of two breeds of cattle are to be compared\nTwelve (12) samples were taken randomly from Breed 1 and 15 samples from Breed 2.\nAre there any differences in the mean weights of the two breeds?\n\n\n\n\nCode\ncattle &lt;- read.csv(\"data/cattle.csv\")\ncattle\n\n\n   Breed1 Breed2\n1   187.6  148.1\n2   180.3  146.2\n3   198.6  152.8\n4   190.7  135.3\n5   196.3  151.2\n6   203.8  146.3\n7   190.2  163.5\n8   201.0  146.6\n9   194.7  162.4\n10  221.1  140.2\n11  186.7  159.4\n12  203.1  181.8\n13     NA  165.1\n14     NA  165.0\n15     NA  141.6"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#thanks",
    "href": "lectures/L03/Lecture-03a.html#thanks",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Thanks!",
    "text": "Thanks!\n\nQuestions?\nThis presentation is based on the SOLES Quarto reveal.js template and is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "lectures/L04/index.html",
    "href": "lectures/L04/index.html",
    "title": "Lecture 04",
    "section": "",
    "text": "Important\n\n\n\nLecture 04 is not available in Quarto. Please refer to the Canvas site to access the lecture material."
  },
  {
    "objectID": "lectures/L06/index.html",
    "href": "lectures/L06/index.html",
    "title": "Lecture 06",
    "section": "",
    "text": "Important\n\n\n\nLecture 06 is not available in Quarto. Please refer to the Canvas site to access the lecture material."
  },
  {
    "objectID": "lectures/L07/index.html",
    "href": "lectures/L07/index.html",
    "title": "Lecture 07",
    "section": "",
    "text": "Lecture 07 – Regression modelling: Full screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2: Finding patterns**",
      "L07 -- Regression modelling"
    ]
  },
  {
    "objectID": "lectures/L08/index.html",
    "href": "lectures/L08/index.html",
    "title": "Lecture 08",
    "section": "",
    "text": "Lecture 08 – Regression model development Full screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2: Finding patterns**",
      "L08 -- Model development"
    ]
  },
  {
    "objectID": "lectures/L09/index.html",
    "href": "lectures/L09/index.html",
    "title": "Lecture 09",
    "section": "",
    "text": "Lecture 09 – Predictive modelling Full screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2: Finding patterns**",
      "L09 -- Model assessment"
    ]
  }
]