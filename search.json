[
  {
    "objectID": "lectures/L08/index.html",
    "href": "lectures/L08/index.html",
    "title": "Lecture 08",
    "section": "",
    "text": "Lecture 08 – Regression model development Full screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2: Finding patterns**",
      "L08 -- Model development"
    ]
  },
  {
    "objectID": "lectures/L06/index.html",
    "href": "lectures/L06/index.html",
    "title": "Lecture 06",
    "section": "",
    "text": "Important\n\n\n\nLecture 06 is not available in Quarto. Please refer to the Canvas site to access the lecture material."
  },
  {
    "objectID": "lectures/L04/index.html",
    "href": "lectures/L04/index.html",
    "title": "Lecture 04",
    "section": "",
    "text": "Important\n\n\n\nLecture 04 is not available in Quarto. Please refer to the Canvas site to access the lecture material."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html",
    "href": "lectures/L03/Lecture-03a.html",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "",
    "text": "William Gosset (1908)\n\n\nFormulated the \\(t\\)-distribution and \\(t\\)-tests.\n\n\n\nRonald FIsher\n\n\nFormulated the \\(t\\)-distribution and \\(t\\)-tests."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#data",
    "href": "lectures/L03/Lecture-03a.html#data",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Data",
    "text": "Data\n\nWeights of two breeds of cattle are to be compared\nTwelve (12) samples were taken randomly from Breed 1 and 15 samples from Breed 2.\nAre there any differences in the mean weights of the two breeds?\n\n\n\n\nCode\ncattle &lt;- read.csv(\"data/cattle.csv\")\ncattle\n\n\n   Breed1 Breed2\n1   187.6  148.1\n2   180.3  146.2\n3   198.6  152.8\n4   190.7  135.3\n5   196.3  151.2\n6   203.8  146.3\n7   190.2  163.5\n8   201.0  146.6\n9   194.7  162.4\n10  221.1  140.2\n11  186.7  159.4\n12  203.1  181.8\n13     NA  165.1\n14     NA  165.0\n15     NA  141.6"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#thanks",
    "href": "lectures/L03/Lecture-03a.html#thanks",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Thanks!",
    "text": "Thanks!\n\nQuestions?\nThis presentation is based on the SOLES Quarto reveal.js template and is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling",
    "text": "Simple random sampling\n\n\nEach unit has an equal chance of being selected.\n\n\nNot always the case, but still a good technique."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-1",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-1",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling",
    "text": "Simple random sampling\nEach unit has an equal chance of being selected.\nNot always the case, but still a good technique."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-potential-problems",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-potential-problems",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling: potential problems",
    "text": "Simple random sampling: potential problems\n\nImagine tossing 10 random points onto a landscape.\n\n\nBy pure chance…\n\nWe might miss some important areas entirely\nOr sample some areas too much\n\n\n\nThis is more likely when:\n\nSample size is small\nThe landscape has distinct zones"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-theoretical-example",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-theoretical-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling: theoretical example",
    "text": "Simple random sampling: theoretical example\n\nIf an area has:\n\n80% grassland\n20% wetland\n\n\n\nWith simple random sampling:\n\nWe expect ~8 samples in grassland, ~2 in wetland\nBut by chance, we might get:\n\n10 grassland, 0 wetland!\nOr 6 grassland, 4 wetland\n\n\n\n\nBut what if we have more information about the population?"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#soil-carbon-example",
    "href": "lectures/L02/Lecture-02b.html#soil-carbon-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Soil carbon example",
    "text": "Soil carbon example\n\nSoil carbon\n\n\n\nDifferent land types\n\nLand type A covers 62% of the area, land type B covers 38%\nType A has a higher chance of being selected with simple random sampling\nCan we use this information to our advantage?"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Stratified random sampling",
    "text": "Stratified random sampling\n3 steps\n\n\nDivide the population into homogeneous subgroups (strata).\nSample from each stratum using simple random sampling.\nPool (or combine) the estimates from each stratum to get an overall population estimate.\n\n\n\nReal-world example\nIf studying plant biodiversity in a national park:\n\nStep 1: Divide park into strata (e.g., forest, grassland, wetland)\nStep 2: Take random samples within each habitat type\nStep 3: Combine data to estimate overall biodiversity, giving proper weight to each habitat’s area"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#strata-rules",
    "href": "lectures/L02/Lecture-02b.html#strata-rules",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Strata rules",
    "text": "Strata rules\nStrata are…\n\n\nMutually exclusive and collectively exhaustive (simple explanation: every sample belongs to exactly one stratum – no overlaps, no leftovers)\nHomogeneous - Samples within a stratum should be similar to each other (less variable than the overall population)\nEach stratum must be sampled - The goal is to ensure every important group is represented"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#good-vs.-poor-stratification-choices",
    "href": "lectures/L02/Lecture-02b.html#good-vs.-poor-stratification-choices",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Good vs. poor stratification choices",
    "text": "Good vs. poor stratification choices\n\nEveryday examples\n\n\nGood strata\n\nUniversity students: Undergrad, Masters, PhD\nForest types: Deciduous, Coniferous, Mixed\nIncome levels: Low, Medium, High\n\n\nPoor strata choices\n\nInterests: Sports fans, Music lovers, Foodies (a person can be in multiple groups)\nWater quality: Clean, Somewhat polluted (too subjective, not clearly defined)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#advantages",
    "href": "lectures/L02/Lecture-02b.html#advantages",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Advantages",
    "text": "Advantages\nWe address:\n\nBias. Each stratum is sampled, so the sample is representative of the population.\nAccuracy. Each stratum is represented by a minimum number of sampling units.\nInsight. We can compare strata and make inferences about the population.\n\n\nDoes this make simple random sampling obsolete?\n\nNo. Still a good technique.\nWith large enough samples, the two methods will converge.\nChance of not selecting a unit from a stratum is always there, but reduces as the sample size increases."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#what-are-we-trying-to-achieve-with-our-calculations",
    "href": "lectures/L02/Lecture-02b.html#what-are-we-trying-to-achieve-with-our-calculations",
    "title": "Lecture 02b – Sampling designs II",
    "section": "What are we trying to achieve with our calculations?",
    "text": "What are we trying to achieve with our calculations?\nThe statistical journey\n\nOnce we have our stratified sample, we need to:\n\nEstimate the population central tendency: Calculate the pooled mean\nQuantify our uncertainty: Calculate the pooled standard error\nCreate an inference tool: Build a confidence interval\nMake decisions: Compare estimates, test hypotheses\n\nAll of these steps must account for our stratified design."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#the-statistical-workflow-for-stratified-sampling",
    "href": "lectures/L02/Lecture-02b.html#the-statistical-workflow-for-stratified-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "The statistical workflow for stratified sampling",
    "text": "The statistical workflow for stratified sampling\nFour key steps:\n\nPooled Mean (\\bar{y}_{s}): Sum of (stratum weight × stratum mean)\n\nBest estimate of the population parameter\n\nPooled Standard Error: SE(\\bar{y}_{s}) = \\sqrt{\\sum w_i^2 \\times \\frac{s_i^2}{n_i}}\n\nAccounts for stratum weights and within-stratum variability\n\nt-Critical Value: Based on df = n - L and α = 0.05\n\nAccounts for sample size in uncertainty estimates\n\nConfidence Interval: \\text{Pooled mean} \\pm (t-\\text{critical} \\times SE(\\bar{y}_{s}))\n\nRange likely containing true population mean"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#accounting-for-strata-using-weight",
    "href": "lectures/L02/Lecture-02b.html#accounting-for-strata-using-weight",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Accounting for strata using “weight”",
    "text": "Accounting for strata using “weight”\nWeighted estimates\n\nWe need to “weigh” the estimates from each stratum to account for the different stratum sizes and inclusion probabilities.\nMost of the time, we use the stratum size as the weight to calculate weighted estimates.\nThe overall population estimate is the sum of the weighted estimates from each stratum, i.e. we pool the individual strata information into a single, overall population estimate.\n\n\nExample\n\n\nA forest contains two types of trees: A and B, with 60% and 40% of the population, respectively.\nWe want to estimate the mean height of the trees.\nTake 10 height measurements, of which 7 are randomly selected from type A and 3 are randomly selected from type B.\nThe pooled estimate for the mean height of the trees is: 0.6 \\times \\text{average height of A} + 0.4 \\times \\text{average height of B}"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#soil-carbon-data",
    "href": "lectures/L02/Lecture-02b.html#soil-carbon-data",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Soil carbon data",
    "text": "Soil carbon data\nOur case study\nSoil carbon content was measured at 7 locations across the area. The amounts were: 48, 56, 90, 78, 86, 71, 42 tonnes per hectare (t/ha).\n\n\nSetting up the data in R\nWe know which land type each sample came from:\n\nlandA &lt;- c(90, 78, 86, 71)  # stratum A samples (62% of the area)\nlandB &lt;- c(48, 56, 42)      # stratum B samples (38% of the area)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#pooled-mean-bar-y_s",
    "href": "lectures/L02/Lecture-02b.html#pooled-mean-bar-y_s",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Pooled mean \\bar y_{s}",
    "text": "Pooled mean \\bar y_{s}\n\nThe pooled mean is our best estimate of the overall population mean, taking into account the different stratum sizes.\n\n\n\\bar{y}_{s} = \\sum_{i=1}^L \\bar{y}_i \\times w_i\nIn simple terms:\n\nWe calculate the mean for each stratum separately (\\bar{y}_i)\nWe multiply each stratum’s mean by its weight (w_i)\nWe add these weighted means together to get the overall pooled mean"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#calculating-pooled-mean-soil-carbon-example",
    "href": "lectures/L02/Lecture-02b.html#calculating-pooled-mean-soil-carbon-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Calculating pooled mean: soil carbon example",
    "text": "Calculating pooled mean: soil carbon example\n\nWe first define the weights w_i for each stratum based on their area:\n\n\nCode\nweight &lt;- c(0.62, 0.38)  # 62% of area is land type A, 38% is land type B\n\n\n\n\nThen we calculate the weighted mean:\n\n\nCode\nweighted_mean &lt;- mean(landA) * weight[1] + mean(landB) * weight[2]\nweighted_mean\n\n\n[1] 68.86833\n\n\nThis is like saying: “62% of our land has soil carbon like land type A, and 38% has soil carbon like land type B, so our overall estimate takes both into account in these proportions.”"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#pooled-standard-error-of-the-mean-sebar-y_s",
    "href": "lectures/L02/Lecture-02b.html#pooled-standard-error-of-the-mean-sebar-y_s",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Pooled standard error of the mean SE(\\bar y_{s})",
    "text": "Pooled standard error of the mean SE(\\bar y_{s})\nThe formula looks similar to a standard error…\nSE(\\bar y_{s}) = \\sqrt{\\color{blue}{{\\sum_{i=1}^L w_i^2}} \\times \\frac{s_i^2}{n_i}}\n\n\n\n\n\n\nWhat’s different?\n\n\n\nInstead of a single variance term, we use the sum of weighted variances from each stratum\nThe \\color{blue}{w_i^2} term ensures we account for the relative size of each stratum\nEach stratum contributes its own variance (s_i^2) and sample size (n_i)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#t-critical-value",
    "href": "lectures/L02/Lecture-02b.html#t-critical-value",
    "title": "Lecture 02b – Sampling designs II",
    "section": "t-critical value",
    "text": "t-critical value\nDegrees of freedom df\ndf = n - L\nwhere n is the total number of samples and L is the number of strata.\n\n\nThe degrees of freedom tells us how much “free information” we have for making estimates\nFor stratified sampling, we lose one degree of freedom for each stratum\nExample: If we have 12 samples in 3 strata:\n\nThe degrees of freedom is 12 - 3 = 9\nThink of it this way: 9 samples can be placed anywhere, but we must have at least 1 sample in each of the 3 strata\n\n\n\n\nIn R\n\n\nCode\ndf &lt;- length(landA) + length(landB) - 2\nt_crit &lt;- qt(0.975, df)\nt_crit\n\n\n[1] 2.570582"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "95 % Confidence interval for stratified random sampling",
    "text": "95 % Confidence interval for stratified random sampling\nThe formula\n95\\%\\ CI = \\bar y_{s} \\pm t^{0.025}_{n-L} \\times SE(\\bar y_{s})\nwhere L is the number of strata, n is the total number of samples, and \\bar y_{s} is the weighted mean of the strata.\nIn simple terms:\n\n\nWe’re creating a range where we’re 95% confident the true population mean lies\nWe start with our best estimate (the pooled mean \\bar y_{s})\nWe add and subtract a “margin of error” (which depends on our sample size and variability)\nThe margin of error = t-critical value × standard error\n\n\nVisualising this:\nLower bound ← [Pooled mean - Margin of error] ... [Pooled mean + Margin of error] → Upper bound"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling-1",
    "href": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling-1",
    "title": "Lecture 02b – Sampling designs II",
    "section": "95 % Confidence interval for stratified random sampling",
    "text": "95 % Confidence interval for stratified random sampling\nPutting it all together\n\n\nCode\nvarA &lt;- var(landA) / length(landA)  # variance of the mean for A\nvarB &lt;- var(landB) / length(landB)  # variance of the mean for B\nweighted_var &lt;- weight[1]^2 * varA + weight[2]^2 * varB\nweighted_se &lt;- sqrt(weighted_var)\nci &lt;- c(\n  L95 = weighted_mean - t_crit * weighted_se,\n  u95 = weighted_mean + t_crit * weighted_se\n)\nci\n\n\n     L95      u95 \n61.04864 76.68803"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-vs.-stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#simple-random-vs.-stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random vs. stratified random sampling",
    "text": "Simple random vs. stratified random sampling\nWhat if we had used stratified random sampling instead of simple random sampling (and collected the same amount of data)?\nWhat differences can you see?\n\n\nCode\nlibrary(tidyverse)\n# Manually printing the results below as SRS data is in previous lecture\ncompare &lt;- tibble(\n  Design = c(\"Simple Random\", \"Stratified Random\"),\n  Mean = c(67.29, 68.9), \n  `Var (mean)` = c(50.83, 9.30),\n  L95 = c(49.85, 61), \n  U95 = c(84.73, 76.7), \n  df = c(6, 5))\nknitr::kable(compare)\n\n\n\n\n\nDesign\nMean\nVar (mean)\nL95\nU95\ndf\n\n\n\n\nSimple Random\n67.29\n50.83\n49.85\n84.73\n6\n\n\nStratified Random\n68.90\n9.30\n61.00\n76.70\n5"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#visual-comparison-of-95-confidence-intervals",
    "href": "lectures/L02/Lecture-02b.html#visual-comparison-of-95-confidence-intervals",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Visual comparison of 95% confidence intervals",
    "text": "Visual comparison of 95% confidence intervals\n\n\nCode\n# Creating a visual comparison of confidence intervals\nggplot(compare, aes(x = Design, y = Mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = L95, ymax = U95), width = 0.2, size = 1) +\n  labs(title = \"95% Confidence Intervals by Sampling Design\",\n       y = \"Soil Carbon (tonnes/ha)\",\n       x = \"\") +\n  theme_minimal(base_size = 14) +\n  annotate(\"text\", x = 2, y = 55, \n           label = \"Stratified sampling gives a\\nnarrower confidence interval\\n(more precise estimate)\", \n           color = \"blue\")\n\n\n\n\nKey insights:\n\nBoth methods give similar estimates of the mean\nStratified sampling produces a much narrower confidence interval\nThe variance of the mean is about 5 times smaller with stratified sampling\nThis means stratified sampling is much more precise with the same number of samples"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#efficiency",
    "href": "lectures/L02/Lecture-02b.html#efficiency",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Efficiency",
    "text": "Efficiency\nWhat is sampling efficiency?\n\nA measure of how much “bang for your buck” you get with different sampling methods\nCalculated as a ratio: \\text{Efficiency} = \\frac{\\text{Variance of SRS}}{\\text{Variance of Stratified}}\n\n\nIn simple terms:\n\nEfficiency &gt; 1: Stratified sampling is better (more precise with same sample size)\nEfficiency = 5 means: You’d need 5 times as many samples with simple random sampling to get the same precision as stratified sampling\n\n\n\nIn R\n\n\nCode\nefficiency &lt;- 50.83 / 9.30\nefficiency\n\n\n[1] 5.465591\n\n\nHow many samples would we have had to collect using simple random sampling to achieve the same precision as our stratified sample?\n\n\nCode\nround(7 * efficiency, 0)\n\n\n[1] 38\n\n\nSo we would need about 38 samples with simple random sampling to get the same precision that we achieved with just 7 samples using stratified sampling!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#tips-on-implementation",
    "href": "lectures/L02/Lecture-02b.html#tips-on-implementation",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Tips on implementation",
    "text": "Tips on implementation\n\nThe most difficult part is to identify the strata and assign the sampling units to the strata\nCommon stratification variables in environmental science:\n\nSpatial: elevation bands, soil types, vegetation zones\nTemporal: seasons, time of day, growth stages\nManagement: treatment types, land-use history\n\nStrata sampling size: allocate samples to strata based on the size of the strata, either proportional to:\n\nthe size of the strata (e.g. 60% of area = 60% of samples)\nthe variance of the strata (more samples where variation is higher)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#the-change-in-mean-delta-bar-y",
    "href": "lectures/L02/Lecture-02b.html#the-change-in-mean-delta-bar-y",
    "title": "Lecture 02b – Sampling designs II",
    "section": "The change in mean \\Delta \\bar y",
    "text": "The change in mean \\Delta \\bar y\nImportant considerations\n\n\nWe want to measure change in soil carbon over time\nKey question: How do we select sites for the second measurement?\n\nReturn to the same sites?\nSelect completely new sites?\n\nThis choice affects our statistical analysis (covariance)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#monitoring-estimates",
    "href": "lectures/L02/Lecture-02b.html#monitoring-estimates",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Monitoring estimates",
    "text": "Monitoring estimates\nChange in mean \\Delta \\bar y\n\nThe difference between the means of the two sets of measurements.\n\n\\Delta \\bar y = \\bar y_2 - \\bar y_1\nwhere \\bar y_2 and \\bar y_1 are the means of the second and first set of measurements, respectively."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#uncertainty-in-change-estimates",
    "href": "lectures/L02/Lecture-02b.html#uncertainty-in-change-estimates",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Uncertainty in change estimates",
    "text": "Uncertainty in change estimates\n\nVariance of the change in mean Var(\\Delta{\\bar y})\nThis tells us how precise our estimate of the change is. It depends on:\nVar(\\Delta{\\bar y}) = Var(\\bar y_2) + Var(\\bar y_1) - 2 \\times Cov(\\bar y_2, \\bar y_1)\nIn simple terms:\n\nThe uncertainty in our change estimate comes from the uncertainties in both measurements\nHowever, if we sample the same sites twice, they are related to each other (covariance)\nThis relationship usually reduces the overall uncertainty in our change estimate\n\n\n\nImportant: Visiting the same sites twice (paired sampling) usually gives more precise estimates of change than visiting different sites each time!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#covariance-and-site-selection",
    "href": "lectures/L02/Lecture-02b.html#covariance-and-site-selection",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Covariance and site selection",
    "text": "Covariance and site selection\nQuick decision guide\n\nSame sites? Use paired approach:\n\nSites are the same in both visits\nUse paired t-test\nAccount for covariance between visits\n\nDifferent sites? Use independent approach:\n\nNew random sites in second visit\nUse two-sample t-test\nNo covariance between visits"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#what-is-covariance",
    "href": "lectures/L02/Lecture-02b.html#what-is-covariance",
    "title": "Lecture 02b – Sampling designs II",
    "section": "What is covariance?",
    "text": "What is covariance?\n\nCovariance measures how two measurements relate to each other:\nExample with soil carbon:\n\nSite 1: First visit = 90 t/ha, Second visit = 95 t/ha\nSite 2: First visit = 48 t/ha, Second visit = 52 t/ha\nSite 3: First visit = 71 t/ha, Second visit = 75 t/ha\n\n\n\nWhat do you notice? Sites with high carbon in the first measurement still have high carbon in the second measurement (positive covariance).\nWhy this matters: Knowing the first measurement helps us predict the second one, reducing uncertainty in our estimate of change.\n\n\nPractical takeaway: When measuring change over time, returning to the same sites usually gives more precise results because it removes site-to-site variation."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#calculating-the-95-ci-for-the-change-in-mean",
    "href": "lectures/L02/Lecture-02b.html#calculating-the-95-ci-for-the-change-in-mean",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Calculating the 95% CI for the change in mean",
    "text": "Calculating the 95% CI for the change in mean\nThe formula looks similar to before:\n95\\%\\ CI = \\Delta \\bar y \\pm t^{0.025}_{n-1} \\times SE(\\Delta \\bar y)\n\nIn plain language:\n\nWe have our best estimate of the change (the difference between the two means)\nWe add and subtract a margin of error to create a range\nWe’re 95% confident that the true change falls within this range\n\n\n\nThe standard error of the change SE(\\Delta \\bar y)\n\nThis tells us how precise our estimate of the change is\nIt’s complicated to calculate by hand, especially when we visit the same sites twice\nIf we visit the same sites twice, we need to account for their relationship (covariance)\n\n\n\nGood news! You don’t need to calculate this by hand!\n\nR can do these calculations for you using the t.test() function\nFor same sites: use paired = TRUE option\nFor different sites: use paired = FALSE option\nWe’ll practice this in the lab!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#thanks",
    "href": "lectures/L02/Lecture-02b.html#thanks",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Thanks!",
    "text": "Thanks!\nQuestions?\nThis presentation is based on the SOLES Quarto reveal.js template and is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "lectures/L01/index.html",
    "href": "lectures/L01/index.html",
    "title": "Lecture 01",
    "section": "",
    "text": "Lecture 01a – Welcome Full Screen | PDF\n\nLecture 01b – The beginning is the end: a revision Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1: Designed studies**",
      "L01 -- Introduction"
    ]
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#staff",
    "href": "lectures/L01/Lecture-01a.html#staff",
    "title": "Lecture 01a – Welcome",
    "section": "Staff",
    "text": "Staff\n\n\n\n\n\n\n\n\n\nA. Prof Aaron Greenville\n\n\n\n\n\n\n\nDr Si Yang Han\n\n\n\n\n\n\n\nDr Januar Harianto\n\n\n\n\n\n\n\nProf Mathew Crowther"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#structure",
    "href": "lectures/L01/Lecture-01a.html#structure",
    "title": "Lecture 01a – Welcome",
    "section": "Structure",
    "text": "Structure\nThis unit includes lectures, self-guided tutorials, labs, discussions, and drop-in sessions.\n\nLectures: Tuesdays 10 AM, Wednesdays 11 AM, Chemistry Lecture Theatre 3\nTutorials: Self-guided (1 hour), complete before each week’s lab.\nLabs: South Eveleigh Precinct, Thursday 9 am – 12 pm, Friday 10 am – 1 pm, 2 pm – 5 pm\nDiscussion: Via Ed discussion, we usually respond the same day unless it is the weekend.\nDrop-in sessions: Scheduled as necessary (Zoom or in person). Email us to arrange a session."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#attendance",
    "href": "lectures/L01/Lecture-01a.html#attendance",
    "title": "Lecture 01a – Welcome",
    "section": "Attendance",
    "text": "Attendance\n\nLectures: Highly recommended but not compulsory. Lectures are recorded, capturing slides and audio only, which may miss important discussions.\nLabs: Mandatory, 80% minimum attendance required. Attendance will be taken by QR code. If you miss a lab, you may attend another session that week – send us an email!\nTutorials: Self-guided (1 hour), complete before each week’s lab."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#assessments",
    "href": "lectures/L01/Lecture-01a.html#assessments",
    "title": "Lecture 01a – Welcome",
    "section": "Assessments",
    "text": "Assessments\nCheck Unit Outline\n\n\n\n\n\n\n\n\n\nWeek\nAssessment\nWeight\nType\n\n\n\n\n4\nEarly Feedback Task\n1%\nIndividual\n\n\n5\nProject 1: Describing data\n10%\nIndividual\n\n\n10\nProject 2: Analysing experimental data\n20%\nIndividual\n\n\n13\nProject 3: Presentation (multivariate)\n20%\nGroup\n\n\n-\nQuizzes (weekly, multiple due dates)\n4%\nIndividual\n\n\n-\nExam (2 hours, MCQs + Short Answers)\n45%\nIndividual"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#biomedical-building",
    "href": "lectures/L01/Lecture-01a.html#biomedical-building",
    "title": "Lecture 01a – Welcome",
    "section": "Biomedical Building",
    "text": "Biomedical Building\n\nCredit: Michael Wheatland"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#directions",
    "href": "lectures/L01/Lecture-01a.html#directions",
    "title": "Lecture 01a – Welcome",
    "section": "Directions",
    "text": "Directions\nBuses\nCourtesy buses are available:\n\nThe best option is to take the bus from Fisher Library to Redfern Station, then walk to the precinct (through the new station platform as “local traffic”).\nAlternatively, direct buses are available – but less frequent.\n\nDriving\nFree parking is available around Henderson Road, but it is extremely crowded. We do not recommend driving to the precinct."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#walking",
    "href": "lectures/L01/Lecture-01a.html#walking",
    "title": "Lecture 01a – Welcome",
    "section": "Walking",
    "text": "Walking\nWalking to the South Eveleigh Precinct takes about 20 minutes. However, you can save approximately 5 minutes by using Redfern station’s community access gates, where you don’t need to use an Opal card to get through.\nIf the map does not load, click here"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#statistics-in-action",
    "href": "lectures/L01/Lecture-01a.html#statistics-in-action",
    "title": "Lecture 01a – Welcome",
    "section": "Statistics in Action",
    "text": "Statistics in Action\nModern science and decision-making are driven by data:\n\n\nResearch: From lab experiments to field studies\nPolicy: Environmental management decisions\nIndustry: Business analytics and optimisation\nInnovation: AI and machine learning – foundations"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#why-statistics-matters",
    "href": "lectures/L01/Lecture-01a.html#why-statistics-matters",
    "title": "Lecture 01a – Welcome",
    "section": "Why Statistics Matters?",
    "text": "Why Statistics Matters?\nStatistics empowers you to:\n\n\nTurn raw data into meaningful insights\nMake evidence-based decisions\nCommunicate findings effectively\nSolve complex real-world problems\n\n\n\n\n\n\nStatistics helps avoid misinterpreting data. Source: Anchorman (2004)"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#real-world-applications",
    "href": "lectures/L01/Lecture-01a.html#real-world-applications",
    "title": "Lecture 01a – Welcome",
    "section": "Real-world Applications",
    "text": "Real-world Applications\n\nSource: NASA’s Global Temperature Index"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#sports-analytics",
    "href": "lectures/L01/Lecture-01a.html#sports-analytics",
    "title": "Lecture 01a – Welcome",
    "section": "Sports Analytics",
    "text": "Sports Analytics\n\nThe 10 highest-seeded players averaged 3.48 rounds won in the Australian Open since 2011, compared to just 3.03 at Wimbledon. Source: fivethirtyeight"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#your-path-ahead",
    "href": "lectures/L01/Lecture-01a.html#your-path-ahead",
    "title": "Lecture 01a – Welcome",
    "section": "Your Path Ahead",
    "text": "Your Path Ahead\nThis course will develop your:\n\nTechnical Skills\n\nR programming proficiency\nData visualization expertise\nStatistical analysis methods\n\n\n\nProfessional Skills\n\nCritical thinking\nScientific communication\nProblem-solving abilities"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#lecture-attendance-options",
    "href": "lectures/L01/Lecture-01a.html#lecture-attendance-options",
    "title": "Lecture 01a – Welcome",
    "section": "Lecture attendance options",
    "text": "Lecture attendance options\nIn-person vs. online recordings\n\nIn-person benefits:\n\nReal-time interaction with peers and lecturers\nImmediate feedback and clarification of concepts\nActive participation in discussions and polls\nBuilding connections with classmates\n\nOnline recording benefits:\n\nFlexibility to manage other commitments\nAbility to pause and review complex concepts\nLearn at your own pace\nConvenient for those with long commutes"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#on-campus-or-online",
    "href": "lectures/L01/Lecture-01a.html#on-campus-or-online",
    "title": "Lecture 01a – Welcome",
    "section": "On-campus or online?",
    "text": "On-campus or online?\nChoose the option that best suits your learning style and circumstances. If watching online, try to:\n\nStay up to date with recordings to avoid falling behind\nUse Ed discussion board actively for questions\nAttend some lectures in person when possible for key topics\n\nThere is a strong positive correlation between lecture attendance and final grades – but it’s not the only factor. It may just be the case that students who attend lectures are more likely to keep up with the course material."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#put-in-the-hours",
    "href": "lectures/L01/Lecture-01a.html#put-in-the-hours",
    "title": "Lecture 01a – Welcome",
    "section": "Put in the hours",
    "text": "Put in the hours\n\nThis is a 6 credit point unit, which means that you are expected to spend 120 – 150 hours in total, including exam prep time (~10 h per week)!\nPractice makes perfect. Tutorials and Labs help you apply the concepts you learn in lectures – complete all the exercises, and practice with the bonus questions provided."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#ask-questions",
    "href": "lectures/L01/Lecture-01a.html#ask-questions",
    "title": "Lecture 01a – Welcome",
    "section": "Ask questions",
    "text": "Ask questions\n\nEd is the best place to ask questions. We are way more responsive on Ed than on email.\nWe are open to the use of AI tools (including LLMs like ChatGPT) to help you answer questions about code… but don’t use them to cheat yourself out of learning.\nWe have drop-in sessions, where you can jump in and have a chat on Zoom. We will announce the schedule on Ed."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#by-the-end-of-this-course-we-want-you-to-be-able-to",
    "href": "lectures/L01/Lecture-01a.html#by-the-end-of-this-course-we-want-you-to-be-able-to",
    "title": "Lecture 01a – Welcome",
    "section": "By the end of this course, we want you to be able to:",
    "text": "By the end of this course, we want you to be able to:\n\nLO1 demonstrate proficiency in designing sample schemes and analysing data from them using R.\nLO2 describe and identify the basic features of an experimental design: replicate, treatment structure and blocking structure.\nLO3 demonstrate proficiency in the use or the statistical programming language R to apply an ANOVA and fit regression models to experimental data.\nLO4 demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data.\nLO5 interpret the output and understand conceptually how its derived of a regression, ANOVA and multivariate analysis that have been calculated by R.\nLO6 write statistical and modelling results as part of a scientific report.\nLO7 appraise the validity of statistical analyses used publications."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX2001 – Applied Statistical Methods",
    "section": "",
    "text": "This site contains ONLY SOME of the lecture material for ENVX2001. It is meant for lecturers to publish their lecture content in a structured way and has not been designed for student use. Most of the content is still in development and will be updated throughout the semester. If you happen to stumble upon this site, feel free to have a look around… but be aware that the content is not final and may contain errors.\nENVX2001 students should always navigate to Canvas to access the ENVX2001 page and view the course material in the right context. If you are looking for a specific lecture, please refer to the Canvas page for the course.\nModule 1: Designed studies\n\nLecture 01 – Introduction\nLecture 02 – Sampling designs\nLecture 03 – 1-way ANOVA\nLecture 04 – Residual diagnostics & post hoc tests\nLecture 05 – Experimental design\nLecture 06 – ANOVA with blocking\n\nModule 2: Finding patterns in data\n\nLecture 07 – Regression modelling\nLecture 08 – Regression model development\nLecture 09 – Regression model assessment\nLecture 10 – Principle component analysis\nLecture 11 – Clustering\nLecture 12 – Multidimensional scaling",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Home**"
    ]
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#assumed-knowledge",
    "href": "lectures/L01/Lecture-01b.html#assumed-knowledge",
    "title": "Lecture 01b – Revision",
    "section": "Assumed knowledge",
    "text": "Assumed knowledge\n\nSamples, populations and statistical inference\nProbability distributions\nParameter estimation\n\ncentral tendency\nspread or variability\n\nSampling distribution of the mean\n\nStandard error\nConfidence intervals\nCentral Limit Theorem"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#why-is-this-important",
    "href": "lectures/L01/Lecture-01b.html#why-is-this-important",
    "title": "Lecture 01b – Revision",
    "section": "Why is this important?",
    "text": "Why is this important?\n\nUnderstanding sampling informs experimental design (Week 4 onward). How many samples do we need and are our samples representative?\nRecognising sample (not sampling) distributions helps us choose the right statistical model – e.g. t-test to compare two means that are normally distributed.\nMost statistical techniques use sample statistics for interpretation, e.g. the t-test can be explained using confidence intervals, and the ANOVA test can be interpreted in part using means and standard errors.\n\nAll of these concepts will make more sense as we go through the course, but if you do not try to understand them now, you will struggle."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#populations-and-samples",
    "href": "lectures/L01/Lecture-01b.html#populations-and-samples",
    "title": "Lecture 01b – Revision",
    "section": "Populations and samples",
    "text": "Populations and samples\nPopulations\n\nAll the possible units and their associated observations of interest\nScientists are often interested in making inferences about populations, but measuring every unit is impractical\n\nSamples\n\nA collection of observations from any population is a sample, and the number of observations in it is the sample size\nWe assume samples that we collect can be used to make inferences about the population\nNEW: Samples need to be representative of the population"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#statistics-vs-parameters",
    "href": "lectures/L01/Lecture-01b.html#statistics-vs-parameters",
    "title": "Lecture 01b – Revision",
    "section": "Statistics vs parameters",
    "text": "Statistics vs parameters\n\nCharacteristics of the population are called parameters (e.g. population mean or population regression slope)\nCharacteristics of the sample are called statistics (e.g. sample mean or sample regression slope) – they are used to estimate the population parameters\nStatistics are what we use to help us understand the population\nFormal statistical methods can help us make inferences about the population based on the sample – statistical inference\nNot all statistical techniques are inferential, but many are"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sample-data",
    "href": "lectures/L01/Lecture-01b.html#sample-data",
    "title": "Lecture 01b – Revision",
    "section": "Sample data",
    "text": "Sample data\nSample data are usually collected as variables, which are the characteristics we measure or record from each object.\n\nVariables can be:\nCategorical Variables\n\nNominal: categories without a natural order (e.g. colors, names)\nOrdinal: categories with a natural order (e.g. ratings, rankings)\n\nNumerical Variables\n\nContinuous: can take any value within a range (e.g. height, weight)\nDiscrete: can take only specific values (e.g. counts, presence/absence)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#you-decide-on-what-a-variable-represents",
    "href": "lectures/L01/Lecture-01b.html#you-decide-on-what-a-variable-represents",
    "title": "Lecture 01b – Revision",
    "section": "YOU decide on what a variable represents",
    "text": "YOU decide on what a variable represents\nA numerical, continuous variable can be treated as a categorical variable if you decide to categorise it.\nExamples\n\n\nheight (in cm) – a numerical, continuous variable, can be treated as a categorical variable if you group it into categories (short, medium, tall)\nage (in years) – a numerical, discrete variable, can be treated as a continuous variable (if we allow for certain issues)\ntreatment (A, B, C) – a categorical variable, can be treated as a numerical variable if we assign numbers to the treatments (1, 2, 3) and assume they are ordered e.g. effect of 1 &lt; 2 &lt; 3 – the basis of non-parametric tests"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#types-of-probability-distributions",
    "href": "lectures/L01/Lecture-01b.html#types-of-probability-distributions",
    "title": "Lecture 01b – Revision",
    "section": "Types of probability distributions",
    "text": "Types of probability distributions\nPopulations can be described by probability distributions, and by now, you should be familiar with these distributions and their properties\n\n\nNormal Distribution: Bell-shaped curve, symmetric around the mean. Data is continuous\nBinomial Distribution: Models success/failure outcomes in a fixed number of trials. Data is discrete\nPoisson Distribution: Models count data when events occur at a constant rate. Data is discrete\n\n\n\nKnowing the distribution of your data is important for choosing the right statistical model – although it is not always necessary."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#section",
    "href": "lectures/L01/Lecture-01b.html#section",
    "title": "Lecture 01b – Revision",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nset.seed(908)\nnormal_data &lt;- data.frame(x = rnorm(10000, mean = 0, sd = 1))\nbinomial_data &lt;- data.frame(x = rbinom(10000, size = 10, prob = 0.5))\npoisson_data &lt;- data.frame(x = rpois(500, lambda = 3))\n\n# normal\np1 &lt;- ggplot(normal_data, aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  ggtitle(\"Normal Distribution\")\n\n# binomial\np2 &lt;- ggplot(binomial_data, aes(x = x)) +\n  geom_bar(fill = \"lightgreen\", color = \"black\") +\n  ggtitle(\"Binomial Distribution\")\n\n# poisson\np3 &lt;- ggplot(poisson_data, aes(x = x)) +\n  geom_bar(fill = \"lightpink\", color = \"black\") +\n  ggtitle(\"Poisson Distribution\")\n\n# Arrange plots\nlibrary(patchwork)\np1 | p2 / p3"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-central-tendency",
    "href": "lectures/L01/Lecture-01b.html#measures-of-central-tendency",
    "title": "Lecture 01b – Revision",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nMean \\bar{x}\n\nThe arithmetic average of all values in a dataset\nSum of all values divided by number of observations\nSensitive to extreme values (outliers)\n\n\n\n\n\n\n\n\nFormula\n\n\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\n\nwhere n is the number of observations\nx_i represents each individual value\n\\sum means we add up all values from i=1 to n\nExample: for data {2,4,6,8}, n=4 and \\bar{x} = \\frac{2+4+6+8}{4} = 5"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#section-1",
    "href": "lectures/L01/Lecture-01b.html#section-1",
    "title": "Lecture 01b – Revision",
    "section": "",
    "text": "Median\n\nMiddle value when data is ordered\n50th percentile of the data\nMore robust to outliers than mean\nFor even n, average of two middle values\n\n\nMode\n\nMost frequently occurring value\nCan have multiple modes\nOnly measure of central tendency for categorical data\nNot always meaningful for continuous data"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#how-they-compare",
    "href": "lectures/L01/Lecture-01b.html#how-they-compare",
    "title": "Lecture 01b – Revision",
    "section": "How they compare",
    "text": "How they compare\n\nDepending on the distribution of the data, the mean and median can be different, and this can tell you something about the data."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nVariance s^2\n\nMeasures how spread out the data is from the mean\nCalculated as average squared deviations from the mean\nSquared units make it harder to interpret\nSensitive to outliers (squares large deviations)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion-1",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion-1",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\n\n\n\n\n\n\nFormula\n\n\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\n\nwhere n is the number of observations\nx_i represents each individual value\n\\bar{x} is the sample mean\nFor data {2,4,6,8}:\n\nmean = 5\ndifferences = (-3,-1,1,3), squares = (9,1,1,9), sum = 20\nTherefore, s^2 = \\frac{20}{3} \\approx 6.67"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion-2",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion-2",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nStandard Deviation s\n\nSquare root of variance\nSame units as original data\nMore interpretable than variance\nEmpirical rule for normal distributions:\n\n≈68% of data within ±1 SD\n≈95% of data within ±2 SD\n≈99.7% of data within ±3 SD\n\n\n\n\n\n\n\n\n\nFormula\n\n\ns = \\sqrt{s^2}\n\nSimply the square root of variance\nFor our example data {2,4,6,8}:\n\ns = \\sqrt{6.67} \\approx 2.58\n\nInterpretation: On average, values deviate about 2.58 units from the mean"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#visualising-standard-deviation",
    "href": "lectures/L01/Lecture-01b.html#visualising-standard-deviation",
    "title": "Lecture 01b – Revision",
    "section": "Visualising standard deviation",
    "text": "Visualising standard deviation"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#population-parameters-vs-sample-statistics",
    "href": "lectures/L01/Lecture-01b.html#population-parameters-vs-sample-statistics",
    "title": "Lecture 01b – Revision",
    "section": "Population parameters vs sample statistics",
    "text": "Population parameters vs sample statistics\nFor those of you interested:\nMean\n\n\n\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\n\n\n\nVariance\n\n\n\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\n\n\n\nStandard Deviation\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\sigma = \\sqrt{\\sigma^2}\ns = \\sqrt{s^2}\n\n\n\n\n\n\n\n\n\nNote\n\n\nNotice the use of n-1 in sample variance and standard deviation"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#why-n-1",
    "href": "lectures/L01/Lecture-01b.html#why-n-1",
    "title": "Lecture 01b – Revision",
    "section": "Why n-1?",
    "text": "Why n-1?\n\nWhen calculating sample variance, we use n-1 instead of n in the denominator\nThis is called “Bessel’s correction”\nWhy? Because we lose one “degree of freedom” when we estimate the mean:\n\nIf you know the sample mean (\\bar{x})\nAnd you know all but one value in your sample\nThe last value is constrained - it must make the mean equal \\bar{x}"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#what-is-a-sampling-distribution",
    "href": "lectures/L01/Lecture-01b.html#what-is-a-sampling-distribution",
    "title": "Lecture 01b – Revision",
    "section": "What is a sampling distribution?",
    "text": "What is a sampling distribution?\n\nDistribution of a statistic (e.g., mean) calculated from repeated samples\nShows how sample statistics vary from sample to sample\nImportant for understanding sampling variability and making inferences"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sampling-distribution-of-the-mean",
    "href": "lectures/L01/Lecture-01b.html#sampling-distribution-of-the-mean",
    "title": "Lecture 01b – Revision",
    "section": "Sampling distribution of the mean",
    "text": "Sampling distribution of the mean"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#central-limit-theorem",
    "href": "lectures/L01/Lecture-01b.html#central-limit-theorem",
    "title": "Lecture 01b – Revision",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nI know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the Central Limit Theorem. The law would have been personified by the Greeks and deified, if they had known of it.”\n\n– Sir Francis Galton, 1889, Natural Inheritance\nThe Central Limit Theorem (CLT) states that for sufficiently large samples:\n\nThe sampling distribution of the mean follows a normal distribution\nThe mean of the sampling distribution equals the population mean\nThe standard deviation of the sampling distribution (standard error) = \\frac{\\sigma}{\\sqrt{n}}"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#clt-in-action",
    "href": "lectures/L01/Lecture-01b.html#clt-in-action",
    "title": "Lecture 01b – Revision",
    "section": "CLT in action",
    "text": "CLT in action\n\n\nCode\n# Create a skewed population\nset.seed(456)\nskewed_pop &lt;- exp(rnorm(10000, mean = 0, sd = 0.5))\n\n# Sample means for different sample sizes (ordered small to large)\nsample_sizes &lt;- c(5, 30, 100)\nsample_labels &lt;- factor(paste(\"n =\", sample_sizes),\n  levels = paste(\"n =\", sample_sizes)\n) # preserve order\nsample_dist_data &lt;- lapply(sample_sizes, function(n) {\n  means &lt;- replicate(1000, mean(sample(skewed_pop, size = n)))\n  data.frame(means = means, size = factor(paste(\"n =\", n), levels = levels(sample_labels)))\n})\nsample_dist_df &lt;- do.call(rbind, sample_dist_data)\n\n# Plot\nggplot() +\n  geom_histogram(aes(x = means, y = ..density..),\n    data = sample_dist_df,\n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  geom_density(aes(x = means), data = sample_dist_df, color = \"blue\") +\n  facet_wrap(~size, scales = \"free_x\") +\n  ggtitle(\"Sampling distributions for different sample sizes\")"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#example",
    "href": "lectures/L01/Lecture-01b.html#example",
    "title": "Lecture 01b – Revision",
    "section": "Example",
    "text": "Example\n\n\nCode\nset.seed(239)\n# Generate a skewed distribution\nskewed &lt;- tibble(\n  x = rgamma(1000, shape = 2, scale = 1)\n)\n\n# plot in ggplot2\nggplot(data = skewed, aes(x = x)) +\n  geom_histogram(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlab(\"Height (m)\")\n\n\n\n\nSkewed population distribution for tree heights.\nWe want to estimate the mean height of the trees in the forest."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sample-no-summary-statistic",
    "href": "lectures/L01/Lecture-01b.html#sample-no-summary-statistic",
    "title": "Lecture 01b – Revision",
    "section": "1 sample (no summary statistic)",
    "text": "1 sample (no summary statistic)\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 1,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\nWith only one sample, we are not really seeing a sampling distribution – we are just replicating the same population distribution. A sampling distribution emerges when we take multiple samples and calculate their means."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-1",
    "href": "lectures/L01/Lecture-01b.html#samples-1",
    "title": "Lecture 01b – Revision",
    "section": "2 samples",
    "text": "2 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 2,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nWe sample 2 trees and calculate the mean height, and repeat this 1000 times.\nThe distribution of sample means is starting to look more like a normal distribution."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-2",
    "href": "lectures/L01/Lecture-01b.html#samples-2",
    "title": "Lecture 01b – Revision",
    "section": "5 samples",
    "text": "5 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 5,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nFive random samples per calculated mean, repeated 1000 times.\nThe distribution is becoming more normal, and the spread is decreasing: estimate is getting more precise."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-3",
    "href": "lectures/L01/Lecture-01b.html#samples-3",
    "title": "Lecture 01b – Revision",
    "section": "30 samples",
    "text": "30 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 30,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nThirty random samples per calculated mean, repeated 1000 times.\nThe distribution of sample means is very close to a normal distribution."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-4",
    "href": "lectures/L01/Lecture-01b.html#samples-4",
    "title": "Lecture 01b – Revision",
    "section": "50 samples",
    "text": "50 samples\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nFifty random samples per calculated mean, repeated 1000 times.\nHow many samples is enough?"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#are-50-samples-normal-enough",
    "href": "lectures/L01/Lecture-01b.html#are-50-samples-normal-enough",
    "title": "Lecture 01b – Revision",
    "section": "Are 50 samples “normal” enough?",
    "text": "Are 50 samples “normal” enough?\n\n\nCode\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = 2, # population mean for gamma(2,1)\n      sd = sqrt(2) / sqrt(50) # theoretical SE for gamma(2,1)\n    ),\n    linewidth = 1,\n    color = \"blue\",\n    linetype = \"dashed\"\n  ) +\n  xlab(\"Mean height (m)\")\n\n\n\n\nFifty random samples per calculated mean, repeated 1000 times.\nHow many samples is enough?"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#effect-of-sample-size",
    "href": "lectures/L01/Lecture-01b.html#effect-of-sample-size",
    "title": "Lecture 01b – Revision",
    "section": "Effect of sample size",
    "text": "Effect of sample size\n\n\nCode\nlibrary(tidymodels)\nlibrary(patchwork)\nset.seed(642)\n\nheights &lt;- tibble(heights = rnorm(1000, 1.99, 1))\npopmean &lt;- mean(heights$heights)\nsample_sizes &lt;- c(2, 5, 25, 100)\nn &lt;- length(sample_sizes)\n\nheights &lt;- tibble(heights = rgamma(1000, shape = 2, scale = 1))\nsample_sizes &lt;- c(2, 5, 25, 100)\nn &lt;- length(sample_sizes)\n\nplots &lt;- lapply(sample_sizes, function(size) {\n  df &lt;- heights |&gt;\n    rep_sample_n(size = size, reps = 2000) |&gt;\n    group_by(replicate) |&gt;\n    summarise(xbar = mean(heights))\n\n  mean_xbar &lt;- mean(df$xbar)\n\n  ggplot(df, aes(x = xbar)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    geom_vline(aes(xintercept = mean_xbar), color = \"blue\", linetype = \"dashed\") +\n    geom_text(aes(x = mean_xbar, label = sprintf(\"%.2f\", mean_xbar), y = Inf), hjust = -0.1, vjust = 2, color = \"blue\") +\n    ggtitle(paste0(\"Sample Size: \", size)) +\n    xlab(\"Mean height (m)\") +\n    xlim(-3, 8)\n})\nwrap_plots(plots)\n\n\n\nIncreased sample size leads to a more accurate estimate of the population mean, reflected by the narrower distribution of the sample mean, which is captured by the standard error."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#effect-of-variability",
    "href": "lectures/L01/Lecture-01b.html#effect-of-variability",
    "title": "Lecture 01b – Revision",
    "section": "Effect of variability",
    "text": "Effect of variability\n\n\nCode\nset.seed(1221)\n\n# Define a function to generate ggplot objects\ngenerate_plot &lt;- function(sd) {\n  data &lt;- rnorm(500, 1.99, sd)\n  p &lt;- ggplot(data = tibble(x = data), aes(x = x)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    ggtitle(paste(\"SD =\", sd)) +\n    xlim(-100, 100)\n  return(p)\n}\n\n# Apply the function to a list of standard deviations\nsds &lt;- c(3, 6, 15, 25)\nplots &lt;- lapply(sds, generate_plot)\n\n# Wrap the plots\nwrap_plots(plots)\n\n\n\nIncreased variability (i.e. wide range of tree heights) leads to a wider distribution of the sample mean (i.e. less precision), which is also reflected by the standard error."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#clt-drives-statistical-inference",
    "href": "lectures/L01/Lecture-01b.html#clt-drives-statistical-inference",
    "title": "Lecture 01b – Revision",
    "section": "CLT drives statistical inference",
    "text": "CLT drives statistical inference\nBecause of how predictable the CLT applies to sample means, we can use this to make reasonably accurate inferences about the population mean, even if we do not know the population distribution.\n\nA sampling distribution of the mean will be normally distributed for sufficiently large samples – how large is “sufficient” depends on the population distribution\nThe mean of the sampling distribution trends towards the population mean with increasing sample size\nTo determine how well the sample mean estimates the population mean, we use the standard error of the mean – basically a standard deviation of the sampling distribution"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#standard-error-of-the-mean",
    "href": "lectures/L01/Lecture-01b.html#standard-error-of-the-mean",
    "title": "Lecture 01b – Revision",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\n\nMeasures the precision of a sample mean\nDescribes variation in sample means – around the true population mean\nDecreases as sample size increases, because we become more “confident” in our estimate\n\n\n\n\n\n\n\n\nFormula\n\n\nSE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n\nwhere s is the sample standard deviation\nn is the sample size"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#when-to-report-sd-or-se",
    "href": "lectures/L01/Lecture-01b.html#when-to-report-sd-or-se",
    "title": "Lecture 01b – Revision",
    "section": "When to report SD or SE",
    "text": "When to report SD or SE\nStandard Deviation (SD)\n\nDescribes variability in your data\nStays constant regardless of sample size\n\nStandard Error (SE)\n\nDescribes precision of your mean estimate\nDecreases with larger sample size (SE = \\frac{SD}{\\sqrt{n}})\n\nWhen reporting statistics:\n\nUse mean ± SE to show precision of your estimate\nUse mean ± SD to show spread of your raw data\nSE can appear deceptively small with large sample sizes – always report sample size!"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#confidence-intervals",
    "href": "lectures/L01/Lecture-01b.html#confidence-intervals",
    "title": "Lecture 01b – Revision",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWhat is a confidence interval?\n\nRange of values likely to contain the true population parameter\nLevel of confidence (usually 95%) indicates reliability\nWider intervals = less precise estimates\n\n\n\n\n\n\n\n\nFormula for 95% CI\n\n\n\\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#visualising-confidence-intervals",
    "href": "lectures/L01/Lecture-01b.html#visualising-confidence-intervals",
    "title": "Lecture 01b – Revision",
    "section": "Visualising confidence intervals",
    "text": "Visualising confidence intervals\n\n\nCode\n# Generate sample data\nset.seed(253)\nsample_data &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  value = c(\n    rnorm(30, 100, 15),\n    rnorm(30, 110, 15),\n    rnorm(30, 105, 15)\n  )\n)\n\n# Calculate means and CIs\nci_data &lt;- sample_data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    mean = mean(value),\n    se = sd(value) / sqrt(n()),\n    ci_lower = mean - qt(0.975, n() - 1) * se,\n    ci_upper = mean + qt(0.975, n() - 1) * se\n  )\n\n# Plot\nggplot(ci_data, aes(x = group, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  ggtitle(\"Means with 95% Confidence Intervals\")\n\n\n\nWe will learn more about confidence intervals in the next lecture."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#key-concepts-we-covered",
    "href": "lectures/L02/Lecture-02a.html#key-concepts-we-covered",
    "title": "Lecture 02a – Sampling designs",
    "section": "Key concepts we covered",
    "text": "Key concepts we covered\n\nPopulation vs. sample\n\nPopulation: The complete set of all items we’re studying\nSample: A subset of the population we actually measure\n\nParameters (population) and statistics (sample)\n\nCentral tendency: how data clusters around a middle value\n\nmean (average), median (middle value), mode (most common)\n\nSpread/dispersion: how data points vary from each other\n\nvariance (average squared deviation), standard deviation (square root of variance)\n\n\nConfidence intervals – we’ll explore this further today!"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#overview",
    "href": "lectures/L02/Lecture-02a.html#overview",
    "title": "Lecture 02a – Sampling designs",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\nAspect\nObservational study\nControlled experiment\n\n\n\n\nControl\nNo control over the variables of interest: mensurative and absolute\nControl over the variables of interest: comparative and manipulative\n\n\nCausation\nCannot establish causation, but perhaps association\nCan establish causation\n\n\nFeasibility\nCan be done in many cases\nMay be destructive and thus cannot always be done\n\n\nExamples\nSurveys, monitoring studies, correlational studies, case-control studies, cohort studies\nClinical trials, A/B testing, laboratory experiments, field experiments\n\n\nStatistical Tests\nCorrelation, regression, chi-squared tests, t-tests, one-way ANOVA, time series analysis\nT-tests, one-way ANOVA, factorial ANOVA, regression\n\n\n\n\n\n\n\nWe will focus on the fundamentals behind observational studies this week.\n\n\n\n\n\n\nTip\n\n\nMensurative studies involve measuring without manipulating variables.\nAbsolute studies measure actual values rather than comparing between treatments."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#observational-studies-two-common-types",
    "href": "lectures/L02/Lecture-02a.html#observational-studies-two-common-types",
    "title": "Lecture 02a – Sampling designs",
    "section": "Observational studies: two common types",
    "text": "Observational studies: two common types\n\nSurveys\n\nEstimate a statistic (e.g. mean, variance), but\nno temporal change during estimate.\nE.g. measuring species richness in a forest.\nThink of it as a snapshot at one point in time.\n\n\n\nMonitoring studies\n\nEstimate a change in statistic (same as above), and\ntemporal change across observations, i.e. before and after.\nE.g. measuring species richness in a forest before and after a fire.\nThink of it as taking multiple snapshots over time to see changes."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#sampling-designs",
    "href": "lectures/L02/Lecture-02a.html#sampling-designs",
    "title": "Lecture 02a – Sampling designs",
    "section": "Sampling designs",
    "text": "Sampling designs\n\nSimple random sampling:\n\nEach unit has an equal chance of being selected.\nRandomly sample units from the entire population.\nLike putting all names in a hat and drawing some out randomly.\n\n\n\n\n\n\n\n\nStratified random sampling\n\nThe population is first divided into strata (groups with similar characteristics).\nRandomly sample units within each strata by simple random sampling.\nLike separating students by year level, then randomly selecting some from each year."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#what-is-random-sampling",
    "href": "lectures/L02/Lecture-02a.html#what-is-random-sampling",
    "title": "Lecture 02a – Sampling designs",
    "section": "What is “random” sampling?",
    "text": "What is “random” sampling?\n\nRandom selection of finite or infinite population units.\n\n\n\nWhat does random mean?\n\n\n\nWithin a population, all units have a &gt; 0 probability of being selected i.e. everything has a chance to be selected.\n\n\nThis chance is called the inclusion probability (\\pi_i):\n\n\\pi_i is equal within a population unit – i.e. all units have the same chance of being selected.\n\\pi_i not necessarily equal between different population units – i.e. units from different groups may have different chances of being selected - more on this later.\n\n\n\n\n\nHow do we perform random sampling in real life?\n\nRandom number generator (RNG) – e.g. R’s sample() function.\nRandom number table – e.g. Random number table by the National Institute of Standards and Technology (NIST).\nThink of it like rolling dice or drawing names from a hat, but using mathematics to ensure true randomness."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#we-know-that",
    "href": "lectures/L02/Lecture-02a.html#we-know-that",
    "title": "Lecture 02a – Sampling designs",
    "section": "We know that…",
    "text": "We know that…\nFrom the previous lecture\n\nSample mean is a good measure of central tendency (the “middle” of our data).\nSample variance is a good measure of dispersion (how spread out our data is).\nSample size affects the precision of the sample mean (more samples = more precise).\n\n\nCan we combine all of the above in a single statistic?\nYes! That’s where confidence intervals come in."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#combining-an-estimate-with-its-precision",
    "href": "lectures/L02/Lecture-02a.html#combining-an-estimate-with-its-precision",
    "title": "Lecture 02a – Sampling designs",
    "section": "Combining an estimate with its precision",
    "text": "Combining an estimate with its precision\nA confidence interval (CI) is:\n\nA range of values that likely contains the true population value\nLike saying “We’re 95% confident that out of 100 samples, 95 of them will be within this range”\nCrucial for hypothesis testing and estimation, the basis of statistical inference\n\nYou will often see CIs in scientific papers, reports, and news articles."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#calculating-confidence-intervals",
    "href": "lectures/L02/Lecture-02a.html#calculating-confidence-intervals",
    "title": "Lecture 02a – Sampling designs",
    "section": "Calculating confidence intervals",
    "text": "Calculating confidence intervals\nWhat we need\n\nEstimate of the population parameter, i.e. the sample mean (\\bar{x}) - our best guess of the true value\nCritical value (t_{n-1}) - a number that represents our chosen confidence level (like 95%)\nStandard error of the estimate, SE of the mean (SE_{\\bar{x}}) - tells us how precise our mean is\n\n\n\n\n\n\n\nNote\n\n\nThink of standard error as “how much our sample means would vary if we took many different samples and calculated the mean each time”\n\n\n\nAll these (mean, standard error and critical value) are combined to form the confidence interval."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#breakdown",
    "href": "lectures/L02/Lecture-02a.html#breakdown",
    "title": "Lecture 02a – Sampling designs",
    "section": "Breakdown",
    "text": "Breakdown\nIn general, a CI has the form: \\text{estimate} \\pm \\text{margin of error}\nwhere the margin of error is a function of the standard error of the estimate:\n\\text{estimate} \\pm (\\text{critical value} \\times \\text{standard error (estimate)})\nwhere the critical value is based on the sampling distribution of the estimate i.e. the t-distribution.\n\n\n\n\n\n\nTip\n\n\nThink of margin of error like the “plus or minus” value you often see in survey results (±3%)"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#formula-for-95-confidence-interval-ci",
    "href": "lectures/L02/Lecture-02a.html#formula-for-95-confidence-interval-ci",
    "title": "Lecture 02a – Sampling designs",
    "section": "Formula for 95% Confidence Interval (CI)",
    "text": "Formula for 95% Confidence Interval (CI)\n\n\\bar{x} \\pm \\left(t_{n-1} \\times \\frac{s}{\\sqrt{n}}\\right)\n\nStep-by-step calculation by hand\n\nCalculate the sample mean, \\bar{x} (add all values and divide by number of samples)\nCalculate the sample standard deviation, s (measure of spread in your data)\nDetermine the standard error of the mean, SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}} (how precise your mean estimate is)\nLook up the t-value, t_{n-1}, from the t-distribution table for the 95% confidence level and n-1 degrees of freedom.\n\nThis is a specific number based on how many samples you have\n\nCompute the margin of error: \\text{Margin of Error} = t_{n-1} \\times SE_{\\bar{x}}\nFinally, the 95% CI is: \\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})\n\nYou need to be able to calculate this by hand/calculator."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions",
    "href": "lectures/L02/Lecture-02a.html#more-definitions",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nDegrees of freedom (df)\nThe number of values in a sample that are free to vary while still maintaining the same statistic.\n\n\\text{df} = n - 1\n\nwhere n is the number of samples.\n\nExample\nImagine you have 4 numbers with a mean of 5:\n\nYou can choose the first three numbers freely: 3, 10, and 7\nBut the fourth number MUST be 0 to make the mean = 5\n\n(3 + 10 + 7 + 0) ÷ 4 = 5\n\nSo only 3 numbers (n-1) can be freely chosen = 3 degrees of freedom"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions-1",
    "href": "lectures/L02/Lecture-02a.html#more-definitions-1",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nt-critical value\nA number that helps determine how wide to make your confidence interval.\n\nBased on your confidence level (e.g. 95%) and sample size\nLarger t-values = wider intervals = more confidence but less precision\nSmaller sample sizes = larger t-values (because we’re less certain)"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions-2",
    "href": "lectures/L02/Lecture-02a.html#more-definitions-2",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nt-distribution\nA probability distribution that accounts for the uncertainty when estimating from small samples.\n\nSimilar to the normal “bell curve” distribution, but with heavier tails.\nWith few samples (small n), the t-distribution is wider, reflecting greater uncertainty.\nAs sample size increases, the t-distribution gets closer to the normal distribution.\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(gganimate)\n\n# Variable to control animation speed (higher values = slower transitions)\nanim_speed &lt;- 1\n\n# Create a sequence of x values for the plot\nx &lt;- seq(-4, 4, length.out = 400)\n\n# Degrees of freedom: 1 through 5 then even numbers from 6 to 30\ndfs &lt;- c(1:5, seq(6, 30, by = 2))\n\n# Prepare data for t-distribution curves with a new \"df\" column\nt_curves &lt;- do.call(rbind, lapply(dfs, function(df) {\n  data.frame(\n    x = x,\n    density = dt(x, df),\n    df = df\n  )\n}))\n\n# Create data for the standard normal distribution (static)\nnormal_curve &lt;- data.frame(\n  x = x,\n  density = dnorm(x)\n)\n\n# Plot with gganimate: animate t-distribution curves (each frame shows one df)\np &lt;- ggplot() +\n  # Static normal distribution curve\n  geom_line(\n    data = normal_curve, aes(x = x, y = density),\n    color = \"black\", linetype = \"dashed\", size = 1\n  ) +\n  # t-distribution curve that will animate\n  geom_line(\n    data = t_curves, aes(x = x, y = density, color = factor(df)),\n    size = 1\n  ) +\n  labs(\n    title = \"Degrees of Freedom: {closest_state}\",\n    x = \"x\",\n    y = \"Density\",\n    subtitle = \"Dashed line = Normal distribution; Solid line = t-distribution\"\n  ) +\n  theme(legend.position = \"none\") +\n  transition_states(states = df, transition_length = anim_speed, state_length = anim_speed)\n\np\n\n\n\n\n\n\n\n\n\nNote\n\n\nNotice how the t-distribution (solid line) gets closer to the normal distribution (dashed line) as the degrees of freedom increase!"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#interpreting-confidence-intervals",
    "href": "lectures/L02/Lecture-02a.html#interpreting-confidence-intervals",
    "title": "Lecture 02a – Sampling designs",
    "section": "Interpreting confidence intervals",
    "text": "Interpreting confidence intervals\n\nConfidence intervals depend on a specified confidence level (e.g. 95%, 99%) with higher confidence levels producing wider intervals (i.e. more conservative).\nAnother way to think of it: a range of values that we are fairly sure contains the true value of the population parameter.\n\n\nFishing analogy\nA confidence interval is like a fishing net:\n\nA wider net (interval) is more likely to catch the fish (true value)\nA spear (single point estimate) is less likely to catch the fish\nThe net width represents our uncertainty about the true value\n\n\n\n\n\n\n\nImportant\n\n\nCommon misunderstanding: A 95% CI does NOT mean there’s a 95% chance the true value is inside the interval. It means if you took 100 different samples and made 100 different CIs, about 95 of them would contain the true value."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#soil-carbon",
    "href": "lectures/L02/Lecture-02a.html#soil-carbon",
    "title": "Lecture 02a – Sampling designs",
    "section": "Soil carbon",
    "text": "Soil carbon\n\nSoil carbon content was measured at 7 locations across the area. The amount at each location was 48, 56, 90, 78, 86, 71, 42 tonnes per hectare (t/ha).\n\n\nCode\nsoil &lt;- c(48, 56, 90, 78, 86, 71, 42)\nsoil\n\n\n[1] 48 56 90 78 86 71 42\n\n\nWhat is the mean soil carbon content and how confident are we in this estimate? How this is calculated depends on whether we used simple random sampling or stratified random sampling."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#mean-and-95-ci",
    "href": "lectures/L02/Lecture-02a.html#mean-and-95-ci",
    "title": "Lecture 02a – Sampling designs",
    "section": "Mean and 95% CI",
    "text": "Mean and 95% CI\nStep-by-step calculation\n\n\nMean: \\bar{x} = \\frac{48+56+90+78+86+71+42}{7} \\approx 67.3 t/ha\nStandard deviation: s \\approx 18.84 t/ha\n\nThis tells us how much individual measurements vary from the mean\n\nStandard error: SE = \\frac{s}{\\sqrt{7}} \\approx 7.12 t/ha\n\nThis tells us how precise our estimate of the mean is\n\nt-value (95% CI, df = 6): t_{0.975,6} \\approx 2.447\n\nThis is the critical value for 95% confidence with 6 degrees of freedom\n\nMargin of error: t_{0.975,6} \\times SE \\approx 17.43 t/ha\n\nThis is the “plus or minus” value for our interval\n\nWhich gives: (67.3 - 17.43, 67.3 + 17.43) = (49.87, 84.73) t/ha\n\n\n\nAnd so we report the mean soil carbon content as 67.3 t/ha with a 95% CI of (49.87, 84.73) t/ha or 67.3 ± 17.43 t/ha.\nNote: Our confidence interval is quite wide relative to our mean (about ±26% of the mean value), suggesting moderate uncertainty in our estimate."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#implementation-in-r",
    "href": "lectures/L02/Lecture-02a.html#implementation-in-r",
    "title": "Lecture 02a – Sampling designs",
    "section": "Implementation in R",
    "text": "Implementation in R\nManual calculation\n\n# Step 1: Calculate the sample mean of soil carbon content\nmean_soil &lt;- mean(soil)\n\n# Step 2: Calculate the sample standard deviation of soil carbon content\nsd_soil &lt;- sd(soil)\n\n# Step 3: Calculate the standard error of the mean (SE)\nse_soil &lt;- sd_soil / sqrt(length(soil))\n\n# Step 4: Calculate the t-critical value for a 95% confidence interval\nt_crit &lt;- qt(0.975, df = length(soil) - 1)\n\n# Step 5: Calculate the margin of error (t_crit * SE)\n# and then determine the lower and upper bounds of the confidence interval\nci &lt;- mean_soil + c(-1, 1) * (t_crit * se_soil)\n\n# Step 6: View the calculated 95% confidence interval\nci\n\n[1] 49.84627 84.72516\n\n\nThere are ways to calculate this in R quickly, but it is important to understand the manual calculation.\n\n\n\n\n\n\nUnderstanding the R code\n\n\n\nmean() calculates the average value\nsd() calculates the standard deviation\nqt(0.975, df=6) gives the t-critical value (we use 0.975 because we want 95% in the middle, with 2.5% in each tail)\nc(-1, 1) creates a vector to subtract and add the margin of error"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#questions",
    "href": "lectures/L02/Lecture-02a.html#questions",
    "title": "Lecture 02a – Sampling designs",
    "section": "Questions",
    "text": "Questions\n\nHow precise is our estimate?\n\nOur margin of error is about 17.43 t/ha, which is roughly 26% of our mean value.\n\nHow big a change must there be to estimate a statistically significant change?\n\nChanges larger than about 17.43 t/ha would likely be statistically significant.\n\nCan we sample more efficiently?\n\nYes! This is where stratified sampling becomes valuable.\n\n\nTo answer these questions, we need to compare simple random sampling with a hypothetical stratified random sampling design (i.e. what if we had considered stratification before sampling?)"
  },
  {
    "objectID": "lectures/L02/index.html",
    "href": "lectures/L02/index.html",
    "title": "Lecture 02",
    "section": "",
    "text": "Lecture 02a – Sampling designs Full Screen | PDF\n\nLecture 02a – Sampling designs II Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1: Designed studies**",
      "L02 -- Sampling designs"
    ]
  },
  {
    "objectID": "lectures/L03/index.html",
    "href": "lectures/L03/index.html",
    "title": "Lecture 03",
    "section": "",
    "text": "Lecture 03a – t-tests Full Screen | PDF\nLecture 03b – One-way ANOVA Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1: Designed studies**",
      "L03 -- 1-way ANOVA"
    ]
  },
  {
    "objectID": "lectures/L05/index.html",
    "href": "lectures/L05/index.html",
    "title": "Lecture 05",
    "section": "",
    "text": "Important\n\n\n\nLecture 05 is not available in Quarto. Please refer to the Canvas site to access the lecture material."
  },
  {
    "objectID": "lectures/L07/index.html",
    "href": "lectures/L07/index.html",
    "title": "Lecture 07",
    "section": "",
    "text": "Lecture 07 – Regression modelling Full screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2: Finding patterns**",
      "L07 -- Regression modelling"
    ]
  },
  {
    "objectID": "lectures/L09/index.html",
    "href": "lectures/L09/index.html",
    "title": "Lecture 09",
    "section": "",
    "text": "Lecture 09 – Predictive modelling Full screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2: Finding patterns**",
      "L09 -- Model assessment"
    ]
  }
]