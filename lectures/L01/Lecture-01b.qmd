---
title: Lecture 01b -- Revision
author: Januar Harianto
# format: soles-revealjs
format: revealjs
fontsize: 16pt
# embed-resources: true
execute:
  cache: true
---

```{r}
#| label: setup
#| include: false
#| warning: false
#| message: false

# Load required packages
pacman::p_load(tidyverse, patchwork, cowplot)

# Set random seed for reproducibility
set.seed(909)

# Set default ggplot theme
theme_set(theme_cowplot())
```

# Refresher

## Assumed knowledge

1. Samples, populations and statistical inference
2. Probability distributions
3. Parameter estimation
   - central tendency
   - spread or variability
4. Sampling distribution of the mean
    - Standard error
    - Confidence intervals
    - Central Limit Theorem
5. Controlled experiments vs observational studies

## Why is this important?

- Understanding sampling **informs experimental design (Week 4 onward)**. *How many samples do we need and are our samples representative?*
- Recognising sample (*not sampling*) distributions helps us choose the right statistical model -- e.g. *t*-test to compare two means that are normally distributed.
- Most statistical techniques use sample statistics for interpretation, e.g. the *t*-test can be explained using **confidence intervals**, and the ANOVA test can be interpreted in part using **means** and **standard errors**.

All of these concepts will make more sense as we go through the course, but if you do not try to understand them now, you **will** struggle.

# Samples, populations and statistical inference

## Populations and samples

### Populations
- **All** the possible units and their associated observations of interest
- Scientists are often interested in making *inferences* about populations, but measuring every unit is impractical

### Samples
- A collection of observations from any population is a sample, and the number of observations in it is the **sample size**
- We assume samples that we collect can be used to make inferences about the population
- **NEW**: Samples need to be *representative* of the population



## Statistics vs parameters

- Characteristics of the **population** are called *parameters* (e.g. population mean or population regression slope)
- Characteristics of the **sample** are called *statistics* (e.g. sample mean or sample regression slope) -- they are used to estimate the population parameters
- Statistics are what we use to help us understand the population
- Formal statistical methods can help us make inferences about the population based on the sample -- statistical inference
- *Not all statistical techniques are inferential, but many are*



## Sample data

Sample data are usually collected as **variables**, which are the characteristics we measure or record from each object.

::: fragment

Variables can be:

### Categorical Variables
- **Nominal**: categories without a natural order (e.g. colors, names)
- **Ordinal**: categories with a natural order (e.g. ratings, rankings)

### Numerical Variables
- **Continuous**: can take any value within a range (e.g. height, weight)
- **Discrete**: can take only specific values (e.g. counts, presence/absence)
:::

## YOU decide on what a variable represents

A numerical, continuous variable can be treated as a categorical variable if you decide to categorise it.

### Examples
::: incremental
- **height (in cm)** -- a numerical, **continuous** variable, can be treated as a categorical variable if you group it into categories (short, medium, tall)
- **age (in years)** -- a numerical, **discrete** variable, can be treated as a continuous variable (if we allow for certain *issues*)
- **treatment (A, B, C)** -- a categorical variable, can be treated as a numerical variable if we assign numbers to the treatments (1, 2, 3) and assume they are **ordered** e.g. effect of 1 < 2 < 3 -- *the basis of non-parametric tests*
:::


# Distribution of data
## Types of probability distributions
Populations can be described by probability distributions, and by now, you should be familiar with these distributions and their properties

::: incremental
- **Normal Distribution**: Bell-shaped curve, symmetric around the mean. **Data is continuous**
- **Binomial Distribution**: Models success/failure outcomes in a fixed number of trials. **Data is discrete**
- **Poisson Distribution**: Models count data when events occur at a constant rate. **Data is discrete**
:::
::: fragment
Knowing the distribution of your data is important for choosing the right statistical model -- although it is not always necessary.
:::

## 
```{r}
library(tidyverse)

set.seed(908)
normal_data <- data.frame(x = rnorm(10000, mean = 0, sd = 1))
binomial_data <- data.frame(x = rbinom(10000, size = 10, prob = 0.5))
poisson_data <- data.frame(x = rpois(500, lambda = 3))

# normal
p1 <- ggplot(normal_data, aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
  geom_density(color = "red") +
  ggtitle("Normal Distribution")

# binomial
p2 <- ggplot(binomial_data, aes(x = x)) +
  geom_bar(fill = "lightgreen", color = "black") +
  ggtitle("Binomial Distribution")

# poisson
p3 <- ggplot(poisson_data, aes(x = x)) +
  geom_bar(fill = "lightpink", color = "black") +
  ggtitle("Poisson Distribution")

# Arrange plots
library(patchwork)
p1 | p2 / p3
```

# Parameter estimation

## Measures of central tendency


### Mean $\bar{x}$
- The arithmetic average of all values in a dataset
- Sum of all values divided by number of observations
- Sensitive to extreme values (outliers)

::: fragment
::: callout-warning
## Formula
$$\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$$

- where $n$ is the number of observations
- $x_i$ represents each individual value
- $\sum$ means we add up all values from $i=1$ to $n$
- Example: for data {2,4,6,8}, $n=4$ and $\bar{x} = \frac{2+4+6+8}{4} = 5$
:::
:::

## 


### Median
- Middle value when data is ordered
- 50th percentile of the data
- More robust to outliers than mean
- For even n, average of two middle values


::: fragment
### Mode
- Most frequently occurring value
- Can have multiple modes
- Only measure of central tendency for categorical data
- Not always meaningful for continuous data
:::

## How they compare

```{r}
#| label: central-tendency
#| echo: false
#| warning: false

# Create data with different shapes
set.seed(123)
normal_data <- data.frame(x = rnorm(1000, mean = 0, sd = 1))
skewed_data <- data.frame(x = exp(rnorm(1000, mean = 0, sd = 0.5)))

# Normal distribution plot
p1 <- ggplot(normal_data, aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", alpha = 0.7) +
  geom_density(color = "darkblue") +
  geom_vline(xintercept = mean(normal_data$x), color = "red", linetype = "dashed") +
  geom_vline(xintercept = median(normal_data$x), color = "green", linetype = "dashed") +
  ggtitle("Symmetric Distribution") +
  annotate("text", x = mean(normal_data$x), y = 0.1, 
           label = "Mean ≈ Median", angle = 90, vjust = -0.5)

# Skewed distribution plot
p2 <- ggplot(skewed_data, aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "lightgreen", alpha = 0.7) +
  geom_density(color = "darkgreen") +
  geom_vline(xintercept = mean(skewed_data$x), color = "red", linetype = "dashed") +
  geom_vline(xintercept = median(skewed_data$x), color = "green", linetype = "dashed") +
  ggtitle("Skewed Distribution") +
  annotate("text", x = mean(skewed_data$x), y = 0.2, 
           label = "Mean", angle = 90, vjust = -0.5) +
  annotate("text", x = median(skewed_data$x), y = 0.2, 
           label = "Median", angle = 90, vjust = -0.5)

# Arrange plots side by side
p1 / p2
```

Depending on the distribution of the data, the mean and median can be different, and this can tell you something about the data.



## Measures of dispersion

### Variance $s^2$
- Measures how spread out the data is from the mean
- Calculated as average squared deviations from the mean
- Squared units make it harder to interpret
- Sensitive to outliers (squares large deviations)

## Measures of dispersion

::: callout-warning
## Formula
$$s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$$

- where $n$ is the number of observations
- $x_i$ represents each individual value
- $\bar{x}$ is the sample mean
- For data {2,4,6,8}: 
  - mean = 5 
  - differences = (-3,-1,1,3), squares = (9,1,1,9), sum = 20
  - Therefore, $s^2 = \frac{20}{3} \approx 6.67$
:::

## Measures of dispersion

### Standard Deviation $s$
- Square root of variance
- Same units as original data
- More interpretable than variance
- Empirical rule for normal distributions:
  - ≈68% of data within ±1 SD
  - ≈95% of data within ±2 SD
  - ≈99.7% of data within ±3 SD

::: fragment
::: callout-warning
## Formula
$$s = \sqrt{s^2}$$

- Simply the square root of variance
- For our example data {2,4,6,8}:
  - $s = \sqrt{6.67} \approx 2.58$
- Interpretation: On average, values deviate about 2.58 units from the mean
:::
:::

## Visualising standard deviation
```{r}
#| label: dispersion
#| echo: false
#| warning: false


# Create datasets with different dispersion
set.seed(456)
narrow_data <- data.frame(x = rnorm(1000, mean = 0, sd = 1))
wide_data <- data.frame(x = rnorm(1000, mean = 0, sd = 2))

# Function to add SD bands using colorblind-friendly colors
add_sd_bands <- function(p, data, base_color) {
  mean_val <- mean(data$x)
  sd_val <- sd(data$x)
  p + 
    geom_vline(xintercept = mean_val, color = "#E69F00", linetype = "dashed") +
    annotate("rect", xmin = mean_val - sd_val, xmax = mean_val + sd_val,
             ymin = -Inf, ymax = Inf, fill = base_color, alpha = 0.3) +
    annotate("rect", xmin = mean_val - 2*sd_val, xmax = mean_val + 2*sd_val,
             ymin = -Inf, ymax = Inf, fill = base_color, alpha = 0.15) +
    annotate("text", x = mean_val, y = Inf, label = "±1 SD", vjust = 2) +
    annotate("text", x = mean_val + 2*sd_val, y = Inf, label = "±2 SD", vjust = 2)
}

# Create base plots with fixed x-axis limits
p1 <- ggplot(narrow_data, aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, 
                fill = "#56B4E9", color = "black", alpha = 0.7) +
  geom_density(color = "#0072B2") +
  ggtitle("Smaller dispersion (SD = 1)") +
  xlim(-6, 6) 

p2 <- ggplot(wide_data, aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, 
                fill = "#009E73", color = "black", alpha = 0.7) +
  geom_density(color = "#005C44") +
  ggtitle("Larger dispersion (SD = 2)") +
  xlim(-6, 6)

# Add SD bands
p1 <- add_sd_bands(p1, narrow_data, "#56B4E9")
p2 <- add_sd_bands(p2, wide_data, "#009E73")

# Arrange plots
p1 / p2
```


## Population parameters vs sample statistics

For those of you interested:

### Mean
| Population Parameter | Sample Statistic |
|---------------------|------------------|
| $\mu = \frac{1}{N}\sum_{i=1}^N x_i$ | $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ |

### Variance
| Population Parameter | Sample Statistic |
|---------------------|------------------|
| $\sigma^2 = \frac{1}{N}\sum_{i=1}^N (x_i - \mu)^2$ | $s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$ |

### Standard Deviation
| Population Parameter | Sample Statistic |
|---------------------|------------------|
| $\sigma = \sqrt{\sigma^2}$ | $s = \sqrt{s^2}$ |

::: {.callout-note}
Notice the use of $n-1$ in sample variance and standard deviation
:::

## Why n-1?
- When calculating sample variance, we use $n-1$ instead of $n$ in the denominator
- This is called "[Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)"
- Why? Because we lose one "degree of freedom" when we estimate the mean:
  1. If you know the sample mean ($\bar{x}$)
  2. And you know all but one value in your sample
  3. The last value is *constrained* - it must make the mean equal $\bar{x}$

# Sampling distributions and CLT

## What is a sampling distribution?
- Distribution of a statistic (e.g., mean) calculated from repeated samples
- Shows how sample statistics vary from sample to sample
- Important for understanding sampling variability and making inferences

## Sampling distribution of the mean

```{r}
#| echo: false
#| warning: false
set.seed(123)

# Create population
population <- rnorm(10000, mean = 100, sd = 15)

# Function to take samples and calculate means
sample_means <- replicate(1000, {
  sample_mean <- mean(sample(population, size = 30))
})

# Plot population and sampling distribution
pop_plot <- ggplot() +
  geom_histogram(aes(x = population, y = ..density..), bins = 50,
                fill = "lightblue", color = "black", alpha = 0.7) +
  geom_density(aes(x = population), color = "blue") +
  ggtitle("Population Distribution") +
  theme_minimal()

samp_plot <- ggplot() +
  geom_histogram(aes(x = sample_means, y = ..density..), bins = 30,
                fill = "lightgreen", color = "black", alpha = 0.7) +
  geom_density(aes(x = sample_means), color = "darkgreen") +
  ggtitle("Sampling Distribution of Mean (n=30)") +
  theme_minimal()

pop_plot / samp_plot
```

## Central Limit Theorem

> I know of scarcely anything so apt to impress the imagination as the wonderful form of **cosmic order** expressed by the Central Limit Theorem. The law would have been personified by the Greeks and deified, if they had known of it.”

– Sir Francis Galton, 1889, Natural Inheritance


The Central Limit Theorem (CLT) states that for sufficiently large samples:

1. The sampling distribution of the mean follows a normal distribution
2. The mean of the sampling distribution equals the population mean
3. The standard deviation of the sampling distribution (standard error) = $\frac{\sigma}{\sqrt{n}}$


## CLT in action

```{r}
#| echo: false
#| warning: false

# Create a skewed population
set.seed(456)
skewed_pop <- exp(rnorm(10000, mean = 0, sd = 0.5))

# Sample means for different sample sizes (ordered small to large)
sample_sizes <- c(5, 30, 100)
sample_labels <- factor(paste("n =", sample_sizes), 
                       levels = paste("n =", sample_sizes))  # preserve order
sample_dist_data <- lapply(sample_sizes, function(n) {
  means <- replicate(1000, mean(sample(skewed_pop, size = n)))
  data.frame(means = means, size = factor(paste("n =", n), levels = levels(sample_labels)))
})
sample_dist_df <- do.call(rbind, sample_dist_data)

# Plot
ggplot() +
  geom_histogram(aes(x = means, y = ..density..), data = sample_dist_df,
                bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_density(aes(x = means), data = sample_dist_df, color = "blue") +
  facet_wrap(~size) +
  ggtitle("Sampling distributions for different sample sizes") +
  theme_minimal()
```


## CLT in action

```{r}
#| echo: false
#| warning: false

# Create a skewed binomial population
set.seed(456)
binom_pop <- rbinom(10000, size = 10, prob = 0.1)  # skewed binomial

# Sample means for different sample sizes
sample_sizes <- c(5, 30, 100)
sample_dist_data <- lapply(sample_sizes, function(n) {
  means <- replicate(1000, mean(sample(binom_pop, size = n)))
  data.frame(means = means, size = paste("n =", n))
})
sample_dist_df <- do.call(rbind, sample_dist_data)

# Plot
ggplot() +
  geom_histogram(aes(x = means, y = ..density..), data = sample_dist_df,
                bins = 30, fill = "lightpink", color = "black", alpha = 0.7) +
  geom_density(aes(x = means), data = sample_dist_df, color = "red") +
  facet_wrap(~size) +
  ggtitle("Sampling distributions for different sample sizes (Binomial)") +
  theme_minimal()
```

## CLT and statistics

Because of the CLT, we understand that:

- A sampling distribution of the mean *will* be normally distributed for sufficiently large samples -- how large is "sufficient" depends on the population distribution
- The mean of the sampling distribution will be the population mean
- To determine how well the sample mean estimates the population mean, we use the standard error of the mean -- basically a standard deviation of the sampling distribution

# Standard error and confidence intervals

## Standard Error of the Mean
- Measures the precision of a sample mean
- Describes variation in sample means around the true population mean
- Decreases as sample size increases

::: fragment
::: callout-warning
## Formula
$$SE_{\bar{x}} = \frac{s}{\sqrt{n}}$$

- where $s$ is the sample standard deviation
- $n$ is the sample size
- Note: As $n$ increases, $SE$ decreases
:::
:::

## Confidence intervals

### What is a confidence interval?
- Range of values likely to contain the true population parameter
- Level of confidence (usually 95%) indicates reliability
- Wider intervals = less precise estimates

::: fragment
::: callout-warning
## Formula for 95% CI
$$\bar{x} \pm (t_{n-1} \times SE_{\bar{x}})$$

- where $t_{n-1}$ is the t-value for $n-1$ degrees of freedom
- For large samples ($n > 30$), use 1.96 instead of t-value
:::
:::

## Visualising confidence intervals

```{r}
#| warning: false

# Generate sample data
set.seed(253)
sample_data <- data.frame(
  group = rep(c("A", "B", "C"), each = 30),
  value = c(rnorm(30, 100, 15),
            rnorm(30, 110, 15),
            rnorm(30, 105, 15))
)

# Calculate means and CIs
ci_data <- sample_data %>%
  group_by(group) %>%
  summarise(
    mean = mean(value),
    se = sd(value)/sqrt(n()),
    ci_lower = mean - qt(0.975, n()-1)*se,
    ci_upper = mean + qt(0.975, n()-1)*se
  )

# Plot
ggplot(ci_data, aes(x = group, y = mean)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  ggtitle("Means with 95% Confidence Intervals") +
  theme_minimal()
```

# Controlled Experiments vs Observational Studies

## Key differences

| Aspect | Controlled Experiment | Observational Study |
|--------|---------------------|-------------------|
| Control | Researcher controls variables | No control over variables |
| Random Assignment | Yes | No |
| Causation | Can establish | Can only show association |
| Natural Setting | Less natural | More natural |
| Internal Validity | Higher | Lower |
| External Validity | Lower | Higher |

## Design Characteristics

### Controlled Experiments
- Researcher manipulates independent variable(s)
- Random assignment to treatment groups
- Control group often used
- Confounding variables minimized

### Observational Studies
- No manipulation of variables
- Study subjects in natural settings
- May be prospective or retrospective
- Need to account for confounding variables

## Example Study Types {.smaller}

::: {.incremental}
### Controlled Experiment
- Testing a new fertilizer's effect on plant growth
  - Random assignment to treatments
  - Control environmental conditions
  - Can conclude causation

### Observational Study
- Studying relationship between diet and heart disease
  - Cannot assign diets randomly
  - Natural variation in habits
  - Can only show association
:::

# Thanks for listening! Questions?
This presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by]
