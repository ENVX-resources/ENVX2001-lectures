{
  "hash": "e5e337017940f8e9587f9a2dc9072d1d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression: modelling\"\nauthor: Si Yang Han\nformat: soles-revealjs\n# format: revealjs\n# fontsize: 16pt\nembed-resources: true\nexecute:\n  cache: true\n  echo: true\n  warning: false\n  message: false\nscrolling: true\n---\n\n\n\n\n\n# Welcome to regression modelling!\n\n## About me\n\n::: {layout-ncol=2}\n- Research topics: spatial modelling and mapping, precision agriculture, winter grains\n- Timeline at USYD\n  - BSc (Hons) in Agricultural Science\n  - PhD in Digital Agriculture\n  - Postdoc in Spatial Modelling\n  - Associate Lecturer in Agricultural Data Science\n\n![Faba beans at Trangie](images/hello_world.jpeg)\n:::\n\n\n## Learning Outcomes {auto-animate=true}\n\nLO1. demonstrate proficiency in designing sample schemes and analysing data from them using using R\n\nLO2. describe and identify the basic features of an experimental design; replicate, treatment structure and blocking structure\n\n**LO3. demonstrate proficiency in the use or the statistical programming language R to** an ANOVA and **fit regression models to experimental data**\n\nLO4. demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data\n\n**LO5. interpret the output and understand conceptually how its derived of a regression**, ANOVA and multivariate analysis that have been calculated by R\n\nLO6. write statistical and modelling results as part of a scientific report\n\nLO7. appraise the validity of statistical analyses used publications.\n\n<!---\ntherefore, want students how to fit a linear regression model and be able to interpret \n- Want to make sure we are using it for the right thing (linear relationship)\n- want to make sure we are fitting it properly\n- Want to then be able to interpret the output\n--->\n\n## Refresher from ENVX1002\n\n- Regression modelling is for one *continuous numerical* response ($y$) and one or more *numerical* predictors ($x_1$, $x_2$, $x_n$)\n- Can be for linear or nonlinear relationships -- focus on linear in ENVX2001\n- To help us:\n    + Understand the relationship between variables\n    + Predict new values of $y$ based on $x$\n    + Test hypotheses about the relationship between variables\n- Fit a 'line of best fit' that minimises the sum of the squared residuals (least-squares)\n\n## Workflow {auto-animate=true}\n\n1. Model development\n    + Explore: visualise, summarise\n    + Model: fit, check assumptions, interpret -- (transform, repeat).\n    + Transform predictors\n  \n2. [Variable selection]{style=\"color: #D0D3D4;\"}\n    + [VIF: remove predictors with high variance inflation factor]{style=\"color: #D0D3D4;\"}\n    + [Model selection: stepwise selection, AIC, principle of parsimony, assumption checks]{style=\"color: #D0D3D4;\"}\n  \n3. [Predictive modelling]{style=\"color: #D0D3D4;\"}\n    + [Predict: Use the model to predict new data]{style=\"color: #D0D3D4;\"}\n    + [Validate: Evaluate the model’s performance]{style=\"color: #D0D3D4;\"}\n\n## Brief history\n\n![Adrien-Marie Legendre](images/legendre.jpg)\n![Carl Friedrich Gauss](images/gauss.jpg)\n![Francis Galton](images/galton.jpg)\n\nAdrien-Marie Legendre, Carl Friedrich Gauss, Francis Galton\n\n:::{.callout-note}\nMany other people contributed to the development of regression analysis, but these three are the most well-known.\n:::\n\n## Brief history {auto-animate=\"true\"}\n\n- **Method of least squares** first theorised by Adrien-Marie Legendre in 1805\n- **Technique of least squares** first used by Carl Friedrich Gauss in 1809 (to fit a parabola to the orbit of the asteroid Ceres)\n- **Model fitting** first published by Francis Galton in 1886 (predicting the height of a child from the height of the parents)\n\n# Simple linear regression\n\n> An example with Galton's data: parent and child heights.\n\n## Example: child vs parent height\n\n*Galton, F. (1886). Regression Towards Mediocrity in Hereditary Stature Journal of the Anthropological Institute, 15, 246-263*\n\n:::{.columns}\n:::{.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(HistData)\ndata(Galton)\nstr(Galton)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t928 obs. of  2 variables:\n $ parent: num  70.5 68.5 65.5 64.5 64 67.5 67.5 67.5 66.5 66.5 ...\n $ child : num  61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 ...\n```\n\n\n:::\n:::\n\n\n\n- 928 children of 205 pairs of parents\n- Average height of both parents and their child's height measured in inches\n- Size classes were binned (hence data looks discrete)\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) + \n  geom_smooth(method = \"lm\") +\n  labs(subtitle = paste(\"Correlation:\", round(cor(Galton$parent, Galton$child), 2)))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n## Defining a linear relationship {.nostretch}\n\n- Pearson correlation coefficient ($r$) measures the linear correlation between two variables (ranges from -1 to 1)\n- Useful for distinguishing *strength* (weak/moderate/strong) and *direction* (positive/negative) of the association\n- Does not distinguish different *patterns* -- i.e. is the relationship actually linear?\n\n$$ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(Galton$parent, Galton$child) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.46\n```\n\n\n:::\n:::\n\n\n\n## Anscombe's quartet\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nanscombe %>%\n  pivot_longer(everything(), cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"), names_pattern = \"(.)(.)\") %>%\n  ggplot(aes(x = x, y = y)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    facet_wrap(~set, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n*All of these data have a correlation coefficient of about 0.8* -- always visualise your data.\n\n## Simple linear regression model {auto-animate=\"true\"}\n\nWe want to predict a response $Y$ based on a predictor $x$ for $i$ number of observations: \n\n$$Y_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}$$\n\nwhere\n\n$$\\epsilon_i \\sim N(0, \\sigma^2)$$\n\n- $Y_i$, the *response*, is an observed value of the dependent variable.\n- $\\beta_0$, the *constant*, is the population intercept and is **fixed**.\n- $\\beta_1$ is the population *slope* parameter, and like $\\beta_0$, is also **fixed**.\n- $\\epsilon_i$ is the error associated with predictions of $y_i$, and unlike $\\beta_0$ or $\\beta_1$, it is *not fixed*.\n\nBecause $\\epsilon_i$ is the only part of the equation that is not fixed, we associate it with the **residual error** ($observed-predicted$). It would also cover other aspects of error (e.g. sampling error, parallax error) but these are hard to discern.\n\n## Fitting the model {auto-animate=\"true\"}\n\n- $\\color{royalblue}{\\hat{y}_i}$ is the predicted value of $y_i$:\n\n$$\\color{royalblue}{\\hat{y}_i} = \\beta_0 + \\beta_1 x_i$$\n\n- The *residual* is the difference between the observed value of the response and the predicted value:\n\n$$\\hat\\epsilon_i = y_i - \\color{royalblue}{\\hat{y}_i}$$\n\n- Therefore:\n\n$$\\hat\\epsilon_i = y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)}$$\n\n- We use the **method of least squares** and minimise the sum of the squared residuals (SS):\n\n$$\\sum_{i=1}^n \\hat\\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2$$\n\n## {auto-animate=\"true\"}\n\nFinding the minimum SS requires solving the following problem:\n\n$$\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2$$\n\nWe can find $\\beta_0$ and $\\beta_1$ **analytically**. We first find $\\beta_1$:\n\n$$ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} $$\nAnd then substitute $\\beta_1$ into the equation for $\\beta_0$:\n\n$$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$\n\n## Numerical fitting\n\nComputers use “random guesses” to find set of parameters that minimises objective function (SS) -- more computationally efficient and applies beyond linear regression.\n\n![[source](https://github.com/Enchufa2/ls-springs)](images/leastsquares.gif){fig-align=\"center\"}\n\n\n\n## Fitting a model in R is easy with `lm()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(child ~ parent, data = Galton)\n```\n:::\n\n\n\nThat's it -- the model has been fitted.\n\n**But** there is a process similar to HATPC (hypothesis, assumptions, test, p-value, conclusions).\n\n## Define the hypothesis\n\n$$H_0: \\beta_1=0$$\n\n$$H_1: \\beta_1 \\neq 0$$\n\nThe null model is a line with no slope (i.e. flat or horizontal) at the mean of the child height ($\\bar{y}$ = 68 inches).\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nnull_model <- Galton %>%\n  lm(child ~ 1, data = .) %>%\n  broom::augment(Galton)\nlin_model <- Galton %>%\n  lm(child ~ parent, data = .) %>%\n  broom::augment(Galton)\nmodels <- bind_rows(null_model, lin_model) %>%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 1\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 1\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n\n## Assumptions\n\nThe data **must** meet certain criteria, which we often call *assumptions*. They can be remembered using **LINE**:\n\n- **L**inearity. The relationship between $y$ and $x$ is linear.\n- **I**ndependence. The errors $\\epsilon$ are independent.\n- **N**ormal. The errors $\\epsilon$ are normally distributed.\n- **E**qual Variance. At each value of $x$, the variance of $y$ is the same i.e. homoskedasticity, or constant variance.\n\n:::{.callout-tip}\nAll but the independence assumption can be assessed using diagnostic plots. \n:::\n\n## Assumptions with base R `plot()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow= c(2, 2)) # plots combined into 2x2 grid\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n## Assumptions with `ggfortify` package and `autoplot()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggfortify)\nautoplot(fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n## Assumptions using `performance`\n\n(Also provides a guide on what to check for in the assumption plot)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(performance)\nperformance::check_model(fit) # check all assumptions\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n```\n:::\n\n\n\n## Assumption: Linearity\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\nPrior knowledge and visual inspection comes into play. Does the relationship look approximately linear?\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::{.column width=\"50%\"}\nThe linearity assumption can be checked again by looking at a plot of the residuals against $x$ (i.e. `parent` height).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit, check = \"linearity\")\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n:::\n::::\n\n- Where the green reference line is > 0, the model *underestimates*, and where it is < 0, it *overestimates*.\n- If the linearity assumption is **violated**, we should not be fitting a linear model -- transform or use a nonlinear model.\n\n## Assumption: Independence\n\nThis assumption is addressed during experimental design, but issues like correlation between errors and patterns occurring due to time are possible if:\n\n- Observations of the same subject are related i.e. [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n- Time-series data, if the same subjects are sampled i.e. [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)\n\n## Assumption: Normality\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\nFor a given value of $x$, the residuals should be normally distributed. In a scatterplot of $x$ and $y$, the points would appear evenly distributed (linear and no fanning).\n\n![](images/residual.jpg)\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit, check = c(\"normality\", \"qq\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n:::\n::::\n\n- [How to interpret a QQ plot](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot)\n- [QQ plot interpretation](https://math.illinois.edu/system/files/inline-files/Proj9AY1516-report2.pdf)\n\n## Assessing normality using residuals\n\n- **Light-tailed**: small variance in residuals, resulting in a narrow distribution\n- **Heavy-tailed**: many extreme positive and negative residuals, resulting in a wide distribution\n- **Left-skewed** (n shape): more data falls to the left of the mean\n- **Right-skewed** (u shape): more data falls to the right of the mean\n\n:::{.columns}\n:::{.column width=\"50%\"}\n\nHeavy-tailed, left-skewed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1028)\nx <- rnorm(100)\ny <- 2 + 5 * x + rchisq(100, df = 3) * -1\ndf <- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"qq\")))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::{.column width=\"50%\"}\nLight-tailed, right-skewed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1028)\nx <- rnorm(100)\ny <- 2 + 5 * x + rnbinom(100, 10, .5)\ndf <- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"qq\")))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Asumption: Equal variances\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit, check = c(\"homogeneity\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\nOutliers are not a strict assumption, but they will affect the model fit.\n\n## What is a standardised residual?\n\n- The standardised residual is the residual divided by the standard error of the residual (normalised).\n\n$$Standardised\\ residual = \\frac{Residual}{Standard\\ error\\ of\\ the\\ residual}$$\n\n- The *mean* of the residuals is 0 in linear regression\n- A standardised residual of 2 or above suggests the point is an outlier (far from the regression line)\n- Spread should be random i.e. no pattern (fanning, W), which indicates **equal variances**\n\n# Model Fit\n\nHow well does our fitted model represent the relationship between the variables?\n\n## ANOVA and linear regression {.scrollable}\n\nANOVA is a variation of linear regression -- both partition variance into sum of squares for residuals (variance explained) and sum of squares for error (variance not explained) aka **the components of the F-statistic**.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n### ANOVA Output\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(child ~ parent, data = Galton)\nanova(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: child\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nparent      1 1236.9 1236.93  246.84 < 2.2e-16 ***\nResiduals 926 4640.3    5.01                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n- `parent Sum Sq`: the variation that `parent` explains in the `child` variable\n- `Residuals Mean Sq`: variation (per degree of freedom) that the model does not explain\n- The `F-value` is the ratio, i.e. does `parent` explain enough variation in `child` to be considered significant?\n\n$$\\text{F-value} = \\frac{\\text{parent Sum Sq}}{\\text{Residuals Mean Sq}} = \\frac{1236.9}{5.01} = 246.84 $$\n\n:::\n::::\n\n### Regression Output\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfit <- lm(child ~ parent, data = Galton)\nsummary(fit)\n\n# F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n\n## ANOVA and linear regression \n\nANOVA is a variation of linear regression -- both partition variance into sum of squares for residuals (variance explained) and sum of squares for error (variance not explained) aka **the components of the F-statistic**.\n\n### ANOVA Output\n\nThe ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p < .001)\n\n### Regression Output\n\nWe fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R^2^ = 0.21, F(1, 926) = 246.84, p < .001). Within this model, the effect of parent is statistically significant and positive ($\\beta_1$ = 0.65, 95% CI [0.57, 0.73], t(926) = 15.71, p < .001).\n\n:::{.callout-note}\nFor simple linear regression, the significance of the predictor (i.e. `child`) is the same as the model significance.\n:::\n\n# Interpret output\n\n> Model fit and predictions\n\n## Model fit {auto-animate=\"true\"}\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,\tAdjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n$$\\widehat{child} = 23.9 + 0.65 \\cdot parent$$\n\nFor every unit change in parent (i.e. *1 inch*), we expect a 0.65 unit change in child.\n\nHow much variation is explained? R^2^ = 0.21 = 21%\n\n- **Multiple R^2^**: proportion of variance in the response variable explained by the model.\n- **Adjusted R^2^**: as above but adjusted for the number of predictors in the model.\n    - For multiple linear regression\n    - It only increases if the new term improves the model more than would be expected by chance\n    - *Always lower than multiple R^2^*\n\n:::\n::::\n\n## Making predictions\n\nWhat is the predicted child height for a parent height of 70 inches?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchild <- 23.9 + 0.65 * 70\nchild\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 69.4\n```\n\n\n:::\n:::\n\n\n\nWe use `predict()` to make predictions -- it takes in the `lm()` model, recreates the equation and applies it to new data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit, data.frame(parent = 70)) # using 70 as this is the value we want to sub in and predict\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n69.18187 \n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note}\nHow good is our prediction actually? What if we had more parents and children, would the equation still hold up? We cover this in Week 9.\n:::\n\n# Transformations\n\nWhat if assumptions are not met, or we want to improve the model?\n\n## What if assumptions are not met?\n\n### Violations of...\n\n- **Linearity** can cause systematically wrong predictions\n- **Homoskedasticity** makes it difficult to estimate \"true\" standard deviation of errors (i.e. noisy estimates)\n- **Normality** can compromise inferences and hypothesis testing\n\n## How do we solve these problems?\n\n- Use less restrictive (but more complicated) models, e.g. generalised linear models, non-parametric techniques (ENVX3002)\n- Perform variance corrections (complicated)\n- [**Transform the response variable ($y$)** to stabilise variance and correct normality]{style=\"color: seagreen\"}\n- [**Transform the predictor variable ($x$)** if issues still exist in the diagnostics]{style=\"color: seagreen\"}\n\n:::{.callout-note}\nWe can also perform transformations to improve the model fit, but **beware of overfitting** -- we want to make reasonable predictions, not fit the data!\n:::\n\n## Example: air quality\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\nDaily air quality measurements in New York, May to September 1973.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(airquality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n```\n\n\n:::\n:::\n\n\n\n:::\n:::{.column width=\"50%\"}\n\nWe start with one variable: is ozone concentration influenced by temperature?\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Assumption checks\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(Ozone ~ Temp, data = airquality)\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\nIs a simple linear model appropriate? *Depends on your threshold for what is acceptable.*\n\n## Backtransforming -- FYI\n\nA log transformation (natural or a base) is relatively easy to back-transform.\n\n$$\\widehat{log(Ozone)}=\\color{royalblue}{-1.8380 + 0.0675 \\times Temp}$$\n$$\\widehat{Ozone}=e^{-1.8380 + 0.0675 \\times Temp}=e^{-1.8380} \\times e^{0.0675 \\times Temp}$$\nBut given we are focused on a 1-unit change of `Temp`, $\\widehat{Ozone}$ changes by $e^{0.0675} = 1.07$ **times**.\n\nIf this had been a `sqrt()` transformation...\n\n$$\\widehat{\\sqrt{Ozone}}=-1.8380 + 0.0675 \\times Temp$$\n$$\\widehat{Ozone}=(-1.8380 + 0.0675 \\times Temp)^2 = 3.3782−(0.2481×Temp)+(0.0675×Temp)^2$$\n\n## Interpreting log transformations -- FYI\n\n:::{.fragment}\n- Log-linear: $Log(Y)=\\beta_0+\\beta_1x$\n  - An increase of $x$ by 1 unit corresponds to a $\\beta_1$ unit increase in $log(Y)$\n  - An increase of $x$ by 1 unit corresponds to approximately a $\\beta_1 \\times 100\\%$ increase in $Y$\n:::\n:::{.fragment}\n- Linear-log: $Y=\\beta_0+\\beta_1log(x)$\n  - An increase of $1\\%$ in $x$ corresponds to a $\\frac{\\beta_1}{100}$ increase in $Y$\n:::\n:::{.fragment}\n- Log-log: $Log(Y)=\\beta_0+\\beta_1log(x)$\n  - An increase of $1\\%$ in $x$ corresponds to a $\\beta_1\\%$ increase in $Y$\n:::\n\n## Percent change with $ln$ transformation -- FYI\n\nInterpreting as a percent change can be more meaningful - it can be done with any log transformation (substitute $e$ below for 10 or any other base), but the **quick approximation only works with natural log transformations**. \n\nIf $y$ has been transformed with a natural log (`log(y)`), for a one-unit increase in $x$ the **percent change in $y$** (not `log(y)`) is calculated with:\n\n$$\\Delta y \\% = 100 \\cdot (e^{\\beta_1}-1)$$\n\nIf $\\beta_1$ is small (i.e. $-0.25 < \\beta_1 < 0.25$), then: $e^{\\beta_1} \\approx 1 + \\beta_1$. So $\\Delta y \\% \\approx 100 \\cdot \\beta_1$.\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n|   β   |   Exact $(e^{\\beta} - 1)$%   |   Approximate $100 \\cdot \\beta$   |\n|-----:|-----------------:|------------------:|\n| -0.25 |                      -22.13  |                               -25 |\n| -0.1  |                       -9.52  |                               -10 |\n|  0.01 |                        1.01  |                                 1 |\n|  0.1  |                       10.52  |                                10 |\n|  0.25 |                       28.41  |                                25 |\n|  0.5  |                       64.87  |                                50 |\n|   2   |                      638.91  |                               200 |\n\n:::\n:::{.column width=\"50%\"}\n\n- **$y$ transformed**: a one-unit increase in $x$ is *approximately* a $\\beta_1$% change in $y$.\n- **$x$ transformed**: a 1% increase in $x$ is *approximately* a $0.01 \\cdot \\beta_1$ change in $y$.\n- **Both $x$ and $y$ transformed**: a 1% increase in x is *approximately* a $\\beta_1$% change in y.\n\n:::\n::::\n\n## Transforming Ozone\n\nLet's transform Ozone using the natural log (`log()`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_log <- lm(log(Ozone) ~ Temp, data = airquality)\n```\n:::\n\n\n\n::::{.columns}\n \n:::{.column width=\"50%\"}\n:::{.fragment}\n\n### Before\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (ppb)\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(subtitle = paste(\"Correlation:\", round(cor(airquality$Temp, airquality$Ozone), 2)))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n\n::: {.column width=\"50%\"}\n:::{.fragment}\n\n### After\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(airquality, aes(x = Temp, y = log(Ozone))) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"log(Ozone) (ppb)\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(subtitle = paste(\"Correlation:\", round(cor(airquality$Temp, log(airquality$Ozone)), 2)))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n::::\n\n## Assumption: Linearity\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n\n## Assumption: Normality\n\n::::{.columns}\n:::{.column width=\"50%\"}\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::{.column width=\"50%\"}\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n\n## Assumption: Equal variances\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n## Is transforming better?\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,\tAdjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.14469 -0.33095  0.02961  0.36507  1.49421 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.83797    0.45100  -4.075 8.53e-05 ***\nTemp         0.06750    0.00575  11.741  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5848 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5473,\tAdjusted R-squared:  0.5434 \nF-statistic: 137.8 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n:::\n::::\n\n--- \n\nThe transformed model model equation is: \n\n$$\\widehat{log(Ozone)}=\\color{royalblue}{-1.8380 + 0.0675 \\times Temp}$$\nA 1 degree (&deg;F) increase in temperature is associated with a:\n\n- 0.0675 increase in `log(Ozone)` concentration\n- $e^{0.0675} = 1.07$ *times* increase in `Ozone` concentration\n- Approximately a 6.75% increase in `Ozone` concentration\n\n##Multiple linear regression\n\n### The MLR model\n\n$$Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + \\epsilon$$\n\nwhere\n\n- a response variable ($Y$) which we wish to predict using predictor variables ($x_k$)\n- $\\beta_0$ is the y-intercept\n- $\\beta_k$ is the partial regression coefficient associated with the $k^{th}$ predictor variable\n- $\\epsilon$ is error and $\\epsilon \\sim N(0,\\ \\sigma^2)$\n\n## Can we use more predictors? {auto-animate=true}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npsych::pairs.panels(airquality)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n\nCan we improve the current model by adding *wind* and *solar radiation* as additional predictors?\n\n## Can we use more predictors? {auto-animate=true}\n\n### From:\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\epsilon_i$$\n\n### To:\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i$$\n\n## Can we use more predictors? {auto-animate=true}\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmulti_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n```\n:::\n\n\n\n## Assumptions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(multi_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n\nThere is one additional assumption for multiple linear regression. **Collinearity** is when two or more predictors are very highly correlated. If the predictors are basically identical, the model cannot distinguish how much variability each explains. (Correlations in previous slides look fine).\n\n## Hypothesis\n\nFor multiple linear regression, there are two hypothesis tests:\n\n- Individual predictors, where the significance of each predictor is tested via t-tests\n\n$$H_0: \\beta_k = 0$$\n$$H_1: \\beta_k \\neq 0$$\n\n- The overall model, which is tested with an F-test (to get F-stat). $H_0$ is an intercept-only model (i.e. the mean), so if at least one predictor is useful, the model is better than the intercept-only model.\n\n$$H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0$$\n$$H_1: \\text{At least one } \\beta_k \\neq 0$$\n\n\n## Model Fit {auto-animate=true}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(multi_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n<br>\n\nModel equation:\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n## Interpretation {auto-animate=true}\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n**Holding all other variables constant:**\n\n- A one degree (&deg;F) increase in `Temp` is associated with a 4.9% increase in `Ozone` concentration.\n- A one unit increase in `Solar.R` is associated with a 0.25% increase in `Ozone` concentration.\n- A one unit increase in `Wind` is associated with a 6.2% decrease in `Ozone` concentration.\n\nAutomating extracting the model equation into latex using `extract_eq()` from the package `equatiomatic`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequatiomatic::extract_eq(multi_fit, use_coefs = TRUE, coef_digits = 3) |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$$\n\\operatorname{\\widehat{log(Ozone)}} = -0.262 + 0.049(\\operatorname{Temp}) + 0.003(\\operatorname{Solar.R}) - 0.062(\\operatorname{Wind})\n$$\n```\n\n\n:::\n:::\n\n\n\n## Is MLR model better?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(fit_log, multi_fit, digits = 4, show.ci = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">log(Ozone)</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">log(Ozone)</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.8380</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.2621</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.637</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Temp</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0675</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0492</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Solar R</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0025</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Wind</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.0616</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">116</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">111</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.547 / 0.543</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.664 / 0.655</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n- The adjusted $R^2$ is higher for the MLR model...\n- Interpretation of $R^2$ is the same as for simple linear regression: how much of the variation in the response variable is explained by the model\n- **Are all the variables/predictors needed?** (next week)\n\n# Summing up\n\n::: {.fragment}\n- We fit a simple linear model to represent a linear relationship between two variables\n    + used method of least squares to find the best fitting line\n    + model equation; $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n:::\n\n::: {.fragment}\n- Multiple linear regression\n    + Do more predictors improve model fit?\n    + $Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + \\epsilon$\n:::\n\n::: {.fragment}\n- Hypothesis testing with linear models\n        + Is our model the best representation of the relationship?\n- Check assumptions to understand the validity of the model\n    + collinearity, linearity, independence, normality, equal variance (CLINE)\n- Transformations to meet assumptions and improve model fit\n- Interpreting model output\n        + F-test via ANOVA and summary(), R^2^\n:::\n\n# Next lecture: Variable selection\nWe will discuss how to select the best subset of predictors for a model.\n\n# Thanks!\n\n**Questions? Comments?**\n\nSlides made with [Quarto](https://quarto.org)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}