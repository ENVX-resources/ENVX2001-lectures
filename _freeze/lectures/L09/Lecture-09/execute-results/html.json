{
  "hash": "20b7e795d261be88ad14941fb66b3576",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression: predictive modelling\"\nauthor: Si Yang Han\nformat: soles-revealjs\n# format: revealjs\n# fontsize: 16pt\nembed-resources: true\nexecute:\n  cache: false\n  echo: true\n  warning: false \n  message: false\n  scrolling: true\n---\n\n\n\n\n\n# Predictive modelling\n\n> \"The best way to predict the future is to create it.\"\n\n-- Peter Ferdinand Drucker, 1909--2005\n\n# Our workflow so far\n\n## Workflow {auto-animate=\"true\"}\n\n1.  [Model development]{style=\"color: #D0D3D4;\"}\n    -   [Explore: visualise, summarise]{style=\"color: #D0D3D4;\"}\n    -   [Transform predictors: linearise, reduce skewness/leverage]{style=\"color: #D0D3D4;\"}\n    -   [Model: fit, check assumptions, interpret, transform. Repeat.]{style=\"color: #D0D3D4;\"}\n2.  [Variable selection]{style=\"color: #D0D3D4;\"}\n    -   [VIF: remove predictors with high variance inflation factor]{style=\"color: #D0D3D4;\"}\n    -   [Model selection: stepwise selection, AIC, principle of parsimony, assumption checks]{style=\"color: #D0D3D4;\"}\n3.  Predictive modelling\n    -   **Predict**: Use the model to predict new data\n    -   **Validate**: Evaluate the modelâ€™s performance\n\n## Previously on ENVX2001... {auto-animate=\"true\"}\n\nWe fitted a multiple linear regression model to the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n## Predictions by hand {auto-animate=\"true\"}\n\n$$ \\widehat{log(Ozone)}=-0.262 + \\color{darkorchid}{0.0492} \\cdot Temp + \\color{darkorange}{0.00252} \\cdot Solar.R - \\color{seagreen}{0.0616} \\cdot Wind $$\n\nOn a certain day, we measured (*in imperial units*):\n\n- [temperature `Temp` to be 80 degrees Fahrenheit]{style=\"color: darkorchid\"}\n- [solar radiation `Solar.R` to be 145 units (Langleys)]{style=\"color: darkorange\"}\n- [wind speed `Wind` to be 10.9 miles per hour]{style=\"color: seagreen\"}\n  \n**What is the predicted ozone level?**\n\n$$\\widehat{log(Ozone)}= -0.262 + \\color{darkorchid}{0.0492 \\cdot 80} + \\color{darkorange}{0.00252 \\cdot 145} - \\color{seagreen}{0.0616 \\cdot 10.9}$$\n\nEasy! The two things we need to think about are...\n\n- **What is the uncertainty in this prediction?**\n- **Can this model be used to predict ozone if we collect new data in the future?**\n\n\n## Uncertainty\n\n- **Confidence interval**: uncertainty in the **mean** response at a given predictor value.\n- **Prediction interval**: uncertainty in a **single** response at a given predictor value.\n\n### What it means\n\n> **95% confidence interval**: Given the parameters of the model, we are 95% confident that the *mean* response at a given predictor value is between $y_1$ and $y_2$.\n\n> **95% prediction interval**: Given the parameters of the model, we are 95% confident that a *single* response at a given predictor value is between $y_1$ and $y_2$.\n\n## Equations\n\nThe equation to calculate any prediction interval is:\n\n$$ \\widehat{y} \\pm t_{\\alpha/2} \\cdot se(\\widehat{y}) $$\n\nwhere: \n\n- $\\widehat{y}$ is the predicted value\n- $t_{\\alpha/2}$ is the critical value of the t-distribution for a given confidence level\n- $se(\\widehat{y})$ is the standard error of the prediction\n\nThe difference in calculating a confidence interval and a prediction interval is in the standard error of the prediction.\n\n## Equations\n\n### CI: standard error of the fit\n\n$$ se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)} $$ \n\n### PI: standard error of the prediction\n\n$$ se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)}$$\n\n- $x_0$ is value of the predictor for which we want a response\n- $MSE$ is the mean squared error of the fit ($SS_{xx}$)\n- $\\sum_{i=1}^n (x_i - \\bar{x})^2$ is the sum of squares of the predictor values\n- $n$ is the number of observations\n- $\\bar{x}$ is the mean of the predictor values\n\nThe prediction interval formula has an additional term ($1 \\cdot MSE$). There is uncertainty that the *mean* prediction will be similar to the observed, and additional uncertainty/variability for a *single* response (equivalent to the MSE). **Thus the confidence interval is always narrower than the prediction interval.**\n\n## Predictions in R\n\n- First, we need to create a new data frame with the predictor values we want to predict at -- it must include all variables in the fitted model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_df <- data.frame(Temp = 80, Solar.R = 145, Wind = 10.9)\n```\n:::\n\n\n\n- We use the `predict()` function to obtain the predicted value.\n- Specifying `interval = \"confidence\"` or `interval = \"prediction\"` also calculates the confidence or prediction interval.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit, newdata = predict_df) # the predicted value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n3.365227 \n```\n\n\n:::\n\n```{.r .cell-code}\npredict(fit, newdata = predict_df, interval = \"confidence\") # predicted value and CI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 3.365227 3.246265 3.484189\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(fit, newdata = predict_df, interval = \"prediction\") # predicted value and PI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 3.365227 2.350051 4.380404\n```\n\n\n:::\n:::\n\n\n\n## Visualising CI and PI\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nairquality$pred <- predict(fit, newdata = airquality) # predict for existing data\n\npreds_ci <- predict(fit, newdata = airquality, interval = \"confidence\") # confidence interval\npreds_pi <- predict(fit, newdata = airquality, interval = \"prediction\") # prediction interval\n```\n:::\n\n\n\n- We can now plot the CI and PI as shaded areas around the predicted line\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np <-\n  ggplot(airquality, aes(pred, log(Ozone))) +\n  geom_point() + \n  geom_line(data = preds_ci, aes(fit, lwr), color = \"blue\") +\n  geom_line(data = preds_ci, aes(fit, upr), color = \"blue\") +\n  geom_line(data = preds_pi, aes(fit, lwr), color = \"red\") +\n  geom_line(data = preds_pi, aes(fit, upr), color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\np\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n## Visualising with `geom_smooth()`\n\n:::{.columns}\n:::{.column width=\"50%\"}\n\n- `geom_smooth()` fits a linear model to obtain a smooth line\n- For visualisation we can use `geom_smooth()` instead of `geom_line()` to fit smoothed lines\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np <-\n  ggplot(airquality, aes(pred, log(Ozone))) +\n  geom_point() + \n  geom_smooth(data = preds_ci, aes(fit, lwr), color = \"blue\", se = F) +\n  geom_smooth(data = preds_ci, aes(fit, upr), color = \"blue\", se = F) +\n  geom_smooth(data = preds_pi, aes(fit, lwr), color = \"red\", se = F) +\n  geom_smooth(data = preds_pi, aes(fit, upr), color = \"red\", se = F) +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\np\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n- The hyperparameter `se = TRUE` fits a smoothed CI around the predicted line\n- Smoothed with loess functions\n- Cannot fit the prediction interval\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np + geom_smooth(method = \"lm\", se = TRUE)\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n# Calibration and validation {auto-animate=true}\n\nAll is good when we want to assess uncertainty in a model that we have already fit. \n\nWhat if we want to know how well the model predicts **new** data, i.e. data that we did not use to fit the model?\n\n## Why separate calibration and validation data?\n\nIn model development we try many models and choose the one that *fits that specific dataset* very well. \n\nSo our model *may be* too complex and overfits the data. If we predict onto new data (in the real world) **the model does not give plausible predictions**.\n\nWhy might this be a problem?\n\n- Predict the wrong **ozone levels** (and people with respiratory issues are not warned)\n- Predict the wrong **disease numbers** (and local health services are not prepared)\n- Predict the wrong **crop yield** (and farmers under/overapply fertiliser)\n\nPredictions can directly be used for decision-making, which has consequences.\n\n## General Idea {auto-animate=true}\n\nWhat if we want to know how well the model predicts **new** data, i.e. data that we did not use to fit the model?\n\n- We build the model with one dataset (calibration).\n- We validate the model's predictions with an **independent dataset**.\n  - If the model is good, we expect the predictions to be *close* to the actual values.\n  - If the model is bad, we expect the predictions to be *far* from the actual values.\n- The dataset can be obtained by:\n  - Collecting new data.\n  - Splitting the existing data into two parts before model building.\n    - Data splitting\n    - Cross-validation\n    \n## Definitions\n\nSometimes the terms for calibration and validation can get muddled.\n\nBest practice:\n\n- Calibration/Training Dataset: the data used to train the model\n- Validation: the data used to fine tune the model (e.g. variable selection, hyperparameters in machine learning)\n- Test: remaining data that has not been used in any kind of model training\n\nTo keep things simple (and if datasets are small), also common:\n\n- Calibration/Training: the data used to train the model\n- Validation/Test: the data used to assess the model's prediction performance\n\n## Our data {auto-animate=true}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 10px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2D708EFF;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n## Collecting new data {auto-animate=true}\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 10px;\"}\n:::\n\n[Dataset (train)]{style=\"color: #2D708EFF;\" .absolute top=130 left=155 data-id=\"text1\"}\n\n$+$\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 200px; height: 50px; margin: 10px;\"}\n:::\n:::\n\n[New dataset (test)]{style=\"color: #95D840FF;\" .absolute top=130 left=700}\n\n<br>\n\n- The best way to assess how well a model predicts new data is to collect new data.\n  - [**Training set**]{style=\"color: #2D708EFF\"}: used to fit the model.\n  - [**Test set**]{style=\"color: #95D840FF\"}: used to assess how well the model predicts new data.\n\n## Collecting new data\n\n### Pros\n\n- The new data is completely independent of the data used to fit the model.\n- More data to *fit* and *validate* compared to data splitting.\n\n### Cons\n\n- It can be expensive and time-consuming to collect new data.\n- Some data may be impossible to collect (e.g. historical data).\n\n\n## Data splitting {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2780e3; width: 500px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #3fb618; width: 0px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2780e3;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n## Data splitting {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2780e3; width: 400px; height: 50px; margin: 0px;\"}\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #3fb618; width: 100px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2780e3;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n[(Training)]{style=\"color: #2780e3;\" .absolute top=130 left=368 data-id=\"text2\"}\n\n[Subset (Test)]{style=\"color: #3fb618;\" .absolute top=130 left=680}\n\n## Data splitting {auto-animate=true}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2780e3; width: 400px; height: 50px; margin: 0px;\"}\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #3fb618; width: 100px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2780e3;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n[(Training)]{style=\"color: #2780e3;\" .absolute top=130 left=368 data-id=\"text2\"}\n\n[Subset (Test)]{style=\"color: #3fb618;\" .absolute top=130 left=680}\n\n<br>\n\n- Split the existing dataset into training and test datasets (80:20, 70:30, 60:40, etc.)\n  - [**Training set**]{style=\"color: #2D708EFF\"}: used to fit the model.\n  - [**Test set**]{style=\"color: #95D840FF\"}: used to assess how well the model predicts new data.\n  \n## Data splitting {auto-animate=true}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2780e3; width: 400px; height: 50px; margin: 0px;\"}\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #3fb618; width: 100px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2780e3;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n[(Training)]{style=\"color: #2780e3;\" .absolute top=130 left=368 data-id=\"text2\"}\n\n[Subset (Test)]{style=\"color: #3fb618;\" .absolute top=130 left=680}\n\n<br>\n\n\n- Only possible for larger datasets (hundreds of observations)\n- Split the existing dataset into calibration, validation and test datasets (70:15:15, etc.)\n  - [**Calibration set**]{style=\"color: #2D708EFF\"}: used to fit the model.\n  - [**Validation set**]{style=\"color: #95D840FF\"}: used to test model development (prevent overfitting).\n  - [**Test set**]{style=\"color: #D84095\"}: used to assess how well the model predicts new data.\n\n## Data splitting\n\n### Pros\n\n- Compared to collecting new data, it is cheaper and faster to split existing data.\n\n### Cons\n\n- We have *less* data to fit the model and *less* data to validate the model.\n- How do we split the data? Randomly? By time? By location?\n\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Single dataset -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 0px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 0px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Cross-validated dataset (random split) -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Make bars smaller -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Add iterations -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 250px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 150px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n[Iteration 1]{style=\"color: #000; font-size: 18px;\" .absolute top=55 right=790 data-id=\"text1\"}\n\n[Iteration 2]{style=\"color: #000; font-size: 18px;\" .absolute top=75 right=790 data-id=\"text2\"}\n\n[Iteration 3]{style=\"color: #000; font-size: 18px;\" .absolute top=95 right=790 data-id=\"text3\"}\n\n[And so on...]{style=\"color: #000; font-size: 18px;\" .absolute top=115 right=790 data-id=\"text4\"}\n\n\n## *k*-fold cross-validation \n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 250px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 150px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n[Iteration 1]{style=\"color: #000; font-size: 18px;\" .absolute top=55 right=790 data-id=\"text1\"}\n[Iteration 2]{style=\"color: #000; font-size: 18px;\" .absolute top=75 right=790 data-id=\"text2\"}\n[Iteration 3]{style=\"color: #000; font-size: 18px;\" .absolute top=95 right=790 data-id=\"text3\"}\n[3-fold cross-validation]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=790 data-id=\"text4\"}\n[Fold 1]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=660}\n[Fold 2]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=494}\n[Fold 3]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=328}\n\n<br>\n\n- Like data splitting, where existing data is split into two parts:\n  - [**Training set**]{style=\"color: #2D708EFF\"}: used to fit the model.\n  - [**Test set**]{style=\"color: #95D840FF\"}: used to assess how well the model predicts new data.\n- The **difference** is that the splitting is done *multiple* times, and the model is fit and validated *multiple* times.\n- Each iteration or fold is used for testing once.\n\n## *k*-fold cross-validation\n\n### Pros\n\n- Same as data splitting, but also:\n  - The model is fit and validated *multiple* times, so we can get a better estimate of how well the model predicts new data.\n  - Greatly reduces overfitting as the model's performance is not just a result of the particular way the data was split.\n\n### Cons\n\n- Bias in small datasets: each fold may contain too little data to provide a representative sample.\n- Each fold fits a **new** model so it is not used for interpretation, only for prediction quality.\n  - Computationally more expensive.\n  \n## Cross-validation\n\n*k*-fold cross-validation splits data in each fold randomly.\n\nIf there is some underlying structure to the data, consider:\n\n- Spatial cross-validation (e.g. fields on a farm, each fold is one field)\n- Temporal cross-validation (e.g. time series data, each fold is a time period)\n- Stratified cross-validation (e.g. each fold has the same proportion of each category)\n\n# Assessing prediction quality\n\nHow 'good' are the predictions? Observed vs predicted.\n\n## Visually {auto-animate=true}\n\n- **Plot**: observed ($y_i$) vs predicted ($\\hat y$) values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = airquality, aes(x = log(Ozone), y = pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\nWe can also use metrics to quantify how well the model predicts new data:\n\n- How close points are to the 1:1 line\n- How *linear* the relationship is between observed and predicted values\n\n## Error {auto-animate=true}\n\nThe smaller the error, the better the model.\n\n**Mean error**: the average difference between observed and predicted values.\n\n- Can be positive or negative to indicate over- or under-estimation (a measure of bias)\n  \n$$ME = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\hat{y}_i$$ (in $y$ units)\n\n**Mean absolute error**: the average (absolute) difference between observed and predicted values (residual).\n\n$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$ (in $y$ units)\n\n**Mean squared error**: the average of the squared residuals\n\n- Squared so positive and negative errors do not cancel each other out\n- Penalises poor predictions more\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\n**Root mean squared error**: the standard deviation of the residuals\n\n- Squaring the error penalises poor predictions more\n  \n$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$ (in $y$ units)\n\n## Linearity {auto-animate=true}\n\nThe more linear the relationship between observed and predicted values, the better the model.\n\n**Pearson's correlation coefficient (*r*)**: \n\n- A measure of the strength and direction of a linear relationship between two variables.\n- Ranges from -1 to 1, with 0 indicating no relationship and 1 indicating a perfect positive linear relationship.\n\n$$r = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{\\hat{y}})^2}}$$\n\n**R^2^**: \n\n- The proportion of variance explained by the variables in the model.\n- When two variables are compared (e.g. observed vs predicted) it is the same as correlation squared.\n- A value of 1 indicates a perfect linear relationship.\n\n$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n**Lin's concordance correlation coefficient ($\\rho_c$)**:\n\n- A measure of agreement between two variables (based on covariance, variances, and difference in means).\n- How well the points fit the 1:1 line.\n- A value of 0 indicates no agreement and 1 indicating perfect agreement.\n\n$$LCCC = \\frac{2\\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) + (\\mu_X - \\mu_Y)^2}$$\n\n## LCCC vs $r$ and R^2^\n\nLCCC combines error and linearity so it better measures the fit to the 1:1 line.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf <- tibble(y = seq(0, 100, 5),\n  \"45 degree line | CCC = 1\" = seq(0, 100, 5)) %>%\n  mutate(\"Location shift | CCC = 0.89\" = `45 degree line | CCC = 1` - 15) %>%\n  mutate(\"Scale shift | CCC = 0.52\" = y / 2) %>%\n  mutate(\"Location and scale shift | CCC = 0.67\" = y * 2 - 20)\n\n# pivot\ndf_long <- df %>%\n  pivot_longer(-1, values_to = \"x\") %>%\n  mutate(name = factor(name, \n    levels = c(\"45 degree line | CCC = 1\",\n      \"Location shift | CCC = 0.89\",\n      \"Scale shift | CCC = 0.52\",\n      \"Location and scale shift | CCC = 0.67\")))\n\nggplot(df_long, aes(x, y)) +\n  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = \"grey\") +\n  facet_wrap(~name) +\n  geom_point() +\n  xlim(0, 100) +\n  labs(x = \"\", y = \"\") +\n  theme_classic() +\n  geom_blank() \n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n## Limitations\n\nNo one metric is perfect.\n\nEach prediction below has an LCCC of 0.6.\n\n[Wadoux and Minasny 2024](https://www.sciencedirect.com/science/article/pii/S1574954124003625)\n![](images/LCCC.jpg)\n\n:::{.callout-tip}\nUse multiple metrics to test prediction quality, and always plot the predicted vs observed.\n:::\n\n# Example: Loyn dataset\n\nWe will go through several examples to practice data splitting, cross-validation, and model evaluation.\n\n## About\n\nData on the relationship between bird abundance (bird ha^-1^) and the characteristics of forest patches at 56 locations in SE Victoria.  \n\nThe predictor variables are:\n\n- `ALT` Altitude (m) \n- `YR.ISOL` Year when the patch was isolated (years) \n-\t`GRAZE` Grazing (coded 1-5 which is light to heavy) \n-\t`AREA` Patch area (ha) \n-\t`DIST` Distance to nearest patch (km) \n-\t`LDIST` Distance to largest patch (km)  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn <- read_csv(\"images/loyn.csv\")\n```\n:::\n\n\n\n\n## Dataset splitting\n\nWe will split the data into training and test sets.\n\nAs the dataset is quite small, we will use a 80:20 split.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nindexes <- sample(1:nrow(loyn), size = 0.2 * nrow(loyn)) # randomly sample 20% of rows in the dataset\nloyn_train <- loyn[-indexes, ] # remove the 20% - training dataset\nloyn_test <- loyn[indexes, ] # select the 20% - test dataset\n```\n:::\n\n\n\n## Checking the split\n\nCheck out the `str()` of the data to see if the split worked (number of observations).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(loyn_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [45 Ã— 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ AREA   : num [1:45] 0.1 0.5 0.5 1 1 1 1 1 2 2 ...\n $ YR.ISOL: num [1:45] 1968 1920 1900 1966 1918 ...\n $ DIST   : num [1:45] 39 234 104 66 246 467 284 156 66 93 ...\n $ LDIST  : num [1:45] 39 234 311 66 246 ...\n $ GRAZE  : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT    : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(loyn_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [11 Ã— 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:11] 3 29.5 26 39.6 34.4 19.5 14.6 28.3 15.8 5 ...\n $ AREA   : num [1:11] 1 973 18 49 96 6 2 34 5 4 ...\n $ YR.ISOL: num [1:11] 1900 1970 1966 1972 1976 ...\n $ DIST   : num [1:11] 311 337 40 1427 39 ...\n $ LDIST  : num [1:11] 571 1323 3188 1557 519 ...\n $ GRAZE  : num [1:11] 5 1 2 1 2 3 1 1 3 5 ...\n $ ALT    : num [1:11] 130 190 190 180 175 170 210 110 130 120 ...\n```\n\n\n:::\n:::\n\n\n\n# Model development\nFrom now on, we will work with the **training set only**.\n\n## Exploratory data analysis\n\n- The next step is to visualise the data.\n- Expore relationships between the predictors and the response via histograms, scatterplots, boxplots, correlations etc.\n\nIn this lecture we will just look at histograms.\n\n## Histograms\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nloyn_train %>%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %>% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n- Looks like `AREA` `LDIST` and `DIST` are skewed -- we will transform them so that they are more normally distributed.\n\n## Transforming predictors\n\n We will use `log10()` to transform the predictors. The `mutate()` function from the `dplyr` package is useful for this as it can create new columns in the data frame with the transformed values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn_train <- loyn_train %>%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    )\n```\n:::\n\n\n\nThen, remove the untransformed variables from the dataset. Here we can use the `select()` function from the `dplyr` package to \"delselect\" columns by using the `-` sign.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn_train <- loyn_train %>%\n    select(-AREA, -LDIST, -DIST)\n\nstr(loyn_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [45 Ã— 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND    : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ YR.ISOL  : num [1:45] 1968 1920 1900 1966 1918 ...\n $ GRAZE    : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT      : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n $ AREA_L10 : num [1:45] -1 -0.301 -0.301 0 0 ...\n $ LDIST_L10: num [1:45] 1.59 2.37 2.49 1.82 2.39 ...\n $ DIST_L10 : num [1:45] 1.59 2.37 2.02 1.82 2.39 ...\n```\n\n\n:::\n:::\n\n\n\n## Final inspection\n\nView the histograms again to check that the transformation worked.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nloyn_train %>%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %>% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n## Full model\n\nWe start with a full model that includes all the predictors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_fit <- lm(ABUND ~ ., data = loyn_train)\nsummary(full_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,\tAdjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n```\n\n\n:::\n:::\n\n\n\n## Assumptions - Round 1\n\nAs usual, we should check the assumptions of the model (CLINE + outliers). We will use the `check_model()` function from the `perfomance` package (because it looks nice and has interpretation instructions).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(full_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n## Assumptions - Round 1\n\nWe check multicollinearity with variable inflation factors (VIF) - VIFs are all < 10, so there is no multicollinearity. All assumptions are thus met.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(full_fit, check = c(\"vif\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n## Backwards stepwise selection\n\nUse the `step()` function perform backwards stepwise selection. This function uses AIC to select the best model. \n\nDepending on the dataset splitting, the best model may be different each time we randomly sample the data. In this case we should all have the same results as we set the seed.\n\nIf we compare to the full model, the adjusted r-squared is slightly higher, and the AIC is lower.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep_fit <- step(full_fit, direction = \"backward\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=174.81\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- ALT        1     14.67 1618.5 173.22\n- LDIST_L10  1     16.92 1620.8 173.28\n- DIST_L10   1     69.18 1673.0 174.71\n<none>                   1603.9 174.81\n- GRAZE      1     78.00 1681.9 174.94\n- YR.ISOL    1    126.48 1730.3 176.22\n- AREA_L10   1    867.44 2471.3 192.26\n\nStep:  AIC=173.22\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1     10.76 1629.3 171.52\n<none>                   1618.5 173.22\n- DIST_L10   1     85.56 1704.1 173.54\n- GRAZE      1     98.23 1716.8 173.87\n- YR.ISOL    1    117.80 1736.3 174.38\n- AREA_L10   1   1088.05 2706.6 194.35\n\nStep:  AIC=171.52\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n<none>                  1629.3 171.52\n- GRAZE     1     93.97 1723.3 172.04\n- YR.ISOL   1    107.73 1737.0 172.40\n- DIST_L10  1    114.60 1743.9 172.57\n- AREA_L10  1   1161.66 2791.0 193.74\n```\n\n\n:::\n:::\n\n\n\n## The selected model\n\n:::{.columns}\n:::{.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(full_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,\tAdjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n```\n\n\n:::\n:::\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(step_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10, data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1683  -3.1961   0.3374   3.4834  14.2021 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -135.17508  102.72850  -1.316    0.196    \nYR.ISOL        0.08323    0.05118   1.626    0.112    \nGRAZE         -1.50496    0.99082  -1.519    0.137    \nAREA_L10       8.61888    1.61392   5.340 3.98e-06 ***\nDIST_L10      -4.87726    2.90770  -1.677    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.382 on 40 degrees of freedom\nMultiple R-squared:  0.6701,\tAdjusted R-squared:  0.6371 \nF-statistic: 20.31 on 4 and 40 DF,  p-value: 3.365e-09\n```\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n## Assumptions - Round 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(step_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\", \"vif\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n# Model validation\nIt looks like the model is good, so let's bring in the test set to see how it performs!\n\n## Prepare the test data\n\nSince the test data has not been transformed, we need to do that first.\n\nWe then predict onto the training and test dataset using the reduced stepwise model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn_test <- loyn_test %>%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    ) %>%\n    select(-AREA, -LDIST, -DIST)\n\nloyn_train$pred <- predict(step_fit, newdata = loyn_train)\nloyn_test$pred <- predict(step_fit, newdata = loyn_test)\n```\n:::\n\n\n\n## Plotting observed vs predicted\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np1 <- ggplot(loyn_train, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Training\") +\n  theme_classic()\n\np2 <- ggplot(loyn_test, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Test\") +\n  theme_classic()\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](Lecture-09_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n\n## Calculating metrics - error\n\nGenerally the training dataset will have lower error and higher linearity than the test dataset. If this difference is very large -- it suggests the model is not applicable to the test data and overfitting.\n\nWith error -- lower is better. The following all measure error **in the same units as the response variable** (number of birds in each forest patch).\n\n### Mean error\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(loyn_train$ABUND - loyn_train$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(loyn_test$ABUND - loyn_test$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.65\n```\n\n\n:::\n:::\n\n\n\nWe expect the ME for the training dataset to be near 0 -- a well-fitted linear regression model will have positive and negative residuals balance each other out.\n\n### Mean absolute error\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(abs(loyn_train$ABUND - loyn_train$pred)) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.43\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(abs(loyn_test$ABUND - loyn_test$pred)) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.21\n```\n\n\n:::\n:::\n\n\n\n### Root mean squared error (`caret` package)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRMSE(loyn_train$ABUND, loyn_train$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.02\n```\n\n\n:::\n\n```{.r .cell-code}\nRMSE(loyn_test$ABUND, loyn_test$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.72\n```\n\n\n:::\n:::\n\n\n\n## Calculating metrics -- linearity\n\nWith linearity -- higher is better.\n\nBoth training and test datasets perform similarly, which is a good sign the model is not overfitting.\n\n### Pearson's correlation coefficient *r*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(loyn_train$ABUND, loyn_train$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.82\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(loyn_test$ABUND, loyn_test$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.82\n```\n\n\n:::\n:::\n\n\n\n### R^2^\n\nCan either square the correlation coefficient or use the `R2()` function from the `caret` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2(loyn_train$ABUND, loyn_train$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.67\n```\n\n\n:::\n\n```{.r .cell-code}\nR2(loyn_test$ABUND, loyn_test$pred) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.68\n```\n\n\n:::\n:::\n\n\n\n### Lin's concordance correlation coefficient (CCC) (`epiR` package)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8\n```\n\n\n:::\n\n```{.r .cell-code}\nepi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.81\n```\n\n\n:::\n:::\n\n\n\n## Conclusions\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# put all data into a tible and kable it\ntibble(\n    Dataset = c(\"Training\", \"Test\"),\n    ME = c(\n        mean(loyn_train$ABUND - loyn_train$pred),\n        mean(loyn_test$ABUND - loyn_test$pred)\n    ),\n    MAE = c(\n        mean(abs(loyn_train$ABUND - loyn_train$pred)),\n        mean(abs(loyn_test$ABUND - loyn_test$pred))\n    ),\n    RMSE = c(\n        RMSE(loyn_train$ABUND, loyn_train$pred),\n        RMSE(loyn_test$ABUND, loyn_test$pred)\n    ),\n    cor = c(\n        cor(loyn_train$ABUND, loyn_train$pred),\n        cor(loyn_test$ABUND, loyn_test$pred)\n    ),\n    R2 = c(\n        R2(loyn_train$ABUND, loyn_train$pred),\n        R2(loyn_test$ABUND, loyn_test$pred)\n    ),\n    LCCC = c(\n        epi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est,\n        epi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est\n    )\n) %>%\n    knitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|Dataset  |    ME|  MAE| RMSE|  cor|   R2| LCCC|\n|:--------|-----:|----:|----:|----:|----:|----:|\n|Training |  0.00| 4.43| 6.02| 0.82| 0.67| 0.80|\n|Test     | -1.65| 5.21| 6.72| 0.82| 0.68| 0.81|\n\n\n:::\n:::\n\n\n\n- The model fit for the training dataset is marginally better (slight overfitting, but not a concern)\n- Small differences are also expected due to the small sample size or the chosen `set.seed()`\n- The model predicts bird abundance in forest patches in SE Victoria well (***r*** = 0.82, **LCCC** = 0.81, **MAE** = 5.21 birds/patch, **RMSE** = 6.72 birds/patch).\n\n# Thanks!\n\n**Questions? Comments?**\n\nSlides made with [Quarto](https://quarto.org)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}